{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8089266,"sourceType":"datasetVersion","datasetId":4775518},{"sourceId":8089281,"sourceType":"datasetVersion","datasetId":4775527},{"sourceId":12479627,"sourceType":"datasetVersion","datasetId":7874205},{"sourceId":12479887,"sourceType":"datasetVersion","datasetId":7874406},{"sourceId":258352,"sourceType":"modelInstanceVersion","modelInstanceId":218283,"modelId":240009},{"sourceId":470644,"sourceType":"modelInstanceVersion","modelInstanceId":379672,"modelId":399557},{"sourceId":473434,"sourceType":"modelInstanceVersion","modelInstanceId":381305,"modelId":400996}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1>Creating Graphs</h1>","metadata":{}},{"cell_type":"code","source":"!pip install powerlaw","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:14:50.374091Z","iopub.execute_input":"2025-10-05T15:14:50.375044Z","iopub.status.idle":"2025-10-05T15:14:53.567473Z","shell.execute_reply.started":"2025-10-05T15:14:50.375010Z","shell.execute_reply":"2025-10-05T15:14:53.566726Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: powerlaw in /usr/local/lib/python3.11/dist-packages (1.5)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from powerlaw) (1.15.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from powerlaw) (1.26.4)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from powerlaw) (3.7.2)\nRequirement already satisfied: mpmath in /usr/local/lib/python3.11/dist-packages (from powerlaw) (1.3.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw) (11.2.1)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->powerlaw) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->powerlaw) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->powerlaw) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->powerlaw) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->powerlaw) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->powerlaw) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->powerlaw) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->powerlaw) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->powerlaw) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->powerlaw) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->powerlaw) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->powerlaw) (2024.2.0)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import json\n\nwith open(\"/kaggle/input/cic-ids-20117/df_properties.json\", \"r\") as f:\n    df_properties = json.load(f)\n\nprint(\"üìò Dataset Properties:\")\nfor k, v in df_properties.items():\n    print(f\"{k}: {v}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:14:53.569047Z","iopub.execute_input":"2025-10-05T15:14:53.569263Z","iopub.status.idle":"2025-10-05T15:14:53.575266Z","shell.execute_reply.started":"2025-10-05T15:14:53.569242Z","shell.execute_reply":"2025-10-05T15:14:53.574638Z"}},"outputs":[{"name":"stdout","text":"üìò Dataset Properties:\nname: cic_ids_2017\nlength: 2286203\nnum_benign: 1737235\npercentage_of_benign_records: 75.98778411190958\nnum_attack: 548968\npercentage_of_attack_records: 24.012215888090427\nattacks: ['BENIGN', 'DDoS', 'PortScan', 'Bot', 'Infiltration', 'Web Attack ÔøΩ Brute Force', 'Web Attack ÔøΩ XSS', 'Web Attack ÔøΩ Sql Injection', 'FTP-Patator', 'SSH-Patator', 'DoS slowloris', 'DoS Slowhttptest', 'DoS Hulk', 'DoS GoldenEye', 'Heartbleed']\nweak_columns: ['Bwd Pkt Len Max', 'Fwd PSH Flags', 'Bwd Pkt Len Mean', 'RST Flag Cnt', 'TotLen Fwd Pkts', 'Fwd Pkt Len Max', 'Subflow Fwd Pkts', 'Pkt Len Mean', 'Fwd Blk Rate Avg', 'Bwd Pkts/b Avg', 'Flow IAT Mean', 'Bwd IAT Std', 'Idle Mean', 'Fwd URG Flags', 'Bwd Byts/b Avg', 'Fwd Pkt Len Std', 'Bwd Blk Rate Avg', 'Fwd Pkt Len Mean', 'Fwd Pkts/b Avg', 'Fwd IAT Mean', 'Subflow Bwd Byts', 'Active Std', 'Flow IAT Max', 'Idle Max', 'Bwd IAT Mean', 'Pkt Len Var', 'Subflow Bwd Pkts', 'TotLen Bwd Pkts', 'Pkt Size Avg', 'Fwd IAT Tot', 'Bwd PSH Flags', 'Bwd Pkt Len Std', 'Flow IAT Std', 'Fwd IAT Min', 'Pkt Len Std', 'Fwd IAT Max', 'Pkt Len Max', 'Bwd URG Flags', 'Tot Bwd Pkts', 'Flow Pkts/s', 'Protocol', 'Flow Duration', 'Active Mean', 'Tot Fwd Pkts', 'Fwd Byts/b Avg']\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import pickle\n\nwith open(\"/kaggle/input/cic-ids-20117/labels_names.pkl\", \"rb\") as f:\n    labels_names = pickle.load(f)\nprint(labels_names)\nprint(\"\\nüè∑Ô∏è Label Class Mapping:\")\n# for k, v in labels_names.items():\n#     print(f\"{k}: {v}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:14:53.575944Z","iopub.execute_input":"2025-10-05T15:14:53.576194Z","iopub.status.idle":"2025-10-05T15:14:53.587318Z","shell.execute_reply.started":"2025-10-05T15:14:53.576168Z","shell.execute_reply":"2025-10-05T15:14:53.586665Z"}},"outputs":[{"name":"stdout","text":"[{0: 'BENIGN', 1: 'Bot', 2: 'DDoS', 3: 'DoS GoldenEye', 4: 'DoS Hulk', 5: 'DoS Slowhttptest', 6: 'DoS slowloris', 7: 'FTP-Patator', 8: 'Heartbleed', 9: 'Infiltration', 10: 'PortScan', 11: 'SSH-Patator', 12: 'Web Attack ÔøΩ Brute Force', 13: 'Web Attack ÔøΩ Sql Injection', 14: 'Web Attack ÔøΩ XSS'}, array(['BENIGN', 'DDoS', 'PortScan', 'Bot', 'Infiltration',\n       'Web Attack ÔøΩ Brute Force', 'Web Attack ÔøΩ XSS',\n       'Web Attack ÔøΩ Sql Injection', 'FTP-Patator', 'SSH-Patator',\n       'DoS slowloris', 'DoS Slowhttptest', 'DoS Hulk', 'DoS GoldenEye',\n       'Heartbleed'], dtype=object)]\n\nüè∑Ô∏è Label Class Mapping:\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# !pip install igraph\n# !pip install networkx ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:14:53.588846Z","iopub.execute_input":"2025-10-05T15:14:53.589227Z","iopub.status.idle":"2025-10-05T15:14:53.597191Z","shell.execute_reply.started":"2025-10-05T15:14:53.589205Z","shell.execute_reply":"2025-10-05T15:14:53.596444Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"import os\nimport pickle\nimport random\nimport socket\nimport struct\n\nimport sys\nimport pandas as pd\nsys.path.append(\"/kaggle/input/gnn-nids/pytorch/default/1\")\n\nimport networkx as nx\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom src.dataset.dataset_info import datasets\nfrom src.graph.graph_construction import create_weightless_window_graph\nfrom src.graph.graph_measures import calculate_graph_measures\nfrom src.graph.centralities import add_centralities, add_centralities_as_node_features\n# from local_variables import local_datasets_path\nlocal_datasets_path = \"/kaggle/input/\"\noriginal_datasets_files_path = \"/kaggle/input/cic-ids-2017-gnn-2\"\n# /kaggle/input/cic-ids-2017-gnn-2\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-10-05T15:14:53.597917Z","iopub.execute_input":"2025-10-05T15:14:53.598168Z","iopub.status.idle":"2025-10-05T15:14:53.606731Z","shell.execute_reply.started":"2025-10-05T15:14:53.598146Z","shell.execute_reply":"2025-10-05T15:14:53.605998Z"},"trusted":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":"multi_class = True\n\nuse_node_features = True\n\nuse_port_in_address = False\n\ngenerated_ips = False\n\ngraph_type = \"flow\"\n# graph_type = \"window\"\n# graph_type = \"line\"\n\n# window_size= 400\nwindow_size= 500\n# print(\"1. flow sorted\")\n# print(\"2. flow using node features unsorted\") \nprint(\"3. flow using node features sorted\")\n# 4. flow using port numbers sorted\nsort_timestamp = True\n\n# k_fold = None\n# k_fold = 5\n\nvalidation_size = 0.1\ntest_size = 0.1\n\ncn_measures = [\"betweenness\", \"degree\", \"pagerank\", \"closeness\", \"k_truss\"]\n# cn_measures = [\"betweenness\", \"degree\", \"closeness\"]\n\nnetwork_features = ['src_betweenness', 'dst_betweenness', 'src_degree', 'dst_degree', 'src_pagerank', 'dst_pagerank', 'src_closeness', 'dst_closeness', 'src_k_truss', 'dst_k_truss']\n# network_features = ['src_betweenness', 'dst_betweenness', 'src_degree', 'dst_degree', 'src_pagerank', 'dst_pagerank']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:14:53.782398Z","iopub.execute_input":"2025-10-05T15:14:53.782593Z","iopub.status.idle":"2025-10-05T15:14:53.787331Z","shell.execute_reply.started":"2025-10-05T15:14:53.782578Z","shell.execute_reply":"2025-10-05T15:14:53.786627Z"}},"outputs":[{"name":"stdout","text":"3. flow using node features sorted\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# name = \"cic_ton_iot_5_percent\"\n# name = \"cic_ton_iot\"\n# name = \"cic_ids_2017_5_percent\"\nname = \"cic_ids_2017\"\n# name = \"cic_bot_iot\"\n# name = \"cic_ton_iot_modified\"\n# name = \"nf_ton_iotv2_modified\"\n# name = \"ccd_inid_modified\"\n# name = \"nf_uq_nids_modified\"\n# name = \"edge_iiot\"\n# name = \"nf_cse_cic_ids2018\"\n# name = \"nf_bot_iotv2\"\n# name = \"nf_uq_nids\"\n# name = \"x_iiot\"\n\ndataset = datasets[name]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:15:07.969198Z","iopub.execute_input":"2025-10-05T15:15:07.969757Z","iopub.status.idle":"2025-10-05T15:15:07.973040Z","shell.execute_reply.started":"2025-10-05T15:15:07.969728Z","shell.execute_reply":"2025-10-05T15:15:07.972397Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"g_type = \"\"\nif graph_type == \"flow\":\n    g_type = \"flow\"\nelif graph_type == \"line\":\n    g_type = f\"line_graph_{window_size}\"\nelif graph_type == \"window\":\n    g_type = f\"window_graph_{window_size}\"\n    \nif multi_class:\n    g_type += \"__multi_class\"\n    \nif use_node_features:\n    g_type += \"__n_feats\"\n    \n# if k_fold:\n#     g_type += f\"__{k_fold}_fold\"\n    \nif use_port_in_address:\n    g_type += \"__ports\"\n    \nif generated_ips:\n    g_type += \"__generated_ips\"\n    \nif sort_timestamp:\n    g_type += \"__sorted\"\nelse:\n    g_type += \"__unsorted\"\n# ****************** ALTERED    \n\n\n\ndataset_path = os.path.join(local_datasets_path,name)\nfolder_path = os.path.join(dataset_path, g_type)\nif name == \"cic_ids_2017\":\n    name_ds = \"cic-ids-2017-gnn-2\"\n    dataset_path = os.path.join(local_datasets_path,name_ds)\n    # folder_path = os.path.join(local_datasets_path,name, g_type)\n    folder_path= os.path.join(\"/kaggle/working/\",name,g_type)\n\nprint(folder_path)\nprint(dataset_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:15:08.335452Z","iopub.execute_input":"2025-10-05T15:15:08.335686Z","iopub.status.idle":"2025-10-05T15:15:08.341703Z","shell.execute_reply.started":"2025-10-05T15:15:08.335668Z","shell.execute_reply":"2025-10-05T15:15:08.341016Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/cic_ids_2017/flow__multi_class__unsorted\n/kaggle/input/cic-ids-2017-gnn-2\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# if name== \"cic_ids_2017\":\n#     df=pd.read_parquet(os.path.join(dataset_path, f\"{name}.parquet\"))\n# else:\ndf = pd.read_parquet(os.path.join(dataset_path, f\"{name}.parquet\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:15:09.214290Z","iopub.execute_input":"2025-10-05T15:15:09.214534Z","iopub.status.idle":"2025-10-05T15:15:11.205992Z","shell.execute_reply.started":"2025-10-05T15:15:09.214514Z","shell.execute_reply":"2025-10-05T15:15:11.205417Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2025-10-05T15:15:11.206950Z","iopub.execute_input":"2025-10-05T15:15:11.207166Z","iopub.status.idle":"2025-10-05T15:15:11.227911Z","shell.execute_reply.started":"2025-10-05T15:15:11.207148Z","shell.execute_reply":"2025-10-05T15:15:11.227242Z"},"trusted":true},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"   Flow ID          Src IP Src Port         Dst IP Dst Port  Protocol  \\\n0   758097  104.16.207.165    443.0   192.168.10.5  54865.0       6.0   \n1   758390   104.16.28.216     80.0   192.168.10.5  55054.0       6.0   \n2   758391   104.16.28.216     80.0   192.168.10.5  55055.0       6.0   \n3   422766   104.17.241.25    443.0  192.168.10.16  46236.0       6.0   \n4   758524  104.19.196.102    443.0   192.168.10.5  54863.0       6.0   \n\n                  Timestamp  Flow Duration  Tot Fwd Pkts  Tot Bwd Pkts  ...  \\\n0 2017-07-07 03:30:00+00:00            3.0           2.0           0.0  ...   \n1 2017-07-07 03:30:00+00:00          109.0           1.0           1.0  ...   \n2 2017-07-07 03:30:00+00:00           52.0           1.0           1.0  ...   \n3 2017-07-07 03:30:00+00:00           34.0           1.0           1.0  ...   \n4 2017-07-07 03:30:00+00:00            3.0           2.0           0.0  ...   \n\n   Active Std  Active Max  Active Min  Idle Mean  Idle Std  Idle Max  \\\n0         0.0         0.0         0.0        0.0       0.0       0.0   \n1         0.0         0.0         0.0        0.0       0.0       0.0   \n2         0.0         0.0         0.0        0.0       0.0       0.0   \n3         0.0         0.0         0.0        0.0       0.0       0.0   \n4         0.0         0.0         0.0        0.0       0.0       0.0   \n\n   Idle Min  Label  Attack  Class  \n0       0.0      0  BENIGN      0  \n1       0.0      0  BENIGN      0  \n2       0.0      0  BENIGN      0  \n3       0.0      0  BENIGN      0  \n4       0.0      0  BENIGN      0  \n\n[5 rows x 85 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Flow ID</th>\n      <th>Src IP</th>\n      <th>Src Port</th>\n      <th>Dst IP</th>\n      <th>Dst Port</th>\n      <th>Protocol</th>\n      <th>Timestamp</th>\n      <th>Flow Duration</th>\n      <th>Tot Fwd Pkts</th>\n      <th>Tot Bwd Pkts</th>\n      <th>...</th>\n      <th>Active Std</th>\n      <th>Active Max</th>\n      <th>Active Min</th>\n      <th>Idle Mean</th>\n      <th>Idle Std</th>\n      <th>Idle Max</th>\n      <th>Idle Min</th>\n      <th>Label</th>\n      <th>Attack</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>758097</td>\n      <td>104.16.207.165</td>\n      <td>443.0</td>\n      <td>192.168.10.5</td>\n      <td>54865.0</td>\n      <td>6.0</td>\n      <td>2017-07-07 03:30:00+00:00</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>BENIGN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>758390</td>\n      <td>104.16.28.216</td>\n      <td>80.0</td>\n      <td>192.168.10.5</td>\n      <td>55054.0</td>\n      <td>6.0</td>\n      <td>2017-07-07 03:30:00+00:00</td>\n      <td>109.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>BENIGN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>758391</td>\n      <td>104.16.28.216</td>\n      <td>80.0</td>\n      <td>192.168.10.5</td>\n      <td>55055.0</td>\n      <td>6.0</td>\n      <td>2017-07-07 03:30:00+00:00</td>\n      <td>52.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>BENIGN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>422766</td>\n      <td>104.17.241.25</td>\n      <td>443.0</td>\n      <td>192.168.10.16</td>\n      <td>46236.0</td>\n      <td>6.0</td>\n      <td>2017-07-07 03:30:00+00:00</td>\n      <td>34.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>BENIGN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>758524</td>\n      <td>104.19.196.102</td>\n      <td>443.0</td>\n      <td>192.168.10.5</td>\n      <td>54863.0</td>\n      <td>6.0</td>\n      <td>2017-07-07 03:30:00+00:00</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>BENIGN</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows √ó 85 columns</p>\n</div>"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"cols_to_norm = list(set(list(df.columns))  - set(list([dataset.label_col, dataset.class_num_col])) - set(dataset.drop_columns)  - set(dataset.weak_columns))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:15:11.228623Z","iopub.execute_input":"2025-10-05T15:15:11.228869Z","iopub.status.idle":"2025-10-05T15:15:11.237637Z","shell.execute_reply.started":"2025-10-05T15:15:11.228852Z","shell.execute_reply":"2025-10-05T15:15:11.236950Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"df[dataset.label_col].value_counts()","metadata":{"execution":{"iopub.status.busy":"2025-10-05T15:15:11.238915Z","iopub.execute_input":"2025-10-05T15:15:11.239130Z","iopub.status.idle":"2025-10-05T15:15:11.262069Z","shell.execute_reply.started":"2025-10-05T15:15:11.239116Z","shell.execute_reply":"2025-10-05T15:15:11.261309Z"},"trusted":true},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"Label\n0    1737235\n1     548968\nName: count, dtype: int64"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"if generated_ips:\n    df[dataset.src_ip_col] = df[dataset.src_ip_col].apply(lambda x: socket.inet_ntoa(struct.pack('>I', random.randint(0xac100001, 0xac1f0001))))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:15:11.720977Z","iopub.execute_input":"2025-10-05T15:15:11.721201Z","iopub.status.idle":"2025-10-05T15:15:11.725115Z","shell.execute_reply.started":"2025-10-05T15:15:11.721183Z","shell.execute_reply":"2025-10-05T15:15:11.724332Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"if sort_timestamp:\n    # df[dataset.timestamp_col] = pd.to_datetime(df[dataset.timestamp_col].str.strip(), format=dataset.timestamp_format)\n    df.sort_values(dataset.timestamp_col, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:15:11.726409Z","iopub.execute_input":"2025-10-05T15:15:11.726645Z","iopub.status.idle":"2025-10-05T15:15:11.737421Z","shell.execute_reply.started":"2025-10-05T15:15:11.726600Z","shell.execute_reply":"2025-10-05T15:15:11.736738Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"if use_port_in_address:\n    df[dataset.src_port_col] = df[dataset.src_port_col].astype(float).astype(int).astype(str) # to remove the decimal point\n    df[dataset.src_ip_col] = df[dataset.src_ip_col] + ':' + df[dataset.src_port_col]\n\n    df[dataset.dst_port_col] = df[dataset.dst_port_col].astype(float).astype(int).astype(str) # to remove the decimal point\n    df[dataset.dst_ip_col] = df[dataset.dst_ip_col] + ':' + df[dataset.dst_port_col]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:15:11.855802Z","iopub.execute_input":"2025-10-05T15:15:11.855974Z","iopub.status.idle":"2025-10-05T15:15:11.859943Z","shell.execute_reply.started":"2025-10-05T15:15:11.855961Z","shell.execute_reply":"2025-10-05T15:15:11.859298Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"df.head(20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:15:11.860978Z","iopub.execute_input":"2025-10-05T15:15:11.861170Z","iopub.status.idle":"2025-10-05T15:15:11.896521Z","shell.execute_reply.started":"2025-10-05T15:15:11.861155Z","shell.execute_reply":"2025-10-05T15:15:11.896022Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"    Flow ID          Src IP Src Port         Dst IP Dst Port  Protocol  \\\n0    758097  104.16.207.165    443.0   192.168.10.5  54865.0       6.0   \n1    758390   104.16.28.216     80.0   192.168.10.5  55054.0       6.0   \n2    758391   104.16.28.216     80.0   192.168.10.5  55055.0       6.0   \n3    422766   104.17.241.25    443.0  192.168.10.16  46236.0       6.0   \n4    758524  104.19.196.102    443.0   192.168.10.5  54863.0       6.0   \n5    758666   104.20.10.120    443.0   192.168.10.5  54871.0       6.0   \n6    758672   104.20.10.120    443.0   192.168.10.5  54925.0       6.0   \n7    758672   104.20.10.120    443.0   192.168.10.5  54925.0       6.0   \n8    809266   104.28.13.116    443.0   192.168.10.8   9282.0       6.0   \n9    759808  104.97.123.193    443.0   192.168.10.5  55153.0       6.0   \n10   759823  104.97.125.160    443.0   192.168.10.5  55143.0       6.0   \n11   759824  104.97.125.160    443.0   192.168.10.5  55144.0       6.0   \n12   759825  104.97.125.160    443.0   192.168.10.5  55145.0       6.0   \n13   760003   104.97.139.37    443.0   192.168.10.5  55254.0       6.0   \n14   424025   104.97.140.32     80.0  192.168.10.16  36206.0       6.0   \n15   489585   121.29.54.141    443.0  192.168.10.25  53524.0       6.0   \n16   489585   121.29.54.141    443.0  192.168.10.25  53524.0       6.0   \n17   489586   121.29.54.141    443.0  192.168.10.25  53526.0       6.0   \n18   489586   121.29.54.141    443.0  192.168.10.25  53526.0       6.0   \n19   489587   121.29.54.141    443.0  192.168.10.25  53527.0       6.0   \n\n                   Timestamp  Flow Duration  Tot Fwd Pkts  Tot Bwd Pkts  ...  \\\n0  2017-07-07 03:30:00+00:00            3.0           2.0           0.0  ...   \n1  2017-07-07 03:30:00+00:00          109.0           1.0           1.0  ...   \n2  2017-07-07 03:30:00+00:00           52.0           1.0           1.0  ...   \n3  2017-07-07 03:30:00+00:00           34.0           1.0           1.0  ...   \n4  2017-07-07 03:30:00+00:00            3.0           2.0           0.0  ...   \n5  2017-07-07 03:30:00+00:00         1022.0           2.0           0.0  ...   \n6  2017-07-07 03:30:00+00:00            4.0           2.0           0.0  ...   \n7  2017-07-07 03:30:00+00:00           42.0           1.0           1.0  ...   \n8  2017-07-07 03:30:00+00:00            4.0           2.0           0.0  ...   \n9  2017-07-07 03:30:00+00:00            4.0           2.0           0.0  ...   \n10 2017-07-07 03:30:00+00:00            3.0           2.0           0.0  ...   \n11 2017-07-07 03:30:00+00:00            1.0           2.0           0.0  ...   \n12 2017-07-07 03:30:00+00:00            4.0           2.0           0.0  ...   \n13 2017-07-07 03:30:00+00:00            3.0           3.0           0.0  ...   \n14 2017-07-07 03:30:00+00:00           54.0           1.0           1.0  ...   \n15 2017-07-07 03:30:00+00:00            1.0           2.0           0.0  ...   \n16 2017-07-07 03:30:00+00:00          154.0           1.0           1.0  ...   \n17 2017-07-07 03:30:00+00:00            1.0           2.0           0.0  ...   \n18 2017-07-07 03:30:00+00:00          118.0           1.0           1.0  ...   \n19 2017-07-07 03:30:00+00:00          239.0           1.0           1.0  ...   \n\n    Active Std  Active Max  Active Min  Idle Mean  Idle Std  Idle Max  \\\n0          0.0         0.0         0.0        0.0       0.0       0.0   \n1          0.0         0.0         0.0        0.0       0.0       0.0   \n2          0.0         0.0         0.0        0.0       0.0       0.0   \n3          0.0         0.0         0.0        0.0       0.0       0.0   \n4          0.0         0.0         0.0        0.0       0.0       0.0   \n5          0.0         0.0         0.0        0.0       0.0       0.0   \n6          0.0         0.0         0.0        0.0       0.0       0.0   \n7          0.0         0.0         0.0        0.0       0.0       0.0   \n8          0.0         0.0         0.0        0.0       0.0       0.0   \n9          0.0         0.0         0.0        0.0       0.0       0.0   \n10         0.0         0.0         0.0        0.0       0.0       0.0   \n11         0.0         0.0         0.0        0.0       0.0       0.0   \n12         0.0         0.0         0.0        0.0       0.0       0.0   \n13         0.0         0.0         0.0        0.0       0.0       0.0   \n14         0.0         0.0         0.0        0.0       0.0       0.0   \n15         0.0         0.0         0.0        0.0       0.0       0.0   \n16         0.0         0.0         0.0        0.0       0.0       0.0   \n17         0.0         0.0         0.0        0.0       0.0       0.0   \n18         0.0         0.0         0.0        0.0       0.0       0.0   \n19         0.0         0.0         0.0        0.0       0.0       0.0   \n\n    Idle Min  Label  Attack  Class  \n0        0.0      0  BENIGN      0  \n1        0.0      0  BENIGN      0  \n2        0.0      0  BENIGN      0  \n3        0.0      0  BENIGN      0  \n4        0.0      0  BENIGN      0  \n5        0.0      0  BENIGN      0  \n6        0.0      0  BENIGN      0  \n7        0.0      0  BENIGN      0  \n8        0.0      0  BENIGN      0  \n9        0.0      0  BENIGN      0  \n10       0.0      0  BENIGN      0  \n11       0.0      0  BENIGN      0  \n12       0.0      0  BENIGN      0  \n13       0.0      0  BENIGN      0  \n14       0.0      0  BENIGN      0  \n15       0.0      0  BENIGN      0  \n16       0.0      0  BENIGN      0  \n17       0.0      0  BENIGN      0  \n18       0.0      0  BENIGN      0  \n19       0.0      0  BENIGN      0  \n\n[20 rows x 85 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Flow ID</th>\n      <th>Src IP</th>\n      <th>Src Port</th>\n      <th>Dst IP</th>\n      <th>Dst Port</th>\n      <th>Protocol</th>\n      <th>Timestamp</th>\n      <th>Flow Duration</th>\n      <th>Tot Fwd Pkts</th>\n      <th>Tot Bwd Pkts</th>\n      <th>...</th>\n      <th>Active Std</th>\n      <th>Active Max</th>\n      <th>Active Min</th>\n      <th>Idle Mean</th>\n      <th>Idle Std</th>\n      <th>Idle Max</th>\n      <th>Idle Min</th>\n      <th>Label</th>\n      <th>Attack</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>758097</td>\n      <td>104.16.207.165</td>\n      <td>443.0</td>\n      <td>192.168.10.5</td>\n      <td>54865.0</td>\n      <td>6.0</td>\n      <td>2017-07-07 03:30:00+00:00</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>BENIGN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>758390</td>\n      <td>104.16.28.216</td>\n      <td>80.0</td>\n      <td>192.168.10.5</td>\n      <td>55054.0</td>\n      <td>6.0</td>\n      <td>2017-07-07 03:30:00+00:00</td>\n      <td>109.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>BENIGN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>758391</td>\n      <td>104.16.28.216</td>\n      <td>80.0</td>\n      <td>192.168.10.5</td>\n      <td>55055.0</td>\n      <td>6.0</td>\n      <td>2017-07-07 03:30:00+00:00</td>\n      <td>52.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>BENIGN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>422766</td>\n      <td>104.17.241.25</td>\n      <td>443.0</td>\n      <td>192.168.10.16</td>\n      <td>46236.0</td>\n      <td>6.0</td>\n      <td>2017-07-07 03:30:00+00:00</td>\n      <td>34.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>BENIGN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>758524</td>\n      <td>104.19.196.102</td>\n      <td>443.0</td>\n      <td>192.168.10.5</td>\n      <td>54863.0</td>\n      <td>6.0</td>\n      <td>2017-07-07 03:30:00+00:00</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>BENIGN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>758666</td>\n      <td>104.20.10.120</td>\n      <td>443.0</td>\n      <td>192.168.10.5</td>\n      <td>54871.0</td>\n      <td>6.0</td>\n      <td>2017-07-07 03:30:00+00:00</td>\n      <td>1022.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>BENIGN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>758672</td>\n      <td>104.20.10.120</td>\n      <td>443.0</td>\n      <td>192.168.10.5</td>\n      <td>54925.0</td>\n      <td>6.0</td>\n      <td>2017-07-07 03:30:00+00:00</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>BENIGN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>758672</td>\n      <td>104.20.10.120</td>\n      <td>443.0</td>\n      <td>192.168.10.5</td>\n      <td>54925.0</td>\n      <td>6.0</td>\n      <td>2017-07-07 03:30:00+00:00</td>\n      <td>42.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>BENIGN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>809266</td>\n      <td>104.28.13.116</td>\n      <td>443.0</td>\n      <td>192.168.10.8</td>\n      <td>9282.0</td>\n      <td>6.0</td>\n      <td>2017-07-07 03:30:00+00:00</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>BENIGN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>759808</td>\n      <td>104.97.123.193</td>\n      <td>443.0</td>\n      <td>192.168.10.5</td>\n      <td>55153.0</td>\n      <td>6.0</td>\n      <td>2017-07-07 03:30:00+00:00</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>BENIGN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>759823</td>\n      <td>104.97.125.160</td>\n      <td>443.0</td>\n      <td>192.168.10.5</td>\n      <td>55143.0</td>\n      <td>6.0</td>\n      <td>2017-07-07 03:30:00+00:00</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>BENIGN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>759824</td>\n      <td>104.97.125.160</td>\n      <td>443.0</td>\n      <td>192.168.10.5</td>\n      <td>55144.0</td>\n      <td>6.0</td>\n      <td>2017-07-07 03:30:00+00:00</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>BENIGN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>759825</td>\n      <td>104.97.125.160</td>\n      <td>443.0</td>\n      <td>192.168.10.5</td>\n      <td>55145.0</td>\n      <td>6.0</td>\n      <td>2017-07-07 03:30:00+00:00</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>BENIGN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>760003</td>\n      <td>104.97.139.37</td>\n      <td>443.0</td>\n      <td>192.168.10.5</td>\n      <td>55254.0</td>\n      <td>6.0</td>\n      <td>2017-07-07 03:30:00+00:00</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>BENIGN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>424025</td>\n      <td>104.97.140.32</td>\n      <td>80.0</td>\n      <td>192.168.10.16</td>\n      <td>36206.0</td>\n      <td>6.0</td>\n      <td>2017-07-07 03:30:00+00:00</td>\n      <td>54.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>BENIGN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>489585</td>\n      <td>121.29.54.141</td>\n      <td>443.0</td>\n      <td>192.168.10.25</td>\n      <td>53524.0</td>\n      <td>6.0</td>\n      <td>2017-07-07 03:30:00+00:00</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>BENIGN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>489585</td>\n      <td>121.29.54.141</td>\n      <td>443.0</td>\n      <td>192.168.10.25</td>\n      <td>53524.0</td>\n      <td>6.0</td>\n      <td>2017-07-07 03:30:00+00:00</td>\n      <td>154.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>BENIGN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>489586</td>\n      <td>121.29.54.141</td>\n      <td>443.0</td>\n      <td>192.168.10.25</td>\n      <td>53526.0</td>\n      <td>6.0</td>\n      <td>2017-07-07 03:30:00+00:00</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>BENIGN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>489586</td>\n      <td>121.29.54.141</td>\n      <td>443.0</td>\n      <td>192.168.10.25</td>\n      <td>53526.0</td>\n      <td>6.0</td>\n      <td>2017-07-07 03:30:00+00:00</td>\n      <td>118.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>BENIGN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>489587</td>\n      <td>121.29.54.141</td>\n      <td>443.0</td>\n      <td>192.168.10.25</td>\n      <td>53527.0</td>\n      <td>6.0</td>\n      <td>2017-07-07 03:30:00+00:00</td>\n      <td>239.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>BENIGN</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>20 rows √ó 85 columns</p>\n</div>"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"print(dataset.class_num_col)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:15:11.897210Z","iopub.execute_input":"2025-10-05T15:15:11.897370Z","iopub.status.idle":"2025-10-05T15:15:11.906559Z","shell.execute_reply.started":"2025-10-05T15:15:11.897357Z","shell.execute_reply":"2025-10-05T15:15:11.905910Z"}},"outputs":[{"name":"stdout","text":"Class\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"if multi_class:\n    y = df[dataset.class_num_col]\nelse:\n    y = df[dataset.label_col]\n\nif sort_timestamp:\n    X_tr, X_test, y_tr, y_test = train_test_split(\n        df, y, test_size=test_size)\n    \n    X_train, X_val, y_train, y_val = train_test_split(\n        X_tr, y_tr, test_size=validation_size)\nelse:\n    X_tr, X_test, y_tr, y_test = train_test_split(\n        df, y, test_size=test_size, random_state=13, stratify=y)\n    \n    X_train, X_val, y_train, y_val = train_test_split(\n        X_tr, y_tr, test_size=validation_size, random_state=13, stratify=y_tr)\n\ndel df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:15:12.170525Z","iopub.execute_input":"2025-10-05T15:15:12.171043Z","iopub.status.idle":"2025-10-05T15:15:20.152957Z","shell.execute_reply.started":"2025-10-05T15:15:12.171025Z","shell.execute_reply":"2025-10-05T15:15:20.152293Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"if graph_type == \"line\" and use_node_features:\n    add_centralities(df = X_train, new_path=None, graph_path=None, dataset=dataset, cn_measures=cn_measures, network_features=network_features, create_using=nx.MultiDiGraph())\n    add_centralities(df = X_val, new_path=None, graph_path=None, dataset=dataset, cn_measures=cn_measures, network_features=network_features, create_using=nx.MultiDiGraph())\n    add_centralities(df = X_test, new_path=None, graph_path=None, dataset=dataset, cn_measures=cn_measures, network_features=network_features, create_using=nx.MultiDiGraph())\n    cols_to_norm = list(set(cols_to_norm) | set(network_features))\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:15:20.154050Z","iopub.execute_input":"2025-10-05T15:15:20.154306Z","iopub.status.idle":"2025-10-05T15:15:20.159053Z","shell.execute_reply.started":"2025-10-05T15:15:20.154280Z","shell.execute_reply":"2025-10-05T15:15:20.158332Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"scaler = StandardScaler()\n\nX_train[cols_to_norm] = scaler.fit_transform(X_train[cols_to_norm])\nX_train['h'] = X_train[ cols_to_norm ].values.tolist()\n\ncols_to_drop = list(set(list(X_train.columns)) - set(list([dataset.label_col, dataset.src_ip_col, dataset.dst_ip_col, dataset.class_num_col, 'h'])))\nX_train.drop(cols_to_drop, axis=1, inplace=True)\n\nX_val[cols_to_norm] = scaler.transform(X_val[cols_to_norm])\nX_val['h'] = X_val[ cols_to_norm ].values.tolist()\nX_val.drop(cols_to_drop, axis=1, inplace=True)\n\nX_test[cols_to_norm] = scaler.transform(X_test[cols_to_norm])\nX_test['h'] = X_test[ cols_to_norm ].values.tolist()\nX_test.drop(cols_to_drop, axis=1, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:15:20.159998Z","iopub.execute_input":"2025-10-05T15:15:20.160481Z","iopub.status.idle":"2025-10-05T15:15:29.651880Z","shell.execute_reply.started":"2025-10-05T15:15:20.160457Z","shell.execute_reply":"2025-10-05T15:15:29.651080Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"if graph_type == \"window\" or graph_type == \"line\":\n\n    create_weightless_window_graph(\n        df=X_train,\n        dataset=dataset,\n        window_size=window_size,\n        line_graph=graph_type == \"line\",\n        folder_path=os.path.join(folder_path, \"training\"),\n        edge_attr= ['h', dataset.label_col, dataset.class_num_col],\n        file_type=\"pkl\")\n    \n    create_weightless_window_graph(\n        df=X_val,\n        dataset=dataset,\n        window_size=window_size,\n        line_graph=graph_type == \"line\",\n        folder_path=os.path.join(folder_path, \"validation\"),\n        edge_attr= ['h', dataset.label_col, dataset.class_num_col],\n        file_type=\"pkl\")\n    \n    create_weightless_window_graph(\n        df=X_test,\n        dataset=dataset,\n        window_size=window_size,\n        line_graph=graph_type == \"line\",\n        folder_path=os.path.join(folder_path, \"testing\"),\n        edge_attr= ['h', dataset.label_col, dataset.class_num_col],\n        file_type=\"pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:15:29.653461Z","iopub.execute_input":"2025-10-05T15:15:29.653725Z","iopub.status.idle":"2025-10-05T15:15:29.658757Z","shell.execute_reply.started":"2025-10-05T15:15:29.653699Z","shell.execute_reply":"2025-10-05T15:15:29.658074Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"if graph_type == \"flow\":\n\tos.makedirs(folder_path, exist_ok=True)\n\tprint(f\"==>> X_train.shape: {X_train.shape}\")\n\tprint(f\"==>> X_val.shape: {X_val.shape}\")\n\tprint(f\"==>> X_test.shape: {X_test.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-10-05T15:15:29.659384Z","iopub.execute_input":"2025-10-05T15:15:29.659544Z","iopub.status.idle":"2025-10-05T15:15:29.670466Z","shell.execute_reply.started":"2025-10-05T15:15:29.659530Z","shell.execute_reply":"2025-10-05T15:15:29.669784Z"},"trusted":true},"outputs":[{"name":"stdout","text":"==>> X_train.shape: (1851823, 5)\n==>> X_val.shape: (205759, 5)\n==>> X_test.shape: (228621, 5)\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"if graph_type == \"flow\":\n    graph_name = \"training_graph\"\n\n    G = nx.from_pandas_edgelist(X_train, dataset.src_ip_col, dataset.dst_ip_col, ['h',dataset.label_col, dataset.class_num_col], create_using=nx.MultiDiGraph())\n    \n    if use_node_features:\n        add_centralities_as_node_features(df=None, G=G, graph_path=None, dataset=dataset, cn_measures=cn_measures)\n        \n        for node in G.nodes():\n            centralities = []\n            for centrality in cn_measures:\n                centralities.append(G.nodes[node].get(centrality, 0)) # Default to 0 if missing\n                \n                # Combine features into a single vector\n            n_feats = np.array(centralities, dtype=np.float32)\n            \n            # Add the new feature to the node\n            G.nodes[node][\"n_feats\"] = n_feats\n            \n    # get netowrk properties\n    graph_measures = calculate_graph_measures(G, f\"{folder_path}/{graph_name}_measures.json\", verbose=True)\n    print(f\"==>> graph_measures: {graph_measures}\")\n\n    # graph_measures = calculate_graph_measures(nx.DiGraph(G), \"datasets/\" + name + \"/training_graph_simple_measures.json\", verbose=True)\n    # print(f\"==>> graph_measures: {graph_measures}\")\n\n    with open(f\"{folder_path}/{graph_name}.pkl\", \"wb\") as f:\n        pickle.dump(G, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:15:29.671268Z","iopub.execute_input":"2025-10-05T15:15:29.671527Z","iopub.status.idle":"2025-10-05T15:15:41.253999Z","shell.execute_reply.started":"2025-10-05T15:15:29.671511Z","shell.execute_reply":"2025-10-05T15:15:41.253400Z"}},"outputs":[{"name":"stdout","text":"==>> calculated degrees, in 0.03391512300004251 seconds\n==>> graph_measures: {'number_of_nodes': 16870, 'number_of_edges': 1851823, 'max_degree': 619539, 'avg_degree': 219.54036751630113, 'density': 0.006507213454155585}\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"if graph_type == \"flow\":\n    graph_name = \"validation_graph\"\n\n    G = nx.from_pandas_edgelist(X_val, dataset.src_ip_col, dataset.dst_ip_col, ['h',dataset.label_col, dataset.class_num_col], create_using=nx.MultiDiGraph())\n    \n    if use_node_features:\n        add_centralities_as_node_features(df=None, G=G, graph_path=None, dataset=dataset, cn_measures=cn_measures)\n        \n        for node in G.nodes():\n            centralities = []\n            for centrality in cn_measures:\n                centralities.append(G.nodes[node].get(centrality, 0)) # Default to 0 if missing\n                \n                # Combine features into a single vector\n            n_feats = np.array(centralities, dtype=np.float32)\n            \n            # Add the new feature to the node\n            G.nodes[node][\"n_feats\"] = n_feats\n            \n    # get netowrk properties\n    graph_measures = calculate_graph_measures(G, f\"{folder_path}/{graph_name}_measures.json\", verbose=True)\n    print(f\"==>> graph_measures: {graph_measures}\")\n\n    # graph_measures = calculate_graph_measures(nx.DiGraph(G), \"datasets/\" + name + \"/training_graph_simple_measures.json\", verbose=True)\n    # print(f\"==>> graph_measures: {graph_measures}\")\n\n    with open(f\"{folder_path}/{graph_name}.pkl\", \"wb\") as f:\n        pickle.dump(G, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:15:41.254675Z","iopub.execute_input":"2025-10-05T15:15:41.254946Z","iopub.status.idle":"2025-10-05T15:15:43.569008Z","shell.execute_reply.started":"2025-10-05T15:15:41.254923Z","shell.execute_reply":"2025-10-05T15:15:43.568300Z"}},"outputs":[{"name":"stdout","text":"==>> calculated degrees, in 0.013613691000045947 seconds\n==>> graph_measures: {'number_of_nodes': 10955, 'number_of_edges': 205759, 'max_degree': 68543, 'avg_degree': 37.56439981743496, 'density': 0.0017146430444328537}\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"if graph_type == \"flow\":\n    graph_name = \"testing_graph\"\n    \n    G = nx.from_pandas_edgelist(X_test, dataset.src_ip_col, dataset.dst_ip_col, ['h', dataset.label_col, dataset.class_num_col], create_using=nx.MultiDiGraph())\n    \n    if use_node_features:\n        add_centralities_as_node_features(df=None, G=G, graph_path=None, dataset=dataset, cn_measures=cn_measures)\n        \n        for node in G.nodes():\n            centralities = []\n            for centrality in cn_measures:\n                centralities.append(G.nodes[node].get(centrality, 0)) # Default to 0 if missing\n                \n                # Combine features into a single vector\n            n_feats = np.array(centralities, dtype=np.float32)\n            \n            # Add the new feature to the node\n            G.nodes[node][\"n_feats\"] = n_feats\n            \n    graph_measures = calculate_graph_measures(G, f\"{folder_path}/{graph_name}_measures.json\", verbose=True)\n    print(f\"==>> graph_measures: {graph_measures}\")\n    \n    # graph_measures = calculate_graph_measures(nx.DiGraph(G_test), \"datasets/\" + name + \"/testing_graph_simple_measures.json\", verbose=True)\n    # print(f\"==>> graph_measures: {graph_measures}\")\n    \n    with open(f\"{folder_path}/{graph_name}.pkl\", \"wb\") as f:\n        pickle.dump(G, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:15:43.569813Z","iopub.execute_input":"2025-10-05T15:15:43.569994Z","iopub.status.idle":"2025-10-05T15:15:44.762114Z","shell.execute_reply.started":"2025-10-05T15:15:43.569979Z","shell.execute_reply":"2025-10-05T15:15:44.761498Z"}},"outputs":[{"name":"stdout","text":"==>> calculated degrees, in 0.014158562000147867 seconds\n==>> graph_measures: {'number_of_nodes': 11395, 'number_of_edges': 228621, 'max_degree': 76019, 'avg_degree': 40.12654673102238, 'density': 0.0017608630301484281}\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"# Save all installed package versions\n!pip freeze > requirements_1_GNN.txt\n\n# Display first few lines\n!head -n 20 requirements.txt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:16:26.827996Z","iopub.execute_input":"2025-10-05T15:16:26.828566Z","iopub.status.idle":"2025-10-05T15:16:28.911706Z","shell.execute_reply.started":"2025-10-05T15:16:26.828537Z","shell.execute_reply":"2025-10-05T15:16:28.910817Z"}},"outputs":[{"name":"stdout","text":"head: cannot open 'requirements.txt' for reading: No such file or directory\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"import json\n\nprint(\"-------------------------- SAVING VARIABLES -------------------------- \")\ndata = {\n    \"dataset_path\": dataset_path,\n    \"folder_path\": folder_path,\n    \"name\": name,\n    \"g_type\": g_type\n}\n\n# Save to JSON\nwith open(\"config.json\", \"w\") as f:\n    json.dump(data, f, indent=4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:16:57.071879Z","iopub.execute_input":"2025-10-05T15:16:57.072575Z","iopub.status.idle":"2025-10-05T15:16:57.078092Z","shell.execute_reply.started":"2025-10-05T15:16:57.072545Z","shell.execute_reply":"2025-10-05T15:16:57.077434Z"}},"outputs":[{"name":"stdout","text":"-------------------------- SAVING VARIABLES -------------------------- \n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"import psutil\nimport gc\nram_gb = psutil.virtual_memory().used / 1e9\nprint(f\"Current RAM usage: {ram_gb:.2f} GB\")\ngc.collect()\nprint(\"-------------------------- CLEARING THE MEMORY -------------------------- \")\nprint(\"Clear all variables (like restarting kernel)\")\n%reset -f\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:16:57.354258Z","iopub.execute_input":"2025-10-05T15:16:57.355063Z","iopub.status.idle":"2025-10-05T15:17:03.731671Z","shell.execute_reply.started":"2025-10-05T15:16:57.355037Z","shell.execute_reply":"2025-10-05T15:17:03.731018Z"}},"outputs":[{"name":"stdout","text":"Current RAM usage: 14.13 GB\n-------------------------- CLEARING THE MEMORY -------------------------- \nClear all variables (like restarting kernel)\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"import psutil\nram_gb = psutil.virtual_memory().used / 1e9\nprint(f\"Current RAM usage: {ram_gb:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:17:03.733033Z","iopub.execute_input":"2025-10-05T15:17:03.733218Z","iopub.status.idle":"2025-10-05T15:17:03.749093Z","shell.execute_reply.started":"2025-10-05T15:17:03.733204Z","shell.execute_reply":"2025-10-05T15:17:03.748501Z"}},"outputs":[{"name":"stdout","text":"Current RAM usage: 4.78 GB\n","output_type":"stream"}],"execution_count":50},{"cell_type":"markdown","source":"<h1>TRAINNING</h1>","metadata":{}},{"cell_type":"code","source":"import json\nprint(\"-------------------------start Training-----------------------------\")\nprint(\"-------------------------- RETRIVING VARIABLES --------------------------\")\nwith open(\"config.json\", \"r\") as f:\n    config = json.load(f)\n\n# Access the values\ndataset_path = config[\"dataset_path\"]\nfolder_path = config[\"folder_path\"]\nname = config[\"name\"]\ng_type = config[\"g_type\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:17:03.749883Z","iopub.execute_input":"2025-10-05T15:17:03.750364Z","iopub.status.idle":"2025-10-05T15:17:03.759868Z","shell.execute_reply.started":"2025-10-05T15:17:03.750339Z","shell.execute_reply":"2025-10-05T15:17:03.759249Z"}},"outputs":[{"name":"stdout","text":"-------------------------start Training-----------------------------\n-------------------------- RETRIVING VARIABLES --------------------------\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"print(folder_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:17:03.761498Z","iopub.execute_input":"2025-10-05T15:17:03.761775Z","iopub.status.idle":"2025-10-05T15:17:03.770140Z","shell.execute_reply.started":"2025-10-05T15:17:03.761758Z","shell.execute_reply":"2025-10-05T15:17:03.769558Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/cic_ids_2017/flow__multi_class__unsorted\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"\n# print(folder_path)\n# print(g_type)\n# dataset_name = \"cic_ids_2017\"\n# # dataset_folder = \"/kaggle/input/cic-ids-2017-gnn\"\n# dataset_folder = \"/kaggle/input/cic-ids-2017-gnn-2\"\n# # grpah_folder=\"/kaggle/input/cic-ids-2017-gnnflow-multi-class-unsorted\"\n# graphs_folder=folder_path\n# # /kaggle/working/cic_ids_2017/flow__multi_class__n_feats__unsorted\n# # \"/kaggle/input/cic-ids-2017-gnn\"\n# # /kaggle/input/d/mohammadfleity/cic-ids-2017-gnn/cic_ids_2017.parquet\n# model_path = \"/kaggle/input/gnn-nids-wandb/pytorch/default/6/\"\ndataset_name = name\ndataset_folder = dataset_path\ngraph_folder=folder_path\nmodel_path = \"/kaggle/input/gnn-nids-wandb/pytorch/default/6/\"\nusing_wandb = False\nsave_top_k = 1\n\n\n# save_top_k = 4 save best 4 test model from the traininng based on f1_score \n# early_stopping_patience = max_epochs = 20\nearly_stopping_patience = max_epochs = 500\n# early_stopping_patience = 200 all - epochs = 500 - lr = 0.005\nlearning_rate = 0.005\nweight_decay = 0.01\nndim_out = [32, 32]\nnum_layers = 2\nnumber_neighbors = [25, 10]\ndropout = 0.5\nresidual = True\nmulti_class = True\naggregation = \"mean\"\n# aggregation = \"sum\"\n\nuse_centralities_nfeats = False\n\n# g_type = \"flow\"\n\n# if multi_class:\n#     g_type += \"__multi_class\"\n\n# if use_centralities_nfeats:\n#     g_type += \"__n_feats\"\n    \n# if sort_timestamp:\n#     g_type += \"__sorted\"\n# else:\n#     g_type += \"__unsorted\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:17:03.770903Z","iopub.execute_input":"2025-10-05T15:17:03.771126Z","iopub.status.idle":"2025-10-05T15:17:03.782422Z","shell.execute_reply.started":"2025-10-05T15:17:03.771110Z","shell.execute_reply":"2025-10-05T15:17:03.781709Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"pip uninstall -y torch torchvision torchaudio torchdata pytorch-lightning --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:17:03.783236Z","iopub.execute_input":"2025-10-05T15:17:03.783468Z","iopub.status.idle":"2025-10-05T15:17:21.105270Z","shell.execute_reply.started":"2025-10-05T15:17:03.783452Z","shell.execute_reply":"2025-10-05T15:17:21.104471Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"print(\"dgl\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:17:21.106270Z","iopub.execute_input":"2025-10-05T15:17:21.106533Z","iopub.status.idle":"2025-10-05T15:17:21.110942Z","shell.execute_reply.started":"2025-10-05T15:17:21.106509Z","shell.execute_reply":"2025-10-05T15:17:21.110399Z"}},"outputs":[{"name":"stdout","text":"dgl\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"# pip install dgl ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:17:21.111668Z","iopub.execute_input":"2025-10-05T15:17:21.111936Z","iopub.status.idle":"2025-10-05T15:17:24.498786Z","shell.execute_reply.started":"2025-10-05T15:17:21.111910Z","shell.execute_reply":"2025-10-05T15:17:24.497848Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"pip install dgl -f https://data.dgl.ai/wheels/torch-2.1/cu118/repo.html ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:17:24.499777Z","iopub.execute_input":"2025-10-05T15:17:24.500098Z","iopub.status.idle":"2025-10-05T15:19:53.759026Z","shell.execute_reply.started":"2025-10-05T15:17:24.500070Z","shell.execute_reply":"2025-10-05T15:19:53.757868Z"}},"outputs":[{"name":"stdout","text":"Looking in links: https://data.dgl.ai/wheels/torch-2.1/cu118/repo.html\nCollecting dgl\n  Downloading https://data.dgl.ai/wheels/torch-2.1/cu118/dgl-2.4.0%2Bcu118-cp311-cp311-manylinux1_x86_64.whl (528.4 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m528.4/528.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.11/dist-packages (from dgl) (3.5)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from dgl) (25.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from dgl) (2.2.3)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (7.0.0)\nRequirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (2.11.7)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from dgl) (6.0.2)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (2.32.4)\nRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (1.15.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from dgl) (4.67.1)\nCollecting torch<=2.4.0 (from dgl)\n  Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->dgl) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->dgl) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->dgl) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->dgl) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->dgl) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->dgl) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->dgl) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->dgl) (2.33.2)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->dgl) (4.14.0)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->dgl) (0.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (2025.6.15)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (3.18.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (1.13.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<=2.4.0->dgl)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<=2.4.0->dgl)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<=2.4.0->dgl)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<=2.4.0->dgl)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch<=2.4.0->dgl)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch<=2.4.0->dgl)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch<=2.4.0->dgl)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch<=2.4.0->dgl)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch<=2.4.0->dgl)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.20.5 (from torch<=2.4.0->dgl)\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch<=2.4.0->dgl)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==3.0.0 (from torch<=2.4.0->dgl)\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<=2.4.0->dgl) (12.5.82)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->dgl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->dgl) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->dgl) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->dgl) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<=2.4.0->dgl) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->dgl) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->dgl) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->dgl) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.14.0->dgl) (2024.2.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch<=2.4.0->dgl) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.14.0->dgl) (2024.2.0)\nDownloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl (797.3 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m797.3/797.3 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, dgl\n  Attempting uninstall: triton\n    Found existing installation: triton 3.2.0\n    Uninstalling triton-3.2.0:\n      Successfully uninstalled triton-3.2.0\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.4.127\n    Uninstalling nvidia-nvtx-cu12-12.4.127:\n      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.21.5\n    Uninstalling nvidia-nccl-cu12-2.21.5:\n      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\neasyocr 1.7.2 requires torchvision>=0.5, which is not installed.\ntimm 1.0.15 requires torchvision, which is not installed.\nfastai 2.7.19 requires torchvision>=0.11, which is not installed.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed dgl-2.4.0+cu118 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.0 triton-3.0.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"print(\"done_dgl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:19:53.763771Z","iopub.execute_input":"2025-10-05T15:19:53.764093Z","iopub.status.idle":"2025-10-05T15:19:53.769045Z","shell.execute_reply.started":"2025-10-05T15:19:53.764059Z","shell.execute_reply":"2025-10-05T15:19:53.768259Z"}},"outputs":[{"name":"stdout","text":"done_dgl\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"pip install torch==2.1.0+cu118 torchvision==0.16.0+cu118 torchaudio==2.1.0+cu118 -f https://download.pytorch.org/whl/torch_stable.html --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:19:53.769775Z","iopub.execute_input":"2025-10-05T15:19:53.770068Z","iopub.status.idle":"2025-10-05T15:21:27.768563Z","shell.execute_reply.started":"2025-10-05T15:19:53.770039Z","shell.execute_reply":"2025-10-05T15:21:27.767652Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m438.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m103.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"import torch\nprint(\"torch version:\", torch.__version__)\nprint(\"CUDA available?\", torch.cuda.is_available())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:21:27.769785Z","iopub.execute_input":"2025-10-05T15:21:27.770104Z","iopub.status.idle":"2025-10-05T15:21:29.644983Z","shell.execute_reply.started":"2025-10-05T15:21:27.770066Z","shell.execute_reply":"2025-10-05T15:21:29.644335Z"}},"outputs":[{"name":"stdout","text":"torch version: 2.1.0+cu118\nCUDA available? True\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"!apt update -qq 2>/dev/null >/dev/null;\n!apt install -y libcusparse11 -qq 2>/dev/null >/dev/null;\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:21:29.645729Z","iopub.execute_input":"2025-10-05T15:21:29.646226Z","iopub.status.idle":"2025-10-05T15:21:47.513203Z","shell.execute_reply.started":"2025-10-05T15:21:29.646206Z","shell.execute_reply":"2025-10-05T15:21:47.512154Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"pip install --upgrade -q pytorch-lightning==2.5.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:21:47.514464Z","iopub.execute_input":"2025-10-05T15:21:47.515138Z","iopub.status.idle":"2025-10-05T15:21:51.497728Z","shell.execute_reply.started":"2025-10-05T15:21:47.515107Z","shell.execute_reply":"2025-10-05T15:21:51.496935Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m819.4/819.4 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"!pip install --upgrade -q wandb==0.19.6","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:21:51.498902Z","iopub.execute_input":"2025-10-05T15:21:51.499181Z","iopub.status.idle":"2025-10-05T15:22:07.098042Z","shell.execute_reply.started":"2025-10-05T15:21:51.499156Z","shell.execute_reply":"2025-10-05T15:22:07.097326Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"# import wandb\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"mohammad_wandb_secret\")\n\nwandb.login(key=secret_value_0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:22:07.099163Z","iopub.execute_input":"2025-10-05T15:22:07.099451Z","iopub.status.idle":"2025-10-05T15:22:15.327143Z","shell.execute_reply.started":"2025-10-05T15:22:07.099424Z","shell.execute_reply":"2025-10-05T15:22:15.326543Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohammad-fleity\u001b[0m (\u001b[33mmohammad-fleity-lebanese-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":64},{"cell_type":"code","source":"import os\nimport json\nimport pickle\nimport time\nimport timeit\nimport importlib.util\nimport sys\nimport random\n\nimport numpy as np\n\nos.environ[\"DGLBACKEND\"] = \"pytorch\"\n\nimport torch.nn as nn\nimport torch\nimport warnings\nimport wandb\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\nseed = 42  # or any constant value\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\nrun_dtime = time.strftime(\"%Y%m%d-%H%M%S\")\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:22:15.327943Z","iopub.execute_input":"2025-10-05T15:22:15.328398Z","iopub.status.idle":"2025-10-05T15:22:20.532948Z","shell.execute_reply.started":"2025-10-05T15:22:15.328379Z","shell.execute_reply":"2025-10-05T15:22:20.532135Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/cic-ids-2017-gnn-2/df_properties.json\n/kaggle/input/cic-ids-2017-gnn-2/cic_ids_2017.parquet\n/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\n/kaggle/input/githubrepofiles/pytorch/default/1/time_efficiency.ipynb\n/kaggle/input/githubrepofiles/pytorch/default/1/.gitignore\n/kaggle/input/githubrepofiles/pytorch/default/1/prepare_graph_files.ipynb\n/kaggle/input/githubrepofiles/pytorch/default/1/main.py\n/kaggle/input/githubrepofiles/pytorch/default/1/README.md\n/kaggle/input/githubrepofiles/pytorch/default/1/dataset_properties.ipynb\n/kaggle/input/githubrepofiles/pytorch/default/1/pre_processing.ipynb\n/kaggle/input/githubrepofiles/pytorch/default/1/concat_properties.py\n/kaggle/input/githubrepofiles/pytorch/default/1/requirements.txt\n/kaggle/input/githubrepofiles/pytorch/default/1/graph_properties.ipynb\n/kaggle/input/githubrepofiles/pytorch/default/1/results_analysis.ipynb\n/kaggle/input/githubrepofiles/pytorch/default/1/testing_dfs/cic_ton_iot_5_percent.parquet\n/kaggle/input/githubrepofiles/pytorch/default/1/testing_dfs/cic_ids_2017_5_percent.parquet\n/kaggle/input/githubrepofiles/pytorch/default/1/src/lightning_model.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/utils.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/models.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/__init__.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/lightning_data.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/models old.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/models v2.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/models edge_update.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/dataset/dataset_measures.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/dataset/dataset_utils.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/dataset/features_analysis.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/dataset/ecdf.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/dataset/__init__.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/dataset/dataset_info.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/graph/graph_construction.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/graph/graph_measures.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/graph/centralities.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/graph/graph_utils.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/graph/__init__.py\n/kaggle/input/cic-ids-20117/df_properties.json\n/kaggle/input/cic-ids-20117/labels_names.pkl\n/kaggle/input/gnn-nids-wandb/pytorch/default/6/lightning_model.py\n/kaggle/input/gnn-nids-wandb/pytorch/default/6/models.py\n/kaggle/input/gnn-nids-wandb/pytorch/default/6/lightning_data.py\n/kaggle/input/gnn-nids-wandb/pytorch/default/6/dataset_info.py\n/kaggle/input/cic-ton-iot-parquet/cic_ton_iot.parquet\n/kaggle/input/cic-ids-2017-parquet/cic_ids_2017.parquet\n/kaggle/input/gnn-nids/pytorch/default/1/time_efficiency.ipynb\n/kaggle/input/gnn-nids/pytorch/default/1/.gitignore\n/kaggle/input/gnn-nids/pytorch/default/1/prepare_graph_files.ipynb\n/kaggle/input/gnn-nids/pytorch/default/1/main.py\n/kaggle/input/gnn-nids/pytorch/default/1/README.md\n/kaggle/input/gnn-nids/pytorch/default/1/dataset_properties.ipynb\n/kaggle/input/gnn-nids/pytorch/default/1/pre_processing.ipynb\n/kaggle/input/gnn-nids/pytorch/default/1/concat_properties.py\n/kaggle/input/gnn-nids/pytorch/default/1/requirements.txt\n/kaggle/input/gnn-nids/pytorch/default/1/graph_properties.ipynb\n/kaggle/input/gnn-nids/pytorch/default/1/results_analysis.ipynb\n/kaggle/input/gnn-nids/pytorch/default/1/testing_dfs/cic_ton_iot_5_percent.parquet\n/kaggle/input/gnn-nids/pytorch/default/1/testing_dfs/cic_ids_2017_5_percent.parquet\n/kaggle/input/gnn-nids/pytorch/default/1/src/lightning_model.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/utils.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/models.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/__init__.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/lightning_data.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/models old.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/models v2.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/models edge_update.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/dataset/dataset_measures.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/dataset/dataset_utils.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/dataset/features_analysis.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/dataset/ecdf.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/dataset/__init__.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/dataset/dataset_info.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/graph/graph_construction.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/graph/graph_measures.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/graph/centralities.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/graph/graph_utils.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/graph/__init__.py\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:22:20.533778Z","iopub.execute_input":"2025-10-05T15:22:20.534420Z","iopub.status.idle":"2025-10-05T15:22:20.539308Z","shell.execute_reply.started":"2025-10-05T15:22:20.534393Z","shell.execute_reply":"2025-10-05T15:22:20.538584Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/config.json\n/kaggle/working/requirements_1_GNN.txt\n/kaggle/working/cic_ids_2017/flow__multi_class__unsorted/validation_graph_measures.json\n/kaggle/working/cic_ids_2017/flow__multi_class__unsorted/testing_graph_measures.json\n/kaggle/working/cic_ids_2017/flow__multi_class__unsorted/validation_graph.pkl\n/kaggle/working/cic_ids_2017/flow__multi_class__unsorted/training_graph_measures.json\n/kaggle/working/cic_ids_2017/flow__multi_class__unsorted/testing_graph.pkl\n/kaggle/working/cic_ids_2017/flow__multi_class__unsorted/training_graph.pkl\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"# pip install torchdata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:22:20.539996Z","iopub.execute_input":"2025-10-05T15:22:20.540166Z","iopub.status.idle":"2025-10-05T15:22:20.550751Z","shell.execute_reply.started":"2025-10-05T15:22:20.540152Z","shell.execute_reply":"2025-10-05T15:22:20.550043Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:22:20.551376Z","iopub.execute_input":"2025-10-05T15:22:20.551567Z","iopub.status.idle":"2025-10-05T15:22:20.590521Z","shell.execute_reply.started":"2025-10-05T15:22:20.551552Z","shell.execute_reply":"2025-10-05T15:22:20.589851Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"# import kagglehub\n\n# # Download latest version\n# path = kagglehub.model_download(\"mortadaphdtermos/gnn-nids-wandb/pyTorch/default\")\n\n# print(\"Path to model files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:22:20.591362Z","iopub.execute_input":"2025-10-05T15:22:20.591523Z","iopub.status.idle":"2025-10-05T15:22:20.594722Z","shell.execute_reply.started":"2025-10-05T15:22:20.591509Z","shell.execute_reply":"2025-10-05T15:22:20.593892Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"# pip install torchdata==0.7.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:22:20.595438Z","iopub.execute_input":"2025-10-05T15:22:20.595720Z","iopub.status.idle":"2025-10-05T15:22:20.606406Z","shell.execute_reply.started":"2025-10-05T15:22:20.595696Z","shell.execute_reply":"2025-10-05T15:22:20.605923Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"# pip install torch.utils","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:22:20.607351Z","iopub.execute_input":"2025-10-05T15:22:20.607597Z","iopub.status.idle":"2025-10-05T15:22:20.618119Z","shell.execute_reply.started":"2025-10-05T15:22:20.607575Z","shell.execute_reply":"2025-10-05T15:22:20.617297Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"def add_lib(module_name, path):\n    spec = importlib.util.spec_from_file_location(module_name, path)\n    dataset_info = importlib.util.module_from_spec(spec)\n    sys.modules[module_name] = dataset_info\n    spec.loader.exec_module(dataset_info)\n\nadd_lib(\"dataset_info\", model_path + \"dataset_info.py\")\nadd_lib(\"lightning_data\", model_path + \"lightning_data.py\")\nadd_lib(\"lightning_model\", model_path + \"lightning_model.py\")\nadd_lib(\"models\", model_path + \"models.py\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:22:20.618971Z","iopub.execute_input":"2025-10-05T15:22:20.619208Z","iopub.status.idle":"2025-10-05T15:22:20.790487Z","shell.execute_reply.started":"2025-10-05T15:22:20.619192Z","shell.execute_reply":"2025-10-05T15:22:20.789958Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"from models import EGAT, EGCN, EGRAPHSAGE\nfrom lightning_model import GraphModel\nfrom lightning_data import GraphDataModule\nfrom dataset_info import datasets\nimport time\nimport psutil\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score, confusion_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:22:20.791173Z","iopub.execute_input":"2025-10-05T15:22:20.791375Z","iopub.status.idle":"2025-10-05T15:22:20.795399Z","shell.execute_reply.started":"2025-10-05T15:22:20.791358Z","shell.execute_reply":"2025-10-05T15:22:20.794861Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"# pip install torchmetrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:22:20.796163Z","iopub.execute_input":"2025-10-05T15:22:20.796426Z","iopub.status.idle":"2025-10-05T15:22:20.808294Z","shell.execute_reply.started":"2025-10-05T15:22:20.796404Z","shell.execute_reply":"2025-10-05T15:22:20.807644Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"# pip install torch_geometric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:22:20.808992Z","iopub.execute_input":"2025-10-05T15:22:20.809234Z","iopub.status.idle":"2025-10-05T15:22:20.818851Z","shell.execute_reply.started":"2025-10-05T15:22:20.809211Z","shell.execute_reply":"2025-10-05T15:22:20.818139Z"}},"outputs":[],"execution_count":75},{"cell_type":"markdown","source":"<h2>E_GAT</h2>","metadata":{}},{"cell_type":"code","source":"# pip install _XLAC","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:22:20.822068Z","iopub.execute_input":"2025-10-05T15:22:20.822272Z","iopub.status.idle":"2025-10-05T15:22:20.832274Z","shell.execute_reply.started":"2025-10-05T15:22:20.822256Z","shell.execute_reply":"2025-10-05T15:22:20.831534Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"# import torch_xla.core.xla_model as xm # FOR TPU ONLY\n\n# # For XLA tensors\n# xla_tensor = self.graph.edata[self.class_num_col]\n# target = xm.to_cpu(xla_tensor).detach().numpy()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:22:20.832905Z","iopub.execute_input":"2025-10-05T15:22:20.833130Z","iopub.status.idle":"2025-10-05T15:22:20.843445Z","shell.execute_reply.started":"2025-10-05T15:22:20.833109Z","shell.execute_reply":"2025-10-05T15:22:20.842724Z"}},"outputs":[],"execution_count":77},{"cell_type":"code","source":"import os\nimport pickle\nimport warnings\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom pytorch_lightning.loggers import WandbLogger\nimport numpy as np\nimport wandb\nimport csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:22:20.844181Z","iopub.execute_input":"2025-10-05T15:22:20.844374Z","iopub.status.idle":"2025-10-05T15:22:20.854902Z","shell.execute_reply.started":"2025-10-05T15:22:20.844359Z","shell.execute_reply":"2025-10-05T15:22:20.854130Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"# Save all installed package versions\n!pip freeze > requirements_2_GNN.txt\n\n# Display first few lines\n!head -n 20 requirements.txt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T15:22:20.855662Z","iopub.execute_input":"2025-10-05T15:22:20.855908Z","iopub.status.idle":"2025-10-05T15:22:22.662871Z","shell.execute_reply.started":"2025-10-05T15:22:20.855891Z","shell.execute_reply":"2025-10-05T15:22:22.662099Z"}},"outputs":[{"name":"stdout","text":"head: cannot open 'requirements.txt' for reading: No such file or directory\n","output_type":"stream"}],"execution_count":79},{"cell_type":"code","source":"warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n\ndataset = datasets[dataset_name]\n\n# Hyperparameters\naggregation = \"mean\"\nactivation = F.relu\n\ngraphs_folder = folder_path\n# if dataset_name == \"cic_ids_2017\":\n    # graphs_folder = \"/kaggle/working/cic_ids_2017/flow__multi_class__sorted\"\n\nlogs_folder = os.path.join(\"logs\", dataset.name)\nos.makedirs(logs_folder, exist_ok=True)\n\nwandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\nos.makedirs(wandb_runs_path, exist_ok=True)\n\nlabels_mapping = {0: \"Normal\", 1: \"Attack\"}\nnum_classes = 2\nif multi_class:\n    with open(\"/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\", \"rb\") as f:\n        labels_names = pickle.load(f)\n    labels_mapping = labels_names[0]\n    num_classes = len(labels_mapping)\n\ndataset_kwargs = dict(\n    use_node_features=use_centralities_nfeats,\n    multi_class=True,\n    using_masking=False,\n    masked_class=2,\n    num_workers=0,\n    label_col=dataset.label_col,\n    class_num_col=dataset.class_num_col,\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n)\n\ndata_module = GraphDataModule(graphs_folder, batch_size=1, **dataset_kwargs)\ndata_module.setup()\n\nndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\nedim = next(iter(data_module.train_dataloader())).edata[\"h\"].shape[-1]\n\nmy_models = {\n    \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes),\n    f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,residual, num_classes, num_neighbors=None, aggregation=aggregation),\n    f\"e_graphsage_{aggregation}\": EGRAPHSAGE(\n        ndim, edim, ndim_out, num_layers, activation, dropout,\n        residual, num_classes, num_neighbors=number_neighbors,\n        aggregation=aggregation\n    ),\n    f\"e_graphsage_sum_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,residual, num_classes, num_neighbors=None, aggregation=\"sum\"),\n    f\"e_graphsage_sum\": EGRAPHSAGE(\n        ndim, edim, ndim_out, num_layers, activation, dropout,\n        residual, num_classes, num_neighbors=number_neighbors,\n        aggregation=\"sum\"\n    ),\n    \"e_gat_sampling\": EGAT(\n        ndim, edim, ndim_out, num_layers, activation, dropout,\n        residual, num_classes, num_neighbors=number_neighbors\n    ),\n    \"e_gat_no_sampling\": EGAT(\n        ndim, edim, ndim_out, num_layers, activation, dropout,\n        residual, num_classes, num_neighbors=None\n    ),\n}\n\ncriterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n\nfor model_name, model in my_models.items():\n\n    config = {\n        \"run_dtime\": run_dtime,\n        \"type\": \"GNN\",\n        \"model_name\": model_name,\n        \"max_epochs\": max_epochs,\n        \"learning_rate\": learning_rate,\n        \"weight_decay\": weight_decay,\n        \"ndim_out\": ndim_out,\n        \"num_layers\": num_layers,\n        \"number_neighbors\": number_neighbors,\n        \"activation\": activation.__name__,\n        \"dropout\": dropout,\n        \"residual\": residual,\n        \"multi_class\": multi_class,\n        \"aggregation\": aggregation,\n        \"early_stopping_patience\": early_stopping_patience,\n        \"use_centralities_nfeats\": use_centralities_nfeats,\n    }\n\n    graph_model = GraphModel(\n        model, criterion, learning_rate, config, model_name,\n        labels_mapping, weight_decay=weight_decay,\n        using_wandb=using_wandb, norm=False, multi_class=True\n    )\n\n    if using_wandb:\n        wandb_logger = WandbLogger(\n            project=f\"GNN-Analysis-{dataset.name}\",  # Project Name Wandb\n            name=f\"{model_name}---{g_type}\",\n            config=config,\n            save_dir=wandb_runs_path,\n        )\n    else:\n        wandb_logger = None\n\n    f1_checkpoint_callback = ModelCheckpoint(\n        monitor=\"val_f1_score\",\n        mode=\"max\",\n        filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n        save_top_k=save_top_k,\n        verbose=False,\n    )\n\n    early_stopping_callback = EarlyStopping(\n        monitor=\"val_loss\",\n        mode=\"min\",\n        patience=early_stopping_patience,\n        verbose=False,\n    )\n\n    process = psutil.Process(os.getpid())  # current process\n    start_time = time.time()\n    start_mem = process.memory_info().rss / (1024 ** 2)  # in MB\n    trainer = pl.Trainer(\n        max_epochs=max_epochs,\n        num_sanity_val_steps=0,\n        log_every_n_steps=0,\n        callbacks=[f1_checkpoint_callback, early_stopping_callback],\n        default_root_dir=logs_folder,\n        logger=wandb_logger if using_wandb else None,\n    )\n\n    trainer.fit(graph_model, datamodule=data_module)\n    \n    test_results = []\n    print(process)\n    for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n        graph_model.test_prefix = f\"best_f1_{i}\"\n        # Start time & memory\n        results = trainer.test(graph_model, datamodule=data_module, ckpt_path=k)\n        test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n\n        # End time & memory\n        print(f\"\\n--- Test Run {i+1} ---\")\n        print(\"graph_model: \", graph_model)\n        print(\"data_module: \", data_module)\n        print(\"results: \", results)\n        logs = {\n        \"median_f1_of_best_f1\": np.median(test_results),\n        \"max_f1_of_best_f1\": np.max(test_results),\n        \"avg_f1_of_best_f1\": np.mean(test_results),\n            \n    }\n    print(\"test_results: \", test_results)\n    end_time = time.time()\n    end_mem = process.memory_info().rss / (1024 ** 2)  # in MB\n    \n    elapsed_time = end_time - start_time\n    mem_used = end_mem - start_mem\n    print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n    print(f\"Memory used: {mem_used:.2f} MB (current RSS: {end_mem:.2f} MB)\")\n    # metric_dir= \"/kaggle/working/temp\"\n    # os.makedirs(metric_dir, exist_ok=True)\n    \n    # csv_path = os.path.join(metric_dir, f\"{model_name}_metrics.csv\")\n    \n    # with open(csv_path, mode=\"w\", newline=\"\") as f:\n    #     writer = csv.writer(f)\n    #     writer.writerow([\"TEST_ACCURACY\", test_results[0]])\n    #     writer.writerow([\"MEMORY_USED\", f\"{mem_used:.2f} MB\"])\n    #     writer.writerow([\"TIME_CONSUMED\", f\"{elapsed_time:.2f} s.\"])\n    \n    # print(f\"Metrics saved to {csv_path}\")\n    metric_dir = \"/kaggle/working/temp\"\n    os.makedirs(metric_dir, exist_ok=True)\n    \n    json_path = os.path.join(metric_dir, f\"{model_name}_metrics.json\")\n    \n    # Create dataframe with a single row\n    df = pd.DataFrame([{\n        # \"model_name\": model_name,\n        \"TEST_ACCURACY\": test_results[0],\n        \"MEMORY_USED\": f\"{mem_used:.2f} MB\",\n        \"TIME_CONSUMED\": f\"{elapsed_time:.2f} s.\"\n    }])\n    \n    # Save to JSON (records = list of dicts)\n    df.to_json(json_path, orient=\"records\", indent=4)\n    \n    print(f\"Metrics saved to {json_path}\")\n\n\n    if using_wandb:\n        wandb.log(logs)\n        wandb.finish()\n    else:\n        trainer.logger.log_metrics(logs, step=trainer.global_step)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T11:34:56.206216Z","iopub.execute_input":"2025-09-20T11:34:56.206531Z","iopub.status.idle":"2025-09-20T11:39:55.956892Z","shell.execute_reply.started":"2025-09-20T11:34:56.206504Z","shell.execute_reply":"2025-09-20T11:39:55.956045Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\n\n# Your label mapping (from the print output you shared)\nlabels_map = {\n    \"0\": \"BENIGN\",\n    \"1\": \"Bot\",\n    \"2\": \"DDoS\",\n    \"3\": \"DoS GoldenEye\",\n    \"4\": \"DoS Hulk\",\n    \"5\": \"DoS Slowhttptest\",\n    \"6\": \"DoS slowloris\",\n    \"7\": \"FTP-Patator\",\n    \"8\": \"Heartbleed\",\n    \"9\": \"Infiltration\",\n    \"10\": \"PortScan\",\n    \"11\": \"SSH-Patator\",\n    \"12\": \"Web Attack \\ufffd Brute Force\",\n    \"13\": \"Web Attack \\ufffd Sql Injection\",\n    \"14\": \"Web Attack \\ufffd XSS\"\n}\ndef json_to_csv_per_file(json_folder, output_folder):\n    os.makedirs(output_folder, exist_ok=True)\n\n    for filename in os.listdir(json_folder):\n        if filename.endswith(\"_results.json\"):\n            no_ext = filename.replace(\"_results.json\", \"\")\n            # Remove the trailing \"_metrics\"\n            print(no_ext)\n            model_name=no_ext\n            metric_path = os.path.join(json_folder,f\"{no_ext}_metrics.json\")\n            print(metric_path)\n            filepath = os.path.join(json_folder, filename)\n            with open(filepath, \"r\") as f:\n                data = json.load(f)\n            rows = []\n            # --- Per-class metrics ---\n            classification_report=data[\"classification_report\"]\n            # print(classification_report)\n            if \"results_fpr_fnr\" in data and \"per_class\" in data[\"results_fpr_fnr\"]:\n                # print(data[\"results_fpr_fnr\"])\n                for cls, metrics in data[\"results_fpr_fnr\"][\"per_class\"].items():\n                    attack=labels_map.get(cls,cls)\n                    print(f\"{attack}\")\n                    metrics_d=classification_report.get(attack)\n                    row = {\n                        \"ATTACK TYPE\": attack,\n                        \"precision\": metrics_d.get(\"precision\"),\n                        \"recall\": metrics_d.get(\"recall\"),\n                        \"f1_score\": metrics_d.get(\"f1-score\"),\n                        \"support\": metrics_d.get(\"support\"),\n                        \"FPR\": metrics.get(\"FPR\"),\n                        \"FNR\": metrics.get(\"FNR\"),\n                    }\n                    # print()\n                    # print(metrics_d.get(\"precision\"))\n                    # print()\n                    rows.append(row)\n\n            df_classes = pd.DataFrame(rows)\n            weighted_avg=classification_report['weighted avg']\n            # print(data)\n            # print(data['classification_report']['accuracy'])\n            # --- Global / test metrics ---\n            with open(metric_path, \"r\") as f:\n                device_data = json.load(f)\n            print(device_data)\n            MEMORY_USED=device_data[0][\"MEMORY_USED\"]\n            TIME_CONSUMED=device_data[0][\"TIME_CONSUMED\"]\n            # print(\"Memory:\", MEMORY_USED)\n            # print(\"Time:\", TIME_CONSUMED)\n            global_metrics = {\n                'model_name':model_name,\n                \"test_weighted_f1\": data.get(\"test_weighted_f1\"),\n                'test_accuracy':data['classification_report']['accuracy'],\n                'precision': weighted_avg['precision'],\n                'recall': weighted_avg['recall'], \n                'support': weighted_avg['support'],\n                # \"test_fpr\": data.get(\"test_fpr\"),\n                # \"test_fnr\": data.get(\"test_fnr\"),\n                \"global_FPR\": data[\"results_fpr_fnr\"][\"global\"].get(\"FPR\") if \"results_fpr_fnr\" in data and \"global\" in data[\"results_fpr_fnr\"] else None,\n                \"global_FNR\": data[\"results_fpr_fnr\"][\"global\"].get(\"FNR\") if \"results_fpr_fnr\" in data and \"global\" in data[\"results_fpr_fnr\"] else None,\n                \"time_consumption\":TIME_CONSUMED,\n                \"memory_consumption\":MEMORY_USED,\n            }\n            \n            print(global_metrics)\n            df_global = pd.DataFrame([global_metrics])\n            # --- Save CSV ---\n            csv_output = os.path.join(output_folder, filename.replace(\".json\", \".csv\"))\n            with open(csv_output, \"w\") as f:\n                f.write(\"### Per-class Metrics\\n\")\n            df_classes.to_csv(csv_output, mode=\"a\", index=False)\n            with open(csv_output, \"a\") as f:\n                f.write(\"\\n### Global Metrics\\n\")\n            df_global.to_csv(csv_output, mode=\"a\", index=False)\n\n            print(f\"Saved {csv_output}\")\njson_folder = \"/kaggle/working/temp/\"\noutput_folder = \"/kaggle/working/temp/csv_results\"\njson_to_csv_per_file(json_folder, output_folder)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T12:01:27.518761Z","iopub.execute_input":"2025-09-20T12:01:27.519683Z","iopub.status.idle":"2025-09-20T12:01:27.547164Z","shell.execute_reply.started":"2025-09-20T12:01:27.519647Z","shell.execute_reply":"2025-09-20T12:01:27.546258Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>E_GAT Sampling</h2>","metadata":{}},{"cell_type":"code","source":"# warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n\n# dataset = datasets[dataset_name]\n\n# # Hyperparameters\n\n# activation = F.relu\n# # graphs_folder = os.path.join(dataset_folder, g_type)\n# # if dataset_name == \"cic_ids_2017\":\n# #     graphs_folder = \"/kaggle/working/cic_ids_2017/flow__multi_class__n_feats__unsorted\"\n# logs_folder = os.path.join(\"logs\", dataset.name)\n# os.makedirs(logs_folder, exist_ok=True)\n# wandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\n# os.makedirs(wandb_runs_path, exist_ok=True)\n\n# labels_mapping = {0: \"Normal\", 1: \"Attack\"}\n# num_classes = 2\n# if multi_class:\n#     # with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n#     # /kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\n#     with open(\"/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\", \"rb\") as f:\n#         labels_names = pickle.load(f)\n#     labels_mapping = labels_names[0]\n# num_classes = len(labels_mapping)\n\n# dataset_kwargs = dict(\n#     use_node_features=use_centralities_nfeats,\n#     multi_class=True,\n#     using_masking=False,\n#     masked_class=2,\n#     num_workers=0,\n#     label_col=dataset.label_col,\n#     class_num_col=dataset.class_num_col,\n#     device='cuda' if torch.cuda.is_available() else \"cpu\"\n# )\n\n# data_module = GraphDataModule(\n#     graphs_folder, batch_size=1, **dataset_kwargs)\n# data_module.setup()\n\n# ndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\n# edim = next(iter(data_module.train_dataloader())).edata['h'].shape[-1]\n\n# my_models = {\n#     # \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation,\n#     #               dropout, residual, num_classes),\n#     # f\"e_graphsage_{aggregation}\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                          residual, num_classes, num_neighbors=number_neighbors, aggregation=aggregation),\n#     # f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                                      residual, num_classes, num_neighbors=None, aggregation=aggregation),\n#     \"e_gat_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                   residual, num_classes, num_neighbors=number_neighbors),\n#     # \"e_gat_no_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes, num_neighbors=None),\n# }\n\n# criterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n\n# for model_name, model in my_models.items():\n        \n#     config = {\n#         \"run_dtime\": run_dtime,\n#         \"type\": \"GNN\",\n#         \"model_name\": model_name,\n#         \"max_epochs\": max_epochs,\n#         \"learning_rate\": learning_rate,\n#         \"weight_decay\": weight_decay,\n#         \"ndim_out\": ndim_out,\n#         \"num_layers\": num_layers,\n#         \"number_neighbors\": number_neighbors,\n#         \"activation\": activation.__name__,\n#         \"dropout\": dropout,\n#         \"residual\": residual,\n#         \"multi_class\": multi_class,\n#         \"aggregation\": aggregation,\n#         \"early_stopping_patience\": early_stopping_patience,\n#         \"use_centralities_nfeats\": use_centralities_nfeats,}\n\n#     graph_model = GraphModel(model, criterion, learning_rate, config, model_name,\n#                              labels_mapping, weight_decay=weight_decay, using_wandb=using_wandb, norm=False, multi_class=True)\n\n    \n#     if using_wandb:\n#         wandb_logger = WandbLogger(\n#             project=f\"GNN-Analysis-{dataset.name}\", # Project Name Wandb\n#             name=f\"{model_name}---{g_type}\",\n#             config=config,\n#             save_dir=wandb_runs_path\n#         )\n#     else:\n#         wandb_logger = None\n        \n#     f1_checkpoint_callback = ModelCheckpoint(\n#         monitor=\"val_f1_score\",\n#         mode=\"max\",\n#         filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n#         save_top_k=save_top_k,\n#         verbose=False,\n#     )\n#     early_stopping_callback = EarlyStopping(\n#         monitor=\"val_loss\",\n#         mode=\"min\",\n#         patience=early_stopping_patience,\n#         verbose=False,\n#     )\n\n#     trainer = pl.Trainer(\n#         max_epochs=max_epochs,\n#         num_sanity_val_steps=0,\n#         log_every_n_steps=0,\n#         callbacks=[\n#             f1_checkpoint_callback,\n#             early_stopping_callback\n#         ],\n#         default_root_dir=logs_folder,\n#         logger=wandb_logger if using_wandb else None,\n#     )\n\n#     trainer.fit(graph_model, datamodule=data_module)\n    \n#     test_results = []\n    \n#     for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n#         graph_model.test_prefix = f\"best_f1_{i}\"\n#         results = trainer.test(\n#             graph_model, datamodule=data_module, ckpt_path=k)\n#         test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n\n#     logs = {\n#         \"median_f1_of_best_f1\": np.median(test_results),\n#         \"max_f1_of_best_f1\": np.max(test_results),\n#         \"avg_f1_of_best_f1\": np.mean(test_results)\n#     }\n\n#     if using_wandb:\n#         wandb.log(logs)\n#         wandb.finish()\n#     else:\n#         trainer.logger.log_metrics(logs, step=trainer.global_step)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T11:39:55.963807Z","iopub.execute_input":"2025-09-20T11:39:55.964024Z","iopub.status.idle":"2025-09-20T11:39:56.011908Z","shell.execute_reply.started":"2025-09-20T11:39:55.964009Z","shell.execute_reply":"2025-09-20T11:39:56.011325Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>EGraphSAGE mean</h2>","metadata":{}},{"cell_type":"code","source":"# warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n\n# dataset = datasets[dataset_name]\n\n# # Hyperparameters\n\n# activation = F.relu\n# # graphs_folder = os.path.join(dataset_folder, g_type)\n# # if dataset_name == \"cic_ids_2017\":\n# #     graphs_folder = \"/kaggle/working/cic_ids_2017/flow__multi_class__n_feats__unsorted\"\n# logs_folder = os.path.join(\"logs\", dataset.name)\n# os.makedirs(logs_folder, exist_ok=True)\n# wandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\n# os.makedirs(wandb_runs_path, exist_ok=True)\n\n# labels_mapping = {0: \"Normal\", 1: \"Attack\"}\n# num_classes = 2\n# if multi_class:\n#     # with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n#     # /kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\n#     with open(\"/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\", \"rb\") as f:\n#         labels_names = pickle.load(f)\n#     labels_mapping = labels_names[0]\n# num_classes = len(labels_mapping)\n\n# dataset_kwargs = dict(\n#     use_node_features=use_centralities_nfeats,\n#     multi_class=True,\n#     using_masking=False,\n#     masked_class=2,\n#     num_workers=0,\n#     label_col=dataset.label_col,\n#     class_num_col=dataset.class_num_col,\n#     device='cuda' if torch.cuda.is_available() else \"cpu\"\n# )\n\n# data_module = GraphDataModule(\n#     graphs_folder, batch_size=1, **dataset_kwargs)\n# data_module.setup()\n\n# ndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\n# edim = next(iter(data_module.train_dataloader())).edata['h'].shape[-1]\n\n# my_models = {\n#     # \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation,\n#     #               dropout, residual, num_classes),\n#     # f\"e_graphsage_{aggregation}\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                          residual, num_classes, num_neighbors=number_neighbors, aggregation=aggregation),\n#     f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                                                          residual, num_classes, num_neighbors=None, aggregation=aggregation),\n#     # \"e_gat_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #               residual, num_classes, num_neighbors=number_neighbors),\n#     # \"e_gat_no_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes, num_neighbors=None),\n# }\n\n# criterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n\n# for model_name, model in my_models.items():\n        \n#     config = {\n#         \"run_dtime\": run_dtime,\n#         \"type\": \"GNN\",\n#         \"model_name\": model_name,\n#         \"max_epochs\": max_epochs,\n#         \"learning_rate\": learning_rate,\n#         \"weight_decay\": weight_decay,\n#         \"ndim_out\": ndim_out,\n#         \"num_layers\": num_layers,\n#         \"number_neighbors\": number_neighbors,\n#         \"activation\": activation.__name__,\n#         \"dropout\": dropout,\n#         \"residual\": residual,\n#         \"multi_class\": multi_class,\n#         \"aggregation\": aggregation,\n#         \"early_stopping_patience\": early_stopping_patience,\n#         \"use_centralities_nfeats\": use_centralities_nfeats,}\n\n#     graph_model = GraphModel(model, criterion, learning_rate, config, model_name,\n#                              labels_mapping, weight_decay=weight_decay, using_wandb=using_wandb, norm=False, multi_class=True)\n\n    \n#     if using_wandb:\n#         wandb_logger = WandbLogger(\n#             project=f\"GNN-Analysis-{dataset.name}\", # Project Name Wandb\n#             name=f\"{model_name}---{g_type}\",\n#             config=config,\n#             save_dir=wandb_runs_path\n#         )\n#     else:\n#         wandb_logger = None\n        \n#     f1_checkpoint_callback = ModelCheckpoint(\n#         monitor=\"val_f1_score\",\n#         mode=\"max\",\n#         filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n#         save_top_k=save_top_k,\n#         verbose=False,\n#     )\n#     early_stopping_callback = EarlyStopping(\n#         monitor=\"val_loss\",\n#         mode=\"min\",\n#         patience=early_stopping_patience,\n#         verbose=False,\n#     )\n\n#     trainer = pl.Trainer(\n#         max_epochs=max_epochs,\n#         num_sanity_val_steps=0,\n#         log_every_n_steps=0,\n#         callbacks=[\n#             f1_checkpoint_callback,\n#             early_stopping_callback\n#         ],\n#         default_root_dir=logs_folder,\n#         logger=wandb_logger if using_wandb else None,\n#     )\n\n#     trainer.fit(graph_model, datamodule=data_module)\n    \n#     test_results = []\n    \n#     for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n#         graph_model.test_prefix = f\"best_f1_{i}\"\n#         results = trainer.test(\n#             graph_model, datamodule=data_module, ckpt_path=k)\n#         test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n\n#     logs = {\n#         \"median_f1_of_best_f1\": np.median(test_results),\n#         \"max_f1_of_best_f1\": np.max(test_results),\n#         \"avg_f1_of_best_f1\": np.mean(test_results)\n#     }\n\n#     if using_wandb:\n#         wandb.log(logs)\n#         wandb.finish()\n#     else:\n#         trainer.logger.log_metrics(logs, step=trainer.global_step)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T11:39:56.012555Z","iopub.execute_input":"2025-09-20T11:39:56.012789Z","iopub.status.idle":"2025-09-20T11:39:56.028021Z","shell.execute_reply.started":"2025-09-20T11:39:56.012774Z","shell.execute_reply":"2025-09-20T11:39:56.027277Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>EGraphSAGE sum</h2>","metadata":{}},{"cell_type":"code","source":"# warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n\n# dataset = datasets[dataset_name]\n\n# # Hyperparameters\n# aggregation=\"sum\"\n\n# activation = F.relu\n# # graphs_folder = os.path.join(dataset_folder, g_type)\n# # if dataset_name == \"cic_ids_2017\":\n#     # graphs_folder = \"/kaggle/working/cic_ids_2017/flow__multi_class__n_feats__unsorted\"\n# logs_folder = os.path.join(\"logs\", dataset.name)\n# os.makedirs(logs_folder, exist_ok=True)\n# wandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\n# os.makedirs(wandb_runs_path, exist_ok=True)\n\n# labels_mapping = {0: \"Normal\", 1: \"Attack\"}\n# num_classes = 2\n# if multi_class:\n#     # with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n#     # /kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\n#     with open(\"/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\", \"rb\") as f:\n#         labels_names = pickle.load(f)\n#     labels_mapping = labels_names[0]\n# num_classes = len(labels_mapping)\n\n# dataset_kwargs = dict(\n#     use_node_features=use_centralities_nfeats,\n#     multi_class=True,\n#     using_masking=False,\n#     masked_class=2,\n#     num_workers=0,\n#     label_col=dataset.label_col,\n#     class_num_col=dataset.class_num_col,\n#     device='cuda' if torch.cuda.is_available() else \"cpu\"\n# )\n\n# data_module = GraphDataModule(\n#     graphs_folder, batch_size=1, **dataset_kwargs)\n# data_module.setup()\n\n# ndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\n# edim = next(iter(data_module.train_dataloader())).edata['h'].shape[-1]\n\n# my_models = {\n#     # \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation,\n#     #               dropout, residual, num_classes),\n#     # f\"e_graphsage_{aggregation}\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                          residual, num_classes, num_neighbors=number_neighbors, aggregation=aggregation),\n#     f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                                                          residual, num_classes, num_neighbors=None, aggregation=aggregation),\n#     # \"e_gat_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #               residual, num_classes, num_neighbors=number_neighbors),\n#     # \"e_gat_no_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes, num_neighbors=None),\n# }\n\n# criterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n\n# for model_name, model in my_models.items():\n        \n#     config = {\n#         \"run_dtime\": run_dtime,\n#         \"type\": \"GNN\",\n#         \"model_name\": model_name,\n#         \"max_epochs\": max_epochs,\n#         \"learning_rate\": learning_rate,\n#         \"weight_decay\": weight_decay,\n#         \"ndim_out\": ndim_out,\n#         \"num_layers\": num_layers,\n#         \"number_neighbors\": number_neighbors,\n#         \"activation\": activation.__name__,\n#         \"dropout\": dropout,\n#         \"residual\": residual,\n#         \"multi_class\": multi_class,\n#         \"aggregation\": aggregation,\n#         \"early_stopping_patience\": early_stopping_patience,\n#         \"use_centralities_nfeats\": use_centralities_nfeats,}\n\n#     graph_model = GraphModel(model, criterion, learning_rate, config, model_name,\n#                              labels_mapping, weight_decay=weight_decay, using_wandb=using_wandb, norm=False, multi_class=True)\n\n    \n#     if using_wandb:\n#         wandb_logger = WandbLogger(\n#             project=f\"GNN-Analysis-{dataset.name}\", # Project Name Wandb\n#             name=f\"{model_name}---{g_type}\",\n#             config=config,\n#             save_dir=wandb_runs_path\n#         )\n#     else:\n#         wandb_logger = None\n        \n#     f1_checkpoint_callback = ModelCheckpoint(\n#         monitor=\"val_f1_score\",\n#         mode=\"max\",\n#         filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n#         save_top_k=save_top_k,\n#         verbose=False,\n#     )\n#     early_stopping_callback = EarlyStopping(\n#         monitor=\"val_loss\",\n#         mode=\"min\",\n#         patience=early_stopping_patience,\n#         verbose=False,\n#     )\n\n#     trainer = pl.Trainer(\n#         max_epochs=max_epochs,\n#         num_sanity_val_steps=0,\n#         log_every_n_steps=0,\n#         callbacks=[\n#             f1_checkpoint_callback,\n#             early_stopping_callback\n#         ],\n#         default_root_dir=logs_folder,\n#         logger=wandb_logger if using_wandb else None,\n#     )\n\n#     trainer.fit(graph_model, datamodule=data_module)\n    \n#     test_results = []\n    \n#     for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n#         graph_model.test_prefix = f\"best_f1_{i}\"\n#         results = trainer.test(\n#             graph_model, datamodule=data_module, ckpt_path=k)\n#         test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n\n#     logs = {\n#         \"median_f1_of_best_f1\": np.median(test_results),\n#         \"max_f1_of_best_f1\": np.max(test_results),\n#         \"avg_f1_of_best_f1\": np.mean(test_results)\n#     }\n\n#     if using_wandb:\n#         wandb.log(logs)\n#         wandb.finish()\n#     else:\n#         trainer.logger.log_metrics(logs, step=trainer.global_step)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T11:39:56.028831Z","iopub.execute_input":"2025-09-20T11:39:56.029081Z","iopub.status.idle":"2025-09-20T11:39:56.041445Z","shell.execute_reply.started":"2025-09-20T11:39:56.029044Z","shell.execute_reply":"2025-09-20T11:39:56.040871Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>EGraphSAGE Sampling mean</h2>","metadata":{}},{"cell_type":"code","source":"# warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n\n# dataset = datasets[dataset_name]\n\n# # Hyperparameters\n# aggregation=\"mean\"\n\n# activation = F.relu\n# # graphs_folder = os.path.join(dataset_folder, g_type)\n# # if dataset_name == \"cic_ids_2017\":\n#     # graphs_folder = \"/kaggle/working/cic_ids_2017/flow__multi_class__n_feats__unsorted\"\n# logs_folder = os.path.join(\"logs\", dataset.name)\n# os.makedirs(logs_folder, exist_ok=True)\n# wandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\n# os.makedirs(wandb_runs_path, exist_ok=True)\n\n# labels_mapping = {0: \"Normal\", 1: \"Attack\"}\n# num_classes = 2\n# if multi_class:\n#     # with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n#     # /kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\n#     with open(\"/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\", \"rb\") as f:\n#         labels_names = pickle.load(f)\n#     labels_mapping = labels_names[0]\n# num_classes = len(labels_mapping)\n\n# dataset_kwargs = dict(\n#     use_node_features=use_centralities_nfeats,\n#     multi_class=True,\n#     using_masking=False,\n#     masked_class=2,\n#     num_workers=0,\n#     label_col=dataset.label_col,\n#     class_num_col=dataset.class_num_col,\n#     device='cuda' if torch.cuda.is_available() else \"cpu\"\n# )\n\n# data_module = GraphDataModule(\n#     graphs_folder, batch_size=1, **dataset_kwargs)\n# data_module.setup()\n\n# ndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\n# edim = next(iter(data_module.train_dataloader())).edata['h'].shape[-1]\n\n# my_models = {\n#     # \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation,\n#     #               dropout, residual, num_classes),\n#     f\"e_graphsage_{aggregation}\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                                              residual, num_classes, num_neighbors=number_neighbors, aggregation=aggregation),\n#     # f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                                      residual, num_classes, num_neighbors=None, aggregation=aggregation),\n#     # \"e_gat_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                   # residual, num_classes, num_neighbors=number_neighbors),\n#     # \"e_gat_no_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes, num_neighbors=None),\n# }\n\n# criterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n\n# for model_name, model in my_models.items():\n        \n#     config = {\n#         \"run_dtime\": run_dtime,\n#         \"type\": \"GNN\",\n#         \"model_name\": model_name,\n#         \"max_epochs\": max_epochs,\n#         \"learning_rate\": learning_rate,\n#         \"weight_decay\": weight_decay,\n#         \"ndim_out\": ndim_out,\n#         \"num_layers\": num_layers,\n#         \"number_neighbors\": number_neighbors,\n#         \"activation\": activation.__name__,\n#         \"dropout\": dropout,\n#         \"residual\": residual,\n#         \"multi_class\": multi_class,\n#         \"aggregation\": aggregation,\n#         \"early_stopping_patience\": early_stopping_patience,\n#         \"use_centralities_nfeats\": use_centralities_nfeats,}\n\n#     graph_model = GraphModel(model, criterion, learning_rate, config, model_name,\n#                              labels_mapping, weight_decay=weight_decay, using_wandb=using_wandb, norm=False, multi_class=True)\n\n    \n#     if using_wandb:\n#         wandb_logger = WandbLogger(\n#             project=f\"GNN-Analysis-{dataset.name}\", # Project Name Wandb\n#             name=f\"{model_name}---{g_type}\",\n#             config=config,\n#             save_dir=wandb_runs_path\n#         )\n#     else:\n#         wandb_logger = None\n        \n#     f1_checkpoint_callback = ModelCheckpoint(\n#         monitor=\"val_f1_score\",\n#         mode=\"max\",\n#         filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n#         save_top_k=save_top_k,\n#         verbose=False,\n#     )\n#     early_stopping_callback = EarlyStopping(\n#         monitor=\"val_loss\",\n#         mode=\"min\",\n#         patience=early_stopping_patience,\n#         verbose=False,\n#     )\n\n#     trainer = pl.Trainer(\n#         max_epochs=max_epochs,\n#         num_sanity_val_steps=0,\n#         log_every_n_steps=0,\n#         callbacks=[\n#             f1_checkpoint_callback,\n#             early_stopping_callback\n#         ],\n#         default_root_dir=logs_folder,\n#         logger=wandb_logger if using_wandb else None,\n#     )\n\n#     trainer.fit(graph_model, datamodule=data_module)\n    \n#     test_results = []\n    \n#     for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n#         graph_model.test_prefix = f\"best_f1_{i}\"\n#         results = trainer.test(\n#             graph_model, datamodule=data_module, ckpt_path=k)\n#         test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n\n#     logs = {\n#         \"median_f1_of_best_f1\": np.median(test_results),\n#         \"max_f1_of_best_f1\": np.max(test_results),\n#         \"avg_f1_of_best_f1\": np.mean(test_results)\n#     }\n\n#     if using_wandb:\n#         wandb.log(logs)\n#         wandb.finish()\n#     else:\n#         trainer.logger.log_metrics(logs, step=trainer.global_step)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T11:39:56.042274Z","iopub.execute_input":"2025-09-20T11:39:56.042541Z","iopub.status.idle":"2025-09-20T11:39:56.056353Z","shell.execute_reply.started":"2025-09-20T11:39:56.042523Z","shell.execute_reply":"2025-09-20T11:39:56.055762Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>EGraphSAGE Sampling sum</h2>","metadata":{}},{"cell_type":"code","source":"# warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n\n# dataset = datasets[dataset_name]\n\n# # Hyperparameters\n# aggregation=\"sum\"\n\n# activation = F.relu\n# # graphs_folder = os.path.join(dataset_folder, g_type)\n# # if dataset_name == \"cic_ids_2017\":\n#     # graphs_folder = \"/kaggle/working/cic_ids_2017/flow__multi_class__n_feats__unsorted\"\n# logs_folder = os.path.join(\"logs\", dataset.name)\n# os.makedirs(logs_folder, exist_ok=True)\n# wandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\n# os.makedirs(wandb_runs_path, exist_ok=True)\n\n# labels_mapping = {0: \"Normal\", 1: \"Attack\"}\n# num_classes = 2\n# if multi_class:\n#     # with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n#     # /kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\n#     with open(\"/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\", \"rb\") as f:\n#         labels_names = pickle.load(f)\n#     labels_mapping = labels_names[0]\n# num_classes = len(labels_mapping)\n\n# dataset_kwargs = dict(\n#     use_node_features=use_centralities_nfeats,\n#     multi_class=True,\n#     using_masking=False,\n#     masked_class=2,\n#     num_workers=0,\n#     label_col=dataset.label_col,\n#     class_num_col=dataset.class_num_col,\n#     device='cuda' if torch.cuda.is_available() else \"cpu\"\n# )\n\n# data_module = GraphDataModule(\n#     graphs_folder, batch_size=1, **dataset_kwargs)\n# data_module.setup()\n\n# ndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\n# edim = next(iter(data_module.train_dataloader())).edata['h'].shape[-1]\n\n# my_models = {\n#     # \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation,\n#     #               dropout, residual, num_classes),\n#     f\"e_graphsage_{aggregation}\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                                              residual, num_classes, num_neighbors=number_neighbors, aggregation=aggregation),\n#     # f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                                      residual, num_classes, num_neighbors=None, aggregation=aggregation),\n#     # \"e_gat_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                   # residual, num_classes, num_neighbors=number_neighbors),\n#     # \"e_gat_no_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes, num_neighbors=None),\n# }\n\n# criterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n\n# for model_name, model in my_models.items():\n        \n#     config = {\n#         \"run_dtime\": run_dtime,\n#         \"type\": \"GNN\",\n#         \"model_name\": model_name,\n#         \"max_epochs\": max_epochs,\n#         \"learning_rate\": learning_rate,\n#         \"weight_decay\": weight_decay,\n#         \"ndim_out\": ndim_out,\n#         \"num_layers\": num_layers,\n#         \"number_neighbors\": number_neighbors,\n#         \"activation\": activation.__name__,\n#         \"dropout\": dropout,\n#         \"residual\": residual,\n#         \"multi_class\": multi_class,\n#         \"aggregation\": aggregation,\n#         \"early_stopping_patience\": early_stopping_patience,\n#         \"use_centralities_nfeats\": use_centralities_nfeats,}\n\n#     graph_model = GraphModel(model, criterion, learning_rate, config, model_name,\n#                              labels_mapping, weight_decay=weight_decay, using_wandb=using_wandb, norm=False, multi_class=True)\n\n    \n#     if using_wandb:\n#         wandb_logger = WandbLogger(\n#             project=f\"GNN-Analysis-{dataset.name}\", # Project Name Wandb\n#             name=f\"{model_name}---{g_type}\",\n#             config=config,\n#             save_dir=wandb_runs_path\n#         )\n#     else:\n#         wandb_logger = None\n        \n#     f1_checkpoint_callback = ModelCheckpoint(\n#         monitor=\"val_f1_score\",\n#         mode=\"max\",\n#         filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n#         save_top_k=save_top_k,\n#         verbose=False,\n#     )\n#     early_stopping_callback = EarlyStopping(\n#         monitor=\"val_loss\",\n#         mode=\"min\",\n#         patience=early_stopping_patience,\n#         verbose=False,\n#     )\n\n#     trainer = pl.Trainer(\n#         max_epochs=max_epochs,\n#         num_sanity_val_steps=0,\n#         log_every_n_steps=0,\n#         callbacks=[\n#             f1_checkpoint_callback,\n#             early_stopping_callback\n#         ],\n#         default_root_dir=logs_folder,\n#         logger=wandb_logger if using_wandb else None,\n#     )\n\n#     trainer.fit(graph_model, datamodule=data_module)\n    \n#     test_results = []\n    \n#     for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n#         graph_model.test_prefix = f\"best_f1_{i}\"\n#         results = trainer.test(\n#             graph_model, datamodule=data_module, ckpt_path=k)\n#         test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n\n#     logs = {\n#         \"median_f1_of_best_f1\": np.median(test_results),\n#         \"max_f1_of_best_f1\": np.max(test_results),\n#         \"avg_f1_of_best_f1\": np.mean(test_results)\n#     }\n\n#     if using_wandb:\n#         wandb.log(logs)\n#         wandb.finish()\n#     else:\n#         trainer.logger.log_metrics(logs, step=trainer.global_step)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T11:39:56.057187Z","iopub.execute_input":"2025-09-20T11:39:56.057438Z","iopub.status.idle":"2025-09-20T11:39:56.070525Z","shell.execute_reply.started":"2025-09-20T11:39:56.057417Z","shell.execute_reply":"2025-09-20T11:39:56.069984Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>E_GCN</h2>","metadata":{}},{"cell_type":"code","source":"# warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n\n# dataset = datasets[dataset_name]\n\n# # Hyperparameters\n# aggregation=\"mean\"\n\n# activation = F.relu\n# # graphs_folder = os.path.join(dataset_folder, g_type)\n# # if dataset_name == \"cic_ids_2017\":\n# #     graphs_folder = \"/kaggle/working/cic_ids_2017/flow__multi_class__n_feats__unsorted\"\n# logs_folder = os.path.join(\"logs\", dataset.name)\n# os.makedirs(logs_folder, exist_ok=True)\n# wandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\n# os.makedirs(wandb_runs_path, exist_ok=True)\n\n# labels_mapping = {0: \"Normal\", 1: \"Attack\"}\n# num_classes = 2\n# if multi_class:\n#     # with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n#     # /kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\n#     with open(\"/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\", \"rb\") as f:\n#         labels_names = pickle.load(f)\n#     labels_mapping = labels_names[0]\n# num_classes = len(labels_mapping)\n\n# dataset_kwargs = dict(\n#     use_node_features=use_centralities_nfeats,\n#     multi_class=True,\n#     using_masking=False,\n#     masked_class=2,\n#     num_workers=0,\n#     label_col=dataset.label_col,\n#     class_num_col=dataset.class_num_col,\n#     device='cuda' if torch.cuda.is_available() else \"cpu\"\n# )\n\n# data_module = GraphDataModule(\n#     graphs_folder, batch_size=1, **dataset_kwargs)\n# data_module.setup()\n\n# ndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\n# edim = next(iter(data_module.train_dataloader())).edata['h'].shape[-1]\n\n# my_models = {\n#     \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation,\n#                   dropout, residual, num_classes),\n#     # f\"e_graphsage_{aggregation}\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                          residual, num_classes, num_neighbors=number_neighbors, aggregation=aggregation),\n#     # f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                                      residual, num_classes, num_neighbors=None, aggregation=aggregation),\n#     # \"e_gat_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                   # residual, num_classes, num_neighbors=number_neighbors),\n#     # \"e_gat_no_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes, num_neighbors=None),\n# }\n\n# criterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n\n# for model_name, model in my_models.items():\n        \n#     config = {\n#         \"run_dtime\": run_dtime,\n#         \"type\": \"GNN\",\n#         \"model_name\": model_name,\n#         \"max_epochs\": max_epochs,\n#         \"learning_rate\": learning_rate,\n#         \"weight_decay\": weight_decay,\n#         \"ndim_out\": ndim_out,\n#         \"num_layers\": num_layers,\n#         \"number_neighbors\": number_neighbors,\n#         \"activation\": activation.__name__,\n#         \"dropout\": dropout,\n#         \"residual\": residual,\n#         \"multi_class\": multi_class,\n#         \"aggregation\": aggregation,\n#         \"early_stopping_patience\": early_stopping_patience,\n#         \"use_centralities_nfeats\": use_centralities_nfeats,}\n\n#     graph_model = GraphModel(model, criterion, learning_rate, config, model_name,\n#                              labels_mapping, weight_decay=weight_decay, using_wandb=using_wandb, norm=False, multi_class=True)\n\n    \n#     if using_wandb:\n#         wandb_logger = WandbLogger(\n#             project=f\"GNN-Analysis-{dataset.name}\", # Project Name Wandb\n#             name=f\"{model_name}---{g_type}\",\n#             config=config,\n#             save_dir=wandb_runs_path\n#         )\n#     else:\n#         wandb_logger = None\n        \n#     f1_checkpoint_callback = ModelCheckpoint(\n#         monitor=\"val_f1_score\",\n#         mode=\"max\",\n#         filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n#         save_top_k=save_top_k,\n#         verbose=False,\n#     )\n#     early_stopping_callback = EarlyStopping(\n#         monitor=\"val_loss\",\n#         mode=\"min\",\n#         patience=early_stopping_patience,\n#         verbose=False,\n#     )\n\n#     trainer = pl.Trainer(\n#         max_epochs=max_epochs,\n#         num_sanity_val_steps=0,\n#         log_every_n_steps=0,\n#         callbacks=[\n#             f1_checkpoint_callback,\n#             early_stopping_callback\n#         ],\n#         default_root_dir=logs_folder,\n#         logger=wandb_logger if using_wandb else None,\n#     )\n\n#     trainer.fit(graph_model, datamodule=data_module)\n    \n#     test_results = []\n    \n#     for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n#         graph_model.test_prefix = f\"best_f1_{i}\"\n#         results = trainer.test(\n#             graph_model, datamodule=data_module, ckpt_path=k)\n#         test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n\n#     logs = {\n#         \"median_f1_of_best_f1\": np.median(test_results),\n#         \"max_f1_of_best_f1\": np.max(test_results),\n#         \"avg_f1_of_best_f1\": np.mean(test_results)\n#     }\n\n#     if using_wandb:\n#         wandb.log(logs)\n#         wandb.finish()\n#     else:\n#         trainer.logger.log_metrics(logs, step=trainer.global_step)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T11:39:56.071315Z","iopub.execute_input":"2025-09-20T11:39:56.071711Z","iopub.status.idle":"2025-09-20T11:39:56.085650Z","shell.execute_reply.started":"2025-09-20T11:39:56.071672Z","shell.execute_reply":"2025-09-20T11:39:56.085097Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}