{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8089266,"sourceType":"datasetVersion","datasetId":4775518},{"sourceId":8089281,"sourceType":"datasetVersion","datasetId":4775527},{"sourceId":12479627,"sourceType":"datasetVersion","datasetId":7874205},{"sourceId":12479887,"sourceType":"datasetVersion","datasetId":7874406},{"sourceId":258352,"sourceType":"modelInstanceVersion","modelInstanceId":218283,"modelId":240009},{"sourceId":470644,"sourceType":"modelInstanceVersion","modelInstanceId":379672,"modelId":399557},{"sourceId":473434,"sourceType":"modelInstanceVersion","modelInstanceId":381305,"modelId":400996}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1>Creating Graphs</h1>","metadata":{}},{"cell_type":"code","source":"!pip install powerlaw","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:10:52.136777Z","iopub.execute_input":"2025-10-27T14:10:52.137424Z","iopub.status.idle":"2025-10-27T14:10:56.506674Z","shell.execute_reply.started":"2025-10-27T14:10:52.137398Z","shell.execute_reply":"2025-10-27T14:10:56.506016Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\nwith open(\"/kaggle/input/cic-ids-20117/df_properties.json\", \"r\") as f:\n    df_properties = json.load(f)\n\nprint(\"ðŸ“˜ Dataset Properties:\")\nfor k, v in df_properties.items():\n    print(f\"{k}: {v}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:10:56.508061Z","iopub.execute_input":"2025-10-27T14:10:56.508301Z","iopub.status.idle":"2025-10-27T14:10:56.517974Z","shell.execute_reply.started":"2025-10-27T14:10:56.508274Z","shell.execute_reply":"2025-10-27T14:10:56.517344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\nwith open(\"/kaggle/input/cic-ids-20117/labels_names.pkl\", \"rb\") as f:\n    labels_names = pickle.load(f)\nprint(labels_names)\nprint(\"\\nðŸ·ï¸ Label Class Mapping:\")\n# for k, v in labels_names.items():\n#     print(f\"{k}: {v}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:10:56.518637Z","iopub.execute_input":"2025-10-27T14:10:56.518843Z","iopub.status.idle":"2025-10-27T14:10:56.531513Z","shell.execute_reply.started":"2025-10-27T14:10:56.518823Z","shell.execute_reply":"2025-10-27T14:10:56.531024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install igraph\n# !pip install networkx ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:10:56.533065Z","iopub.execute_input":"2025-10-27T14:10:56.533332Z","iopub.status.idle":"2025-10-27T14:10:56.541902Z","shell.execute_reply.started":"2025-10-27T14:10:56.533299Z","shell.execute_reply":"2025-10-27T14:10:56.541344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pickle\nimport random\nimport socket\nimport struct\n\nimport sys\nimport pandas as pd\nsys.path.append(\"/kaggle/input/gnn-nids/pytorch/default/1\")\n\nimport networkx as nx\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom src.dataset.dataset_info import datasets\nfrom src.graph.graph_construction import create_weightless_window_graph\nfrom src.graph.graph_measures import calculate_graph_measures\nfrom src.graph.centralities import add_centralities, add_centralities_as_node_features\n# from local_variables import local_datasets_path\nlocal_datasets_path = \"/kaggle/input/\"\noriginal_datasets_files_path = \"/kaggle/input/cic-ids-2017-gnn-2\"\n# /kaggle/input/cic-ids-2017-gnn-2\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-10-27T14:10:56.542482Z","iopub.execute_input":"2025-10-27T14:10:56.542652Z","iopub.status.idle":"2025-10-27T14:10:57.925841Z","shell.execute_reply.started":"2025-10-27T14:10:56.542638Z","shell.execute_reply":"2025-10-27T14:10:57.925310Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"multi_class = True\n\nuse_node_features = True    #------- False == No node feature used\n\nuse_port_in_address = False\n\ngenerated_ips = False\n\ngraph_type = \"flow\"\n# graph_type = \"window\"\n# graph_type = \"line\"\n\n# window_size= 400\nwindow_size= 500\n# print(\"1. flow sorted\")\n# print(\"2. flow using node features unsorted\") \n# print(\"3. flow using node features sorted\")\nsort_timestamp = False      # --------- sorting timstamp\n\n# k_fold = None\n# k_fold = 5\n\nvalidation_size = 0.1\ntest_size = 0.1\n\ncn_measures = [\"betweenness\", \"degree\", \"pagerank\", \"closeness\", \"k_truss\"]\n# cn_measures = [\"betweenness\", \"degree\", \"closeness\"]\n\nnetwork_features = ['src_betweenness', 'dst_betweenness', 'src_degree', 'dst_degree', 'src_pagerank', 'dst_pagerank', 'src_closeness', 'dst_closeness', 'src_k_truss', 'dst_k_truss']\n# network_features = ['src_betweenness', 'dst_betweenness', 'src_degree', 'dst_degree', 'src_pagerank', 'dst_pagerank']\nGTP=graph_type+\" \"\nif use_node_features:\n    GTP+=\"using NODE FEATURES \"\nif sort_timestamp:\n    GTP+=\"Sorted \"\nelse:\n    GTP+=\"Unsorted.\"\nprint(GTP)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:24:03.244768Z","iopub.execute_input":"2025-10-27T14:24:03.244964Z","iopub.status.idle":"2025-10-27T14:24:03.255451Z","shell.execute_reply.started":"2025-10-27T14:24:03.244946Z","shell.execute_reply":"2025-10-27T14:24:03.254707Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# name = \"cic_ton_iot_5_percent\"\n# name = \"cic_ton_iot\"\n# name = \"cic_ids_2017_5_percent\"\nname = \"cic_ids_2017\"\n# name = \"cic_bot_iot\"\n# name = \"cic_ton_iot_modified\"\n# name = \"nf_ton_iotv2_modified\"\n# name = \"ccd_inid_modified\"\n# name = \"nf_uq_nids_modified\"\n# name = \"edge_iiot\"\n# name = \"nf_cse_cic_ids2018\"\n# name = \"nf_bot_iotv2\"\n# name = \"nf_uq_nids\"\n# name = \"x_iiot\"\n\ndataset = datasets[name]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:10:57.933282Z","iopub.execute_input":"2025-10-27T14:10:57.933519Z","iopub.status.idle":"2025-10-27T14:10:57.952327Z","shell.execute_reply.started":"2025-10-27T14:10:57.933503Z","shell.execute_reply":"2025-10-27T14:10:57.951793Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"g_type = \"\"\nif graph_type == \"flow\":\n    g_type = \"flow\"\nelif graph_type == \"line\":\n    g_type = f\"line_graph_{window_size}\"\nelif graph_type == \"window\":\n    g_type = f\"window_graph_{window_size}\"\n    \nif multi_class:\n    g_type += \"__multi_class\"\n    \nif use_node_features:\n    g_type += \"__n_feats\"\n    \n# if k_fold:\n#     g_type += f\"__{k_fold}_fold\"\n    \nif use_port_in_address:\n    g_type += \"__ports\"\n    \nif generated_ips:\n    g_type += \"__generated_ips\"\n    \nif sort_timestamp:\n    g_type += \"__sorted\"\nelse:\n    g_type += \"__unsorted\"\n# ****************** ALTERED    \n\n\n\ndataset_path = os.path.join(local_datasets_path,name)\nfolder_path = os.path.join(dataset_path, g_type)\nif name == \"cic_ids_2017\":\n    name_ds = \"cic-ids-2017-gnn-2\"\n    dataset_path = os.path.join(local_datasets_path,name_ds)\n    # folder_path = os.path.join(local_datasets_path,name, g_type)\n    folder_path= os.path.join(\"/kaggle/working/\",name,g_type)\n\nprint(folder_path)\nprint(dataset_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:10:57.953002Z","iopub.execute_input":"2025-10-27T14:10:57.953241Z","iopub.status.idle":"2025-10-27T14:10:57.968049Z","shell.execute_reply.started":"2025-10-27T14:10:57.953215Z","shell.execute_reply":"2025-10-27T14:10:57.967381Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# if name== \"cic_ids_2017\":\n#     df=pd.read_parquet(os.path.join(dataset_path, f\"{name}.parquet\"))\n# else:\ndf = pd.read_parquet(os.path.join(dataset_path, f\"{name}.parquet\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:10:57.968663Z","iopub.execute_input":"2025-10-27T14:10:57.968906Z","iopub.status.idle":"2025-10-27T14:11:02.573848Z","shell.execute_reply.started":"2025-10-27T14:10:57.968886Z","shell.execute_reply":"2025-10-27T14:11:02.573300Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2025-10-27T14:11:02.576590Z","iopub.execute_input":"2025-10-27T14:11:02.576812Z","iopub.status.idle":"2025-10-27T14:11:02.617064Z","shell.execute_reply.started":"2025-10-27T14:11:02.576796Z","shell.execute_reply":"2025-10-27T14:11:02.616438Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cols_to_norm = list(set(list(df.columns))  - set(list([dataset.label_col, dataset.class_num_col])) - set(dataset.drop_columns)  - set(dataset.weak_columns))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:11:02.617623Z","iopub.execute_input":"2025-10-27T14:11:02.617783Z","iopub.status.idle":"2025-10-27T14:11:02.621682Z","shell.execute_reply.started":"2025-10-27T14:11:02.617771Z","shell.execute_reply":"2025-10-27T14:11:02.620927Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[dataset.label_col].value_counts()","metadata":{"execution":{"iopub.status.busy":"2025-10-27T14:11:02.622346Z","iopub.execute_input":"2025-10-27T14:11:02.622584Z","iopub.status.idle":"2025-10-27T14:11:02.652915Z","shell.execute_reply.started":"2025-10-27T14:11:02.622563Z","shell.execute_reply":"2025-10-27T14:11:02.652307Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if generated_ips:\n    df[dataset.src_ip_col] = df[dataset.src_ip_col].apply(lambda x: socket.inet_ntoa(struct.pack('>I', random.randint(0xac100001, 0xac1f0001))))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:11:02.653586Z","iopub.execute_input":"2025-10-27T14:11:02.653799Z","iopub.status.idle":"2025-10-27T14:11:02.662734Z","shell.execute_reply.started":"2025-10-27T14:11:02.653783Z","shell.execute_reply":"2025-10-27T14:11:02.662196Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if sort_timestamp:\n    # df[dataset.timestamp_col] = pd.to_datetime(df[dataset.timestamp_col].str.strip(), format=dataset.timestamp_format)\n    df.sort_values(dataset.timestamp_col, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:11:02.663452Z","iopub.execute_input":"2025-10-27T14:11:02.664089Z","iopub.status.idle":"2025-10-27T14:11:02.677154Z","shell.execute_reply.started":"2025-10-27T14:11:02.664065Z","shell.execute_reply":"2025-10-27T14:11:02.676520Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if use_port_in_address:\n    df[dataset.src_port_col] = df[dataset.src_port_col].astype(float).astype(int).astype(str) # to remove the decimal point\n    df[dataset.src_ip_col] = df[dataset.src_ip_col] + ':' + df[dataset.src_port_col]\n\n    df[dataset.dst_port_col] = df[dataset.dst_port_col].astype(float).astype(int).astype(str) # to remove the decimal point\n    df[dataset.dst_ip_col] = df[dataset.dst_ip_col] + ':' + df[dataset.dst_port_col]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:11:02.677822Z","iopub.execute_input":"2025-10-27T14:11:02.678142Z","iopub.status.idle":"2025-10-27T14:11:02.690206Z","shell.execute_reply.started":"2025-10-27T14:11:02.678121Z","shell.execute_reply":"2025-10-27T14:11:02.689682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:11:02.691076Z","iopub.execute_input":"2025-10-27T14:11:02.691332Z","iopub.status.idle":"2025-10-27T14:11:02.730437Z","shell.execute_reply.started":"2025-10-27T14:11:02.691311Z","shell.execute_reply":"2025-10-27T14:11:02.729814Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(dataset.class_num_col)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:11:02.731049Z","iopub.execute_input":"2025-10-27T14:11:02.731295Z","iopub.status.idle":"2025-10-27T14:11:02.742345Z","shell.execute_reply.started":"2025-10-27T14:11:02.731279Z","shell.execute_reply":"2025-10-27T14:11:02.741766Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if multi_class:\n    y = df[dataset.class_num_col]\nelse:\n    y = df[dataset.label_col]\n\nif sort_timestamp:\n    X_tr, X_test, y_tr, y_test = train_test_split(\n        df, y, test_size=test_size)\n    \n    X_train, X_val, y_train, y_val = train_test_split(\n        X_tr, y_tr, test_size=validation_size)\nelse:\n    X_tr, X_test, y_tr, y_test = train_test_split(\n        df, y, test_size=test_size, random_state=13, stratify=y)\n    \n    X_train, X_val, y_train, y_val = train_test_split(\n        X_tr, y_tr, test_size=validation_size, random_state=13, stratify=y_tr)\n\ndel df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:11:02.743194Z","iopub.execute_input":"2025-10-27T14:11:02.743448Z","iopub.status.idle":"2025-10-27T14:11:09.227311Z","shell.execute_reply.started":"2025-10-27T14:11:02.743427Z","shell.execute_reply":"2025-10-27T14:11:09.226714Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if graph_type == \"line\" and use_node_features:\n    add_centralities(df = X_train, new_path=None, graph_path=None, dataset=dataset, cn_measures=cn_measures, network_features=network_features, create_using=nx.MultiDiGraph())\n    add_centralities(df = X_val, new_path=None, graph_path=None, dataset=dataset, cn_measures=cn_measures, network_features=network_features, create_using=nx.MultiDiGraph())\n    add_centralities(df = X_test, new_path=None, graph_path=None, dataset=dataset, cn_measures=cn_measures, network_features=network_features, create_using=nx.MultiDiGraph())\n    cols_to_norm = list(set(cols_to_norm) | set(network_features))\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:11:09.228098Z","iopub.execute_input":"2025-10-27T14:11:09.228303Z","iopub.status.idle":"2025-10-27T14:11:09.233594Z","shell.execute_reply.started":"2025-10-27T14:11:09.228287Z","shell.execute_reply":"2025-10-27T14:11:09.232949Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaler = StandardScaler()\n\nX_train[cols_to_norm] = scaler.fit_transform(X_train[cols_to_norm])\nX_train['h'] = X_train[ cols_to_norm ].values.tolist()\n\ncols_to_drop = list(set(list(X_train.columns)) - set(list([dataset.label_col, dataset.src_ip_col, dataset.dst_ip_col, dataset.class_num_col, 'h'])))\nX_train.drop(cols_to_drop, axis=1, inplace=True)\n\nX_val[cols_to_norm] = scaler.transform(X_val[cols_to_norm])\nX_val['h'] = X_val[ cols_to_norm ].values.tolist()\nX_val.drop(cols_to_drop, axis=1, inplace=True)\n\nX_test[cols_to_norm] = scaler.transform(X_test[cols_to_norm])\nX_test['h'] = X_test[ cols_to_norm ].values.tolist()\nX_test.drop(cols_to_drop, axis=1, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:11:09.234420Z","iopub.execute_input":"2025-10-27T14:11:09.234636Z","iopub.status.idle":"2025-10-27T14:11:19.770494Z","shell.execute_reply.started":"2025-10-27T14:11:09.234620Z","shell.execute_reply":"2025-10-27T14:11:19.769817Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if graph_type == \"window\" or graph_type == \"line\":\n\n    create_weightless_window_graph(\n        df=X_train,\n        dataset=dataset,\n        window_size=window_size,\n        line_graph=graph_type == \"line\",\n        folder_path=os.path.join(folder_path, \"training\"),\n        edge_attr= ['h', dataset.label_col, dataset.class_num_col],\n        file_type=\"pkl\")\n    \n    create_weightless_window_graph(\n        df=X_val,\n        dataset=dataset,\n        window_size=window_size,\n        line_graph=graph_type == \"line\",\n        folder_path=os.path.join(folder_path, \"validation\"),\n        edge_attr= ['h', dataset.label_col, dataset.class_num_col],\n        file_type=\"pkl\")\n    \n    create_weightless_window_graph(\n        df=X_test,\n        dataset=dataset,\n        window_size=window_size,\n        line_graph=graph_type == \"line\",\n        folder_path=os.path.join(folder_path, \"testing\"),\n        edge_attr= ['h', dataset.label_col, dataset.class_num_col],\n        file_type=\"pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:11:19.771362Z","iopub.execute_input":"2025-10-27T14:11:19.771623Z","iopub.status.idle":"2025-10-27T14:11:19.777773Z","shell.execute_reply.started":"2025-10-27T14:11:19.771603Z","shell.execute_reply":"2025-10-27T14:11:19.776929Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if graph_type == \"flow\":\n\tos.makedirs(folder_path, exist_ok=True)\n\tprint(f\"==>> X_train.shape: {X_train.shape}\")\n\tprint(f\"==>> X_val.shape: {X_val.shape}\")\n\tprint(f\"==>> X_test.shape: {X_test.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-10-27T14:11:19.778669Z","iopub.execute_input":"2025-10-27T14:11:19.779206Z","iopub.status.idle":"2025-10-27T14:11:19.819523Z","shell.execute_reply.started":"2025-10-27T14:11:19.779176Z","shell.execute_reply":"2025-10-27T14:11:19.818667Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if graph_type == \"flow\":\n    graph_name = \"training_graph\"\n\n    G = nx.from_pandas_edgelist(X_train, dataset.src_ip_col, dataset.dst_ip_col, ['h',dataset.label_col, dataset.class_num_col], create_using=nx.MultiDiGraph())\n    \n    if use_node_features:\n        add_centralities_as_node_features(df=None, G=G, graph_path=None, dataset=dataset, cn_measures=cn_measures)\n        \n        for node in G.nodes():\n            centralities = []\n            for centrality in cn_measures:\n                centralities.append(G.nodes[node].get(centrality, 0)) # Default to 0 if missing\n                \n                # Combine features into a single vector\n            n_feats = np.array(centralities, dtype=np.float32)\n            \n            # Add the new feature to the node\n            G.nodes[node][\"n_feats\"] = n_feats\n            \n    # get netowrk properties\n    graph_measures = calculate_graph_measures(G, f\"{folder_path}/{graph_name}_measures.json\", verbose=True)\n    print(f\"==>> graph_measures: {graph_measures}\")\n\n    # graph_measures = calculate_graph_measures(nx.DiGraph(G), \"datasets/\" + name + \"/training_graph_simple_measures.json\", verbose=True)\n    # print(f\"==>> graph_measures: {graph_measures}\")\n\n    with open(f\"{folder_path}/{graph_name}.pkl\", \"wb\") as f:\n        pickle.dump(G, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:11:19.820447Z","iopub.execute_input":"2025-10-27T14:11:19.820718Z","iopub.status.idle":"2025-10-27T14:11:31.913115Z","shell.execute_reply.started":"2025-10-27T14:11:19.820695Z","shell.execute_reply":"2025-10-27T14:11:31.912322Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if graph_type == \"flow\":\n    graph_name = \"validation_graph\"\n\n    G = nx.from_pandas_edgelist(X_val, dataset.src_ip_col, dataset.dst_ip_col, ['h',dataset.label_col, dataset.class_num_col], create_using=nx.MultiDiGraph())\n    \n    if use_node_features:\n        add_centralities_as_node_features(df=None, G=G, graph_path=None, dataset=dataset, cn_measures=cn_measures)\n        \n        for node in G.nodes():\n            centralities = []\n            for centrality in cn_measures:\n                centralities.append(G.nodes[node].get(centrality, 0)) # Default to 0 if missing\n                \n                # Combine features into a single vector\n            n_feats = np.array(centralities, dtype=np.float32)\n            \n            # Add the new feature to the node\n            G.nodes[node][\"n_feats\"] = n_feats\n            \n    # get netowrk properties\n    graph_measures = calculate_graph_measures(G, f\"{folder_path}/{graph_name}_measures.json\", verbose=True)\n    print(f\"==>> graph_measures: {graph_measures}\")\n\n    # graph_measures = calculate_graph_measures(nx.DiGraph(G), \"datasets/\" + name + \"/training_graph_simple_measures.json\", verbose=True)\n    # print(f\"==>> graph_measures: {graph_measures}\")\n\n    with open(f\"{folder_path}/{graph_name}.pkl\", \"wb\") as f:\n        pickle.dump(G, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:11:31.914020Z","iopub.execute_input":"2025-10-27T14:11:31.914220Z","iopub.status.idle":"2025-10-27T14:11:34.272662Z","shell.execute_reply.started":"2025-10-27T14:11:31.914206Z","shell.execute_reply":"2025-10-27T14:11:34.272130Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if graph_type == \"flow\":\n    graph_name = \"testing_graph\"\n    \n    G = nx.from_pandas_edgelist(X_test, dataset.src_ip_col, dataset.dst_ip_col, ['h', dataset.label_col, dataset.class_num_col], create_using=nx.MultiDiGraph())\n    \n    if use_node_features:\n        add_centralities_as_node_features(df=None, G=G, graph_path=None, dataset=dataset, cn_measures=cn_measures)\n        \n        for node in G.nodes():\n            centralities = []\n            for centrality in cn_measures:\n                centralities.append(G.nodes[node].get(centrality, 0)) # Default to 0 if missing\n                \n                # Combine features into a single vector\n            n_feats = np.array(centralities, dtype=np.float32)\n            \n            # Add the new feature to the node\n            G.nodes[node][\"n_feats\"] = n_feats\n            \n    graph_measures = calculate_graph_measures(G, f\"{folder_path}/{graph_name}_measures.json\", verbose=True)\n    print(f\"==>> graph_measures: {graph_measures}\")\n    \n    # graph_measures = calculate_graph_measures(nx.DiGraph(G_test), \"datasets/\" + name + \"/testing_graph_simple_measures.json\", verbose=True)\n    # print(f\"==>> graph_measures: {graph_measures}\")\n    \n    with open(f\"{folder_path}/{graph_name}.pkl\", \"wb\") as f:\n        pickle.dump(G, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:11:34.273353Z","iopub.execute_input":"2025-10-27T14:11:34.273614Z","iopub.status.idle":"2025-10-27T14:11:35.499426Z","shell.execute_reply.started":"2025-10-27T14:11:34.273592Z","shell.execute_reply":"2025-10-27T14:11:35.498654Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save all installed package versions\n!pip freeze > requirements_1_GNN.txt\n\n# Display first few lines\n!head -n 20 requirements.txt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:11:35.500279Z","iopub.execute_input":"2025-10-27T14:11:35.500700Z","iopub.status.idle":"2025-10-27T14:11:37.711853Z","shell.execute_reply.started":"2025-10-27T14:11:35.500676Z","shell.execute_reply":"2025-10-27T14:11:37.711162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\nprint(\"-------------------------- SAVING VARIABLES -------------------------- \")\ndata = {\n    \"dataset_path\": dataset_path,\n    \"folder_path\": folder_path,\n    \"name\": name,\n    \"g_type\": g_type\n}\n\n# Save to JSON\nwith open(\"config.json\", \"w\") as f:\n    json.dump(data, f, indent=4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:11:37.712937Z","iopub.execute_input":"2025-10-27T14:11:37.713206Z","iopub.status.idle":"2025-10-27T14:11:37.720146Z","shell.execute_reply.started":"2025-10-27T14:11:37.713183Z","shell.execute_reply":"2025-10-27T14:11:37.719186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import psutil\nimport gc\nram_gb = psutil.virtual_memory().used / 1e9\nprint(f\"Current RAM usage: {ram_gb:.2f} GB\")\ngc.collect()\nprint(\"-------------------------- CLEARING THE MEMORY -------------------------- \")\nprint(\"Clear all variables (like restarting kernel)\")\n%reset -f\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:11:37.725508Z","iopub.execute_input":"2025-10-27T14:11:37.725766Z","iopub.status.idle":"2025-10-27T14:11:44.013391Z","shell.execute_reply.started":"2025-10-27T14:11:37.725750Z","shell.execute_reply":"2025-10-27T14:11:44.012774Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import psutil\nram_gb = psutil.virtual_memory().used / 1e9\nprint(f\"Current RAM usage: {ram_gb:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:11:44.014061Z","iopub.execute_input":"2025-10-27T14:11:44.014233Z","iopub.status.idle":"2025-10-27T14:11:44.018640Z","shell.execute_reply.started":"2025-10-27T14:11:44.014218Z","shell.execute_reply":"2025-10-27T14:11:44.018132Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h1>TRAINNING</h1>","metadata":{}},{"cell_type":"code","source":"import json\nprint(\"-------------------------start Training-----------------------------\")\nprint(\"-------------------------- RETRIVING VARIABLES --------------------------\")\nwith open(\"config.json\", \"r\") as f:\n    config = json.load(f)\n\n# Access the values\ndataset_path = config[\"dataset_path\"]\nfolder_path = config[\"folder_path\"]\nname = config[\"name\"]\ng_type = config[\"g_type\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:11:44.019368Z","iopub.execute_input":"2025-10-27T14:11:44.019597Z","iopub.status.idle":"2025-10-27T14:11:44.033431Z","shell.execute_reply.started":"2025-10-27T14:11:44.019572Z","shell.execute_reply":"2025-10-27T14:11:44.032927Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(folder_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:11:44.034121Z","iopub.execute_input":"2025-10-27T14:11:44.034324Z","iopub.status.idle":"2025-10-27T14:11:44.046475Z","shell.execute_reply.started":"2025-10-27T14:11:44.034301Z","shell.execute_reply":"2025-10-27T14:11:44.045922Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# print(folder_path)\n# print(g_type)\n# dataset_name = \"cic_ids_2017\"\n# # dataset_folder = \"/kaggle/input/cic-ids-2017-gnn\"\n# dataset_folder = \"/kaggle/input/cic-ids-2017-gnn-2\"\n# # grpah_folder=\"/kaggle/input/cic-ids-2017-gnnflow-multi-class-unsorted\"\n# graphs_folder=folder_path\n# # /kaggle/working/cic_ids_2017/flow__multi_class__n_feats__unsorted\n# # \"/kaggle/input/cic-ids-2017-gnn\"\n# # /kaggle/input/d/mohammadfleity/cic-ids-2017-gnn/cic_ids_2017.parquet\n# model_path = \"/kaggle/input/gnn-nids-wandb/pytorch/default/6/\"\ndataset_name = name\ndataset_folder = dataset_path\ngraph_folder=folder_path\nmodel_path = \"/kaggle/input/gnn-nids-wandb/pytorch/default/6/\"\nusing_wandb = False\nsave_top_k = 1\n\n\n# save_top_k = 4 save best 4 test model from the traininng based on f1_score \n# early_stopping_patience = max_epochs = 20\nearly_stopping_patience = max_epochs = 500\n# early_stopping_patience = 200 all - epochs = 500 - lr = 0.005\nlearning_rate = 0.005\nweight_decay = 0.01\nndim_out = [32, 32]\nnum_layers = 2\nnumber_neighbors = [25, 10]\ndropout = 0.5\nresidual = True\nmulti_class = True\naggregation = \"mean\"\n# aggregation = \"sum\"\n\nuse_centralities_nfeats = False\n\n# g_type = \"flow\"\n\n# if multi_class:\n#     g_type += \"__multi_class\"\n\n# if use_centralities_nfeats:\n#     g_type += \"__n_feats\"\n    \n# if sort_timestamp:\n#     g_type += \"__sorted\"\n# else:\n#     g_type += \"__unsorted\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:11:44.047327Z","iopub.execute_input":"2025-10-27T14:11:44.047820Z","iopub.status.idle":"2025-10-27T14:11:44.060370Z","shell.execute_reply.started":"2025-10-27T14:11:44.047803Z","shell.execute_reply":"2025-10-27T14:11:44.059822Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip uninstall -y torch torchvision torchaudio torchdata pytorch-lightning --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:11:44.061244Z","iopub.execute_input":"2025-10-27T14:11:44.061470Z","iopub.status.idle":"2025-10-27T14:12:03.869552Z","shell.execute_reply.started":"2025-10-27T14:11:44.061451Z","shell.execute_reply":"2025-10-27T14:12:03.868772Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"dgl\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:12:03.870682Z","iopub.execute_input":"2025-10-27T14:12:03.870962Z","iopub.status.idle":"2025-10-27T14:12:03.875121Z","shell.execute_reply.started":"2025-10-27T14:12:03.870938Z","shell.execute_reply":"2025-10-27T14:12:03.874344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pip install dgl ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:12:03.875976Z","iopub.execute_input":"2025-10-27T14:12:03.876221Z","iopub.status.idle":"2025-10-27T14:12:03.924971Z","shell.execute_reply.started":"2025-10-27T14:12:03.876199Z","shell.execute_reply":"2025-10-27T14:12:03.924203Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install dgl -f https://data.dgl.ai/wheels/torch-2.1/cu118/repo.html ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:12:03.925714Z","iopub.execute_input":"2025-10-27T14:12:03.925899Z","iopub.status.idle":"2025-10-27T14:14:47.579975Z","shell.execute_reply.started":"2025-10-27T14:12:03.925863Z","shell.execute_reply":"2025-10-27T14:14:47.579191Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"done_dgl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:14:47.580918Z","iopub.execute_input":"2025-10-27T14:14:47.581202Z","iopub.status.idle":"2025-10-27T14:14:47.585448Z","shell.execute_reply.started":"2025-10-27T14:14:47.581177Z","shell.execute_reply":"2025-10-27T14:14:47.584806Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install torch==2.1.0+cu118 torchvision==0.16.0+cu118 torchaudio==2.1.0+cu118 -f https://download.pytorch.org/whl/torch_stable.html --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:14:47.586279Z","iopub.execute_input":"2025-10-27T14:14:47.586524Z","iopub.status.idle":"2025-10-27T14:16:22.892998Z","shell.execute_reply.started":"2025-10-27T14:14:47.586505Z","shell.execute_reply":"2025-10-27T14:16:22.891941Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(\"torch version:\", torch.__version__)\nprint(\"CUDA available?\", torch.cuda.is_available())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:16:22.894194Z","iopub.execute_input":"2025-10-27T14:16:22.894456Z","iopub.status.idle":"2025-10-27T14:16:24.895005Z","shell.execute_reply.started":"2025-10-27T14:16:22.894431Z","shell.execute_reply":"2025-10-27T14:16:24.894298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!apt update -qq 2>/dev/null >/dev/null;\n!apt install -y libcusparse11 -qq 2>/dev/null >/dev/null;\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:16:24.895706Z","iopub.execute_input":"2025-10-27T14:16:24.896165Z","iopub.status.idle":"2025-10-27T14:16:46.109466Z","shell.execute_reply.started":"2025-10-27T14:16:24.896147Z","shell.execute_reply":"2025-10-27T14:16:46.108689Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install --upgrade -q pytorch-lightning==2.5.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:16:46.110471Z","iopub.execute_input":"2025-10-27T14:16:46.110703Z","iopub.status.idle":"2025-10-27T14:16:50.303492Z","shell.execute_reply.started":"2025-10-27T14:16:46.110680Z","shell.execute_reply":"2025-10-27T14:16:50.302705Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade -q wandb==0.19.6","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:16:50.304472Z","iopub.execute_input":"2025-10-27T14:16:50.304753Z","iopub.status.idle":"2025-10-27T14:17:09.198677Z","shell.execute_reply.started":"2025-10-27T14:16:50.304729Z","shell.execute_reply":"2025-10-27T14:17:09.197850Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import wandb\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"mohammad_wandb_secret\")\n\nwandb.login(key=secret_value_0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:17:09.199825Z","iopub.execute_input":"2025-10-27T14:17:09.200575Z","iopub.status.idle":"2025-10-27T14:17:18.069051Z","shell.execute_reply.started":"2025-10-27T14:17:09.200547Z","shell.execute_reply":"2025-10-27T14:17:18.068383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport pickle\nimport time\nimport timeit\nimport importlib.util\nimport sys\nimport random\n\nimport numpy as np\n\nos.environ[\"DGLBACKEND\"] = \"pytorch\"\n\nimport torch.nn as nn\nimport torch\nimport warnings\nimport wandb\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\nseed = 42  # or any constant value\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\nrun_dtime = time.strftime(\"%Y%m%d-%H%M%S\")\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:17:18.069918Z","iopub.execute_input":"2025-10-27T14:17:18.070397Z","iopub.status.idle":"2025-10-27T14:17:23.905110Z","shell.execute_reply.started":"2025-10-27T14:17:18.070378Z","shell.execute_reply":"2025-10-27T14:17:23.904449Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:17:23.905830Z","iopub.execute_input":"2025-10-27T14:17:23.906437Z","iopub.status.idle":"2025-10-27T14:17:23.911136Z","shell.execute_reply.started":"2025-10-27T14:17:23.906418Z","shell.execute_reply":"2025-10-27T14:17:23.910448Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pip install torchdata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:17:23.911866Z","iopub.execute_input":"2025-10-27T14:17:23.912132Z","iopub.status.idle":"2025-10-27T14:17:23.928483Z","shell.execute_reply.started":"2025-10-27T14:17:23.912112Z","shell.execute_reply":"2025-10-27T14:17:23.927913Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:17:23.929347Z","iopub.execute_input":"2025-10-27T14:17:23.929571Z","iopub.status.idle":"2025-10-27T14:17:23.970475Z","shell.execute_reply.started":"2025-10-27T14:17:23.929557Z","shell.execute_reply":"2025-10-27T14:17:23.969883Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import kagglehub\n\n# # Download latest version\n# path = kagglehub.model_download(\"mortadaphdtermos/gnn-nids-wandb/pyTorch/default\")\n\n# print(\"Path to model files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:17:23.971146Z","iopub.execute_input":"2025-10-27T14:17:23.971397Z","iopub.status.idle":"2025-10-27T14:17:23.974476Z","shell.execute_reply.started":"2025-10-27T14:17:23.971374Z","shell.execute_reply":"2025-10-27T14:17:23.973904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pip install torchdata==0.7.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:17:23.975184Z","iopub.execute_input":"2025-10-27T14:17:23.975413Z","iopub.status.idle":"2025-10-27T14:17:23.987867Z","shell.execute_reply.started":"2025-10-27T14:17:23.975398Z","shell.execute_reply":"2025-10-27T14:17:23.987160Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pip install torch.utils","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:17:23.988805Z","iopub.execute_input":"2025-10-27T14:17:23.989317Z","iopub.status.idle":"2025-10-27T14:17:24.000395Z","shell.execute_reply.started":"2025-10-27T14:17:23.989295Z","shell.execute_reply":"2025-10-27T14:17:23.999919Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_lib(module_name, path):\n    spec = importlib.util.spec_from_file_location(module_name, path)\n    dataset_info = importlib.util.module_from_spec(spec)\n    sys.modules[module_name] = dataset_info\n    spec.loader.exec_module(dataset_info)\n\nadd_lib(\"dataset_info\", model_path + \"dataset_info.py\")\nadd_lib(\"lightning_data\", model_path + \"lightning_data.py\")\nadd_lib(\"lightning_model\", model_path + \"lightning_model.py\")\nadd_lib(\"models\", model_path + \"models.py\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:17:24.001256Z","iopub.execute_input":"2025-10-27T14:17:24.001509Z","iopub.status.idle":"2025-10-27T14:17:24.183309Z","shell.execute_reply.started":"2025-10-27T14:17:24.001489Z","shell.execute_reply":"2025-10-27T14:17:24.182739Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from models import EGAT, EGCN, EGRAPHSAGE\nfrom lightning_model import GraphModel\nfrom lightning_data import GraphDataModule\nfrom dataset_info import datasets\nimport time\nimport psutil\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score, confusion_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:17:24.184036Z","iopub.execute_input":"2025-10-27T14:17:24.184278Z","iopub.status.idle":"2025-10-27T14:17:24.188421Z","shell.execute_reply.started":"2025-10-27T14:17:24.184257Z","shell.execute_reply":"2025-10-27T14:17:24.187900Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pip install torchmetrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:17:24.189066Z","iopub.execute_input":"2025-10-27T14:17:24.189277Z","iopub.status.idle":"2025-10-27T14:17:24.202927Z","shell.execute_reply.started":"2025-10-27T14:17:24.189256Z","shell.execute_reply":"2025-10-27T14:17:24.202307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pip install torch_geometric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:17:24.203654Z","iopub.execute_input":"2025-10-27T14:17:24.204229Z","iopub.status.idle":"2025-10-27T14:17:24.217207Z","shell.execute_reply.started":"2025-10-27T14:17:24.204206Z","shell.execute_reply":"2025-10-27T14:17:24.216590Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import threading, time, psutil\n\ntry:\n    import pynvml\n    pynvml.nvmlInit()\n    GPU_AVAILABLE = True\nexcept:\n    GPU_AVAILABLE = False\n\ndef monitor_resources(process, peaks, interval=0.2):\n    \"\"\"Monitor CPU + GPU memory usage in a background thread.\"\"\"\n    peaks[\"cpu\"] = 0.0\n    peaks[\"gpu\"] = 0.0\n    \n    handle = None\n    if GPU_AVAILABLE:\n        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n    \n    while getattr(threading.current_thread(), \"running\", True):\n        try:\n            cpu_mem = process.memory_info().rss / (1024 ** 2)\n            gpu_mem = 0.0\n            if GPU_AVAILABLE:\n                gpu_mem = pynvml.nvmlDeviceGetMemoryInfo(handle).used / (1024 ** 2)\n            peaks[\"cpu\"] = max(peaks[\"cpu\"], cpu_mem)\n            peaks[\"gpu\"] = max(peaks[\"gpu\"], gpu_mem)\n        except psutil.NoSuchProcess:\n            break\n        time.sleep(interval)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:17:24.217858Z","iopub.execute_input":"2025-10-27T14:17:24.218114Z","iopub.status.idle":"2025-10-27T14:17:24.232594Z","shell.execute_reply.started":"2025-10-27T14:17:24.218099Z","shell.execute_reply":"2025-10-27T14:17:24.232033Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>E_GAT</h2>","metadata":{}},{"cell_type":"code","source":"# pip install _XLAC","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:17:24.233218Z","iopub.execute_input":"2025-10-27T14:17:24.233445Z","iopub.status.idle":"2025-10-27T14:17:24.245245Z","shell.execute_reply.started":"2025-10-27T14:17:24.233424Z","shell.execute_reply":"2025-10-27T14:17:24.244615Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch_xla.core.xla_model as xm # FOR TPU ONLY\n\n# # For XLA tensors\n# xla_tensor = self.graph.edata[self.class_num_col]\n# target = xm.to_cpu(xla_tensor).detach().numpy()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:17:24.245848Z","iopub.execute_input":"2025-10-27T14:17:24.246081Z","iopub.status.idle":"2025-10-27T14:17:24.257829Z","shell.execute_reply.started":"2025-10-27T14:17:24.246066Z","shell.execute_reply":"2025-10-27T14:17:24.257212Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pickle\nimport warnings\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom pytorch_lightning.loggers import WandbLogger\nimport numpy as np\nimport wandb\nimport csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:17:24.258539Z","iopub.execute_input":"2025-10-27T14:17:24.258724Z","iopub.status.idle":"2025-10-27T14:17:24.272531Z","shell.execute_reply.started":"2025-10-27T14:17:24.258709Z","shell.execute_reply":"2025-10-27T14:17:24.271925Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save all installed package versions\n!pip freeze > requirements_2_GNN.txt\n\n# Display first few lines\n!head -n 20 requirements.txt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:17:24.273209Z","iopub.execute_input":"2025-10-27T14:17:24.273390Z","iopub.status.idle":"2025-10-27T14:17:26.082686Z","shell.execute_reply.started":"2025-10-27T14:17:24.273377Z","shell.execute_reply":"2025-10-27T14:17:26.081899Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n\ndataset = datasets[dataset_name]\n\n# Hyperparameters\naggregation = \"mean\"\nactivation = F.relu\n\ngraphs_folder = folder_path\n# if dataset_name == \"cic_ids_2017\":\n    # graphs_folder = \"/kaggle/working/cic_ids_2017/flow__multi_class__sorted\"\n\nlogs_folder = os.path.join(\"logs\", dataset.name)\nos.makedirs(logs_folder, exist_ok=True)\n\nwandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\nos.makedirs(wandb_runs_path, exist_ok=True)\n\nlabels_mapping = {0: \"Normal\", 1: \"Attack\"}\nnum_classes = 2\nif multi_class:\n    with open(\"/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\", \"rb\") as f:\n        labels_names = pickle.load(f)\n    labels_mapping = labels_names[0]\n    num_classes = len(labels_mapping)\n\ndataset_kwargs = dict(\n    use_node_features=use_centralities_nfeats,\n    multi_class=True,\n    using_masking=False,\n    masked_class=2,\n    num_workers=0,\n    label_col=dataset.label_col,\n    class_num_col=dataset.class_num_col,\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n)\n\ndata_module = GraphDataModule(graphs_folder, batch_size=1, **dataset_kwargs)\ndata_module.setup()\n\nndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\nedim = next(iter(data_module.train_dataloader())).edata[\"h\"].shape[-1]\n\nmy_models = {\n    \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes),\n    f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,residual, num_classes, num_neighbors=None, aggregation=aggregation),\n    f\"e_graphsage_{aggregation}\": EGRAPHSAGE(\n        ndim, edim, ndim_out, num_layers, activation, dropout,\n        residual, num_classes, num_neighbors=number_neighbors,\n        aggregation=aggregation\n    ),\n    f\"e_graphsage_sum_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,residual, num_classes, num_neighbors=None, aggregation=\"sum\"),\n    f\"e_graphsage_sum\": EGRAPHSAGE(\n        ndim, edim, ndim_out, num_layers, activation, dropout,\n        residual, num_classes, num_neighbors=number_neighbors,\n        aggregation=\"sum\"\n    ),\n    \"e_gat_sampling\": EGAT(\n        ndim, edim, ndim_out, num_layers, activation, dropout,\n        residual, num_classes, num_neighbors=number_neighbors\n    ),\n    \"e_gat_no_sampling\": EGAT(\n        ndim, edim, ndim_out, num_layers, activation, dropout,\n        residual, num_classes, num_neighbors=None\n    ),\n}\n\ncriterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n\nfor model_name, model in my_models.items():\n\n    config = {\n        \"run_dtime\": run_dtime,\n        \"type\": \"GNN\",\n        \"model_name\": model_name,\n        \"max_epochs\": max_epochs,\n        \"learning_rate\": learning_rate,\n        \"weight_decay\": weight_decay,\n        \"ndim_out\": ndim_out,\n        \"num_layers\": num_layers,\n        \"number_neighbors\": number_neighbors,\n        \"activation\": activation.__name__,\n        \"dropout\": dropout,\n        \"residual\": residual,\n        \"multi_class\": multi_class,\n        \"aggregation\": aggregation,\n        \"early_stopping_patience\": early_stopping_patience,\n        \"use_centralities_nfeats\": use_centralities_nfeats,\n    }\n\n    graph_model = GraphModel(\n        model, criterion, learning_rate, config, model_name,\n        labels_mapping, weight_decay=weight_decay,\n        using_wandb=using_wandb, norm=False, multi_class=True\n    )\n\n    if using_wandb:\n        wandb_logger = WandbLogger(\n            project=f\"GNN-Analysis-{dataset.name}\",  # Project Name Wandb\n            name=f\"{model_name}---{g_type}\",\n            config=config,\n            save_dir=wandb_runs_path,\n        )\n    else:\n        wandb_logger = None\n\n    f1_checkpoint_callback = ModelCheckpoint(\n        monitor=\"val_f1_score\",\n        mode=\"max\",\n        filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n        save_top_k=save_top_k,\n        verbose=False,\n    )\n\n    early_stopping_callback = EarlyStopping(\n        monitor=\"val_loss\",\n        mode=\"min\",\n        patience=early_stopping_patience,\n        verbose=False,\n    )\n    trainer = pl.Trainer(\n        max_epochs=max_epochs,\n        num_sanity_val_steps=0,\n        log_every_n_steps=0,\n        callbacks=[f1_checkpoint_callback, early_stopping_callback],\n        default_root_dir=logs_folder,\n        logger=wandb_logger if using_wandb else None,\n    )\n    # start_time = time.time()\n    # process = psutil.Process()\n    print(\"------------- start training ---------------------\")\n    \n    # Measure start time and memory\n    cpu_mem_used=gpu_mem_used=elapsed_time=0\n    process = psutil.Process(os.getpid())\n    cpu_mem_before = process.memory_info().rss / (1024 ** 2)\n    handle = None\n    if GPU_AVAILABLE:\n        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n    if GPU_AVAILABLE:\n        gpu_mem_before = pynvml.nvmlDeviceGetMemoryInfo(handle).used / (1024 ** 2)\n    \n    process = psutil.Process(os.getpid())\n    peaks = {}\n    \n    monitor_thread = threading.Thread(target=monitor_resources, args=(process, peaks))\n    monitor_thread.running = True\n    monitor_thread.start()\n    \n    start_time = time.time()\n    \n    trainer.fit(graph_model, datamodule=data_module)\n    \n    test_results = []\n    print(process)\n    for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):      # ---- currently 1 model is saved only \n        graph_model.test_prefix = f\"best_f1_{i}\"\n        results = trainer.test(graph_model, datamodule=data_module, ckpt_path=k)\n        # End time and memory\n        elapsed_time = time.time() - start_time\n        # Stop thread safely\n        monitor_thread.running = False\n        monitor_thread.join()\n        \n        peak_cpu = peaks.get(\"cpu\", 0.0)\n        peak_gpu = peaks.get(\"gpu\", 0.0)\n        cpu_mem_used=peak_cpu-cpu_mem_before\n        gpu_mem_used=peak_gpu-gpu_mem_before\n        \n        test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n        # End time & memory\n        print(f\"\\n--- Test Run {i+1} ---\")\n        print(\"graph_model: \", graph_model)\n        print(\"data_module: \", data_module)\n        print(\"results: \", results)\n        logs = {\n        \"median_f1_of_best_f1\": np.median(test_results),\n        \"max_f1_of_best_f1\": np.max(test_results),\n        \"avg_f1_of_best_f1\": np.mean(test_results),\n            \n    }\n    print(\"test_results: \", test_results)\n\n    metric_dir = \"/kaggle/working/temp\"\n    os.makedirs(metric_dir, exist_ok=True)\n    \n    json_path = os.path.join(metric_dir, f\"{model_name}_metrics.json\")\n    \n    # Create dataframe with a single row\n    df = pd.DataFrame([{\n        # \"model_name\": model_name,\n        \"TEST_ACCURACY\": test_results[0],\n        \"Time Consumption (s)\": elapsed_time,\n        # \"Memory Consumption (MB)\": memory_consumption_mb\n        \"CPU_Peak_MB\": peak_cpu,\n        \"GPU_Peak_MB\": peak_gpu,\n        \"cpu_mem_used\":cpu_mem_used,\n        \"gpu_mem_used\":gpu_mem_used,\n    }])\n    \n    # Save to JSON (records = list of dicts)\n    df.to_json(json_path, orient=\"records\", indent=4)\n    \n    print(f\"Metrics saved to {json_path}\")\n\n\n    if using_wandb:\n        wandb.log(logs)\n        wandb.finish()\n    else:\n        trainer.logger.log_metrics(logs, step=trainer.global_step)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:17:26.084059Z","iopub.execute_input":"2025-10-27T14:17:26.084374Z","iopub.status.idle":"2025-10-27T14:22:28.075310Z","shell.execute_reply.started":"2025-10-27T14:17:26.084340Z","shell.execute_reply":"2025-10-27T14:22:28.074544Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\n\n# Your label mapping (from the print output you shared)\nlabels_map = {\n    \"0\": \"BENIGN\",\n    \"1\": \"Bot\",\n    \"2\": \"DDoS\",\n    \"3\": \"DoS GoldenEye\",\n    \"4\": \"DoS Hulk\",\n    \"5\": \"DoS Slowhttptest\",\n    \"6\": \"DoS slowloris\",\n    \"7\": \"FTP-Patator\",\n    \"8\": \"Heartbleed\",\n    \"9\": \"Infiltration\",\n    \"10\": \"PortScan\",\n    \"11\": \"SSH-Patator\",\n    \"12\": \"Web Attack \\ufffd Brute Force\",\n    \"13\": \"Web Attack \\ufffd Sql Injection\",\n    \"14\": \"Web Attack \\ufffd XSS\"\n}\ndef json_to_csv_per_file(json_folder, output_folder):\n    os.makedirs(output_folder, exist_ok=True)\n    rows_all=[]\n    rows_m_avg=[]\n    for filename in os.listdir(json_folder):\n        if filename.endswith(\"_results.json\"):\n            no_ext = filename.replace(\"_results.json\", \"\")\n            # Remove the trailing \"_metrics\"\n            print(no_ext)\n            model_name=no_ext\n            metric_path = os.path.join(json_folder,f\"{no_ext}_metrics.json\")\n            print(metric_path)\n            filepath = os.path.join(json_folder, filename)\n            with open(filepath, \"r\") as f:\n                data = json.load(f)\n            rows = []\n            # --- Per-class metrics ---\n            classification_report=data[\"classification_report\"]\n            # print(classification_report)\n            if \"results_fpr_fnr\" in data and \"per_class\" in data[\"results_fpr_fnr\"]:\n                # print(data[\"results_fpr_fnr\"])\n                for cls, metrics in data[\"results_fpr_fnr\"][\"per_class\"].items():\n                    attack=labels_map.get(cls,cls)\n                    print(f\"{attack}\")\n                    metrics_d=classification_report.get(attack)\n                    row = {\n                        \"ATTACK TYPE\": attack,\n                        \"precision\": metrics_d.get(\"precision\"),\n                        \"recall\": metrics_d.get(\"recall\"),\n                        \"f1_score\": metrics_d.get(\"f1-score\"),\n                        \"support\": metrics_d.get(\"support\"),\n                        \"FPR\": metrics.get(\"FPR\"),\n                        \"FNR\": metrics.get(\"FNR\"),\n                    }\n                    # print(row)\n                    # print(metrics_d.get(\"precision\"))\n                    # print()\n                    rows.append(row)\n\n            df_classes = pd.DataFrame(rows)\n            weighted_avg=classification_report['weighted avg']\n            macro_avg=classification_report['macro avg']\n            # print(data)\n            # print(macro_avg)\n            # print(data['classification_report']['accuracy'])\n            # --- Global / test metrics ---\n            with open(metric_path, \"r\") as f:\n                device_data = json.load(f)\n            print(device_data)\n            elapsed_time=device_data[0][\"Time Consumption (s)\"]\n            peak_cpu=device_data[0][\"CPU_Peak_MB\"]\n            peak_gpu=device_data[0][\"GPU_Peak_MB\"]\n            cpu_mem_used=device_data[0][\"cpu_mem_used\"]\n            gpu_mem_used=device_data[0][\"gpu_mem_used\"]\n            # print(\"Memory:\", MEMORY_USED)\n            # print(\"Time:\", TIME_CONSUMED)\n            global_metrics = {\n                'model_name':model_name,\n                \"test_weighted_f1\": data.get(\"test_weighted_f1\"),\n                'test_accuracy':data['classification_report']['accuracy'],\n                'precision': weighted_avg['precision'],\n                'recall': weighted_avg['recall'], \n                'support': weighted_avg['support'],\n                # \"test_fpr\": data.get(\"test_fpr\"),\n                # \"test_fnr\": data.get(\"test_fnr\"),\n                \"global_FPR\": data[\"results_fpr_fnr\"][\"global\"].get(\"FPR\") if \"results_fpr_fnr\" in data and \"global\" in data[\"results_fpr_fnr\"] else None,\n                \"global_FNR\": data[\"results_fpr_fnr\"][\"global\"].get(\"FNR\") if \"results_fpr_fnr\" in data and \"global\" in data[\"results_fpr_fnr\"] else None,\n                # \"time_consumption\":TIME_CONSUMED,\n                # \"memory_consumption\":MEMORY_USED,\n                \"Time Consumption (s)\": elapsed_time,\n                \"CPU_Peak_MB\": peak_cpu,\n                \"GPU_Peak_MB\": peak_gpu,\n                \"cpu_mem_used\":cpu_mem_used,\n                \"gpu_mem_used\":gpu_mem_used,\n            }\n            rows_all.append(\n                {\n                'model_name':model_name,\n                \"test_weighted_f1\": data.get(\"test_weighted_f1\"),\n                'test_accuracy':data['classification_report']['accuracy'],\n                'precision': weighted_avg['precision'],\n                'recall': weighted_avg['recall'], \n                'support': weighted_avg['support'],\n                # \"test_fpr\": data.get(\"test_fpr\"),\n                # \"test_fnr\": data.get(\"test_fnr\"),\n                \"global_FPR\": data[\"results_fpr_fnr\"][\"global\"].get(\"FPR\") if \"results_fpr_fnr\" in data and \"global\" in data[\"results_fpr_fnr\"] else None,\n                \"global_FNR\": data[\"results_fpr_fnr\"][\"global\"].get(\"FNR\") if \"results_fpr_fnr\" in data and \"global\" in data[\"results_fpr_fnr\"] else None,\n                # \"time_consumption\":TIME_CONSUMED,\n                # \"memory_consumption\":MEMORY_USED,\n                \"Time Consumption (s)\": elapsed_time,\n                \"CPU_Peak_MB\": peak_cpu,\n                \"GPU_Peak_MB\": peak_gpu,\n                \"cpu_mem_used\":cpu_mem_used,\n                \"gpu_mem_used\":gpu_mem_used,\n                }\n            )\n            print(global_metrics)\n            # {'precision': 0.4393356617629928, 'recall': 0.5886290738703498, 'f1-score': 0.3932640639233801, 'support': 228621}\n            macro_metrics = {\n                'model_name':model_name,\n                'f1-score': macro_avg['f1-score'],\n                'precision': macro_avg['precision'],\n                'recall': macro_avg['recall'], \n                'support': macro_avg['support'],\n            }\n            rows_m_avg.append(\n                {\n                'model_name':model_name,\n                'f1-score': macro_avg['f1-score'],\n                'precision': macro_avg['precision'],\n                'recall': macro_avg['recall'], \n                'support': macro_avg['support'],\n            }\n            )\n            print(macro_metrics)\n            df_global = pd.DataFrame([global_metrics])\n            # --- Save CSV ---\n            csv_output = os.path.join(output_folder, filename.replace(\".json\", \".csv\"))\n            with open(csv_output, \"w\") as f:\n                f.write(\"### Per-class Metrics\\n\")\n            df_classes.to_csv(csv_output, mode=\"a\", index=False)\n            with open(csv_output, \"a\") as f:\n                f.write(\"\\n### Global Metrics\\n\")\n            df_global.to_csv(csv_output, mode=\"a\", index=False)\n\n            print(f\"Saved {csv_output}\")\n        df = pd.DataFrame(rows_all)\n    temp_dir = \"/kaggle/working/temp/csv_results/\"\n    # print(folder_path)\n    folder=os.path.join(\"/kaggle/working/\",dataset_name)\n    graph_title=folder_path.replace(f\"{folder}/\",\"\")\n    # print(graph_title)\n    # Create folder if it doesn't exist\n    if not os.path.exists(temp_dir):\n        os.makedirs(temp_dir)\n    \n    # Save CSV inside temp/\n    if dataset_name==\"cic_ton_iot\":\n        csv_path = os.path.join(temp_dir, f\"{graph_title}__GNN_BENCHMARKING_CIC_TON_IOT.csv\")\n    else:\n        csv_path = os.path.join(temp_dir, f\"{graph_title}__GNN_BENCHMARKING_CIC_IDS_2017.csv\")\n    df.to_csv(csv_path, index=False)\n    print(f\"âœ… CSV saved as {csv_path}\")\n    \n    df = pd.DataFrame(rows_m_avg)\n    temp_dir = \"/kaggle/working/temp/csv_results/\"\n    \n    # Create folder if it doesn't exist\n    if not os.path.exists(temp_dir):\n        os.makedirs(temp_dir)\n    \n    # Save CSV inside temp/\n    if dataset_name==\"cic_ton_iot\":\n        csv_path = os.path.join(temp_dir, f\"{graph_title}__GNN_macro_results_CIC_TON_IOT.csv\")\n    else:\n        csv_path = os.path.join(temp_dir, f\"{graph_title}__GNN_macro_results_CIC_IDS_2017.csv\")\n    df.to_csv(csv_path, index=False)\n    print(f\"âœ… CSV saved as {csv_path}\")\n\njson_folder = \"/kaggle/working/temp/\"\noutput_folder = \"/kaggle/working/temp/csv_results\"\njson_to_csv_per_file(json_folder, output_folder)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:22:28.076248Z","iopub.execute_input":"2025-10-27T14:22:28.076466Z","iopub.status.idle":"2025-10-27T14:22:28.109954Z","shell.execute_reply.started":"2025-10-27T14:22:28.076450Z","shell.execute_reply":"2025-10-27T14:22:28.109309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(folder_path)\nfolder=os.path.join(\"/kaggle/working/\",dataset_name)\ngraph_title=folder_path.replace(f\"{folder}/\",\"\")\nprint(graph_title)\n# /kaggle/working/cic_ids_2017/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:22:28.110727Z","iopub.execute_input":"2025-10-27T14:22:28.111475Z","iopub.status.idle":"2025-10-27T14:22:28.116372Z","shell.execute_reply.started":"2025-10-27T14:22:28.111451Z","shell.execute_reply":"2025-10-27T14:22:28.115579Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>E_GAT Sampling</h2>","metadata":{}},{"cell_type":"code","source":"# warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n\n# dataset = datasets[dataset_name]\n\n# # Hyperparameters\n\n# activation = F.relu\n# # graphs_folder = os.path.join(dataset_folder, g_type)\n# # if dataset_name == \"cic_ids_2017\":\n# #     graphs_folder = \"/kaggle/working/cic_ids_2017/flow__multi_class__n_feats__unsorted\"\n# logs_folder = os.path.join(\"logs\", dataset.name)\n# os.makedirs(logs_folder, exist_ok=True)\n# wandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\n# os.makedirs(wandb_runs_path, exist_ok=True)\n\n# labels_mapping = {0: \"Normal\", 1: \"Attack\"}\n# num_classes = 2\n# if multi_class:\n#     # with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n#     # /kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\n#     with open(\"/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\", \"rb\") as f:\n#         labels_names = pickle.load(f)\n#     labels_mapping = labels_names[0]\n# num_classes = len(labels_mapping)\n\n# dataset_kwargs = dict(\n#     use_node_features=use_centralities_nfeats,\n#     multi_class=True,\n#     using_masking=False,\n#     masked_class=2,\n#     num_workers=0,\n#     label_col=dataset.label_col,\n#     class_num_col=dataset.class_num_col,\n#     device='cuda' if torch.cuda.is_available() else \"cpu\"\n# )\n\n# data_module = GraphDataModule(\n#     graphs_folder, batch_size=1, **dataset_kwargs)\n# data_module.setup()\n\n# ndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\n# edim = next(iter(data_module.train_dataloader())).edata['h'].shape[-1]\n\n# my_models = {\n#     # \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation,\n#     #               dropout, residual, num_classes),\n#     # f\"e_graphsage_{aggregation}\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                          residual, num_classes, num_neighbors=number_neighbors, aggregation=aggregation),\n#     # f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                                      residual, num_classes, num_neighbors=None, aggregation=aggregation),\n#     \"e_gat_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                   residual, num_classes, num_neighbors=number_neighbors),\n#     # \"e_gat_no_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes, num_neighbors=None),\n# }\n\n# criterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n\n# for model_name, model in my_models.items():\n        \n#     config = {\n#         \"run_dtime\": run_dtime,\n#         \"type\": \"GNN\",\n#         \"model_name\": model_name,\n#         \"max_epochs\": max_epochs,\n#         \"learning_rate\": learning_rate,\n#         \"weight_decay\": weight_decay,\n#         \"ndim_out\": ndim_out,\n#         \"num_layers\": num_layers,\n#         \"number_neighbors\": number_neighbors,\n#         \"activation\": activation.__name__,\n#         \"dropout\": dropout,\n#         \"residual\": residual,\n#         \"multi_class\": multi_class,\n#         \"aggregation\": aggregation,\n#         \"early_stopping_patience\": early_stopping_patience,\n#         \"use_centralities_nfeats\": use_centralities_nfeats,}\n\n#     graph_model = GraphModel(model, criterion, learning_rate, config, model_name,\n#                              labels_mapping, weight_decay=weight_decay, using_wandb=using_wandb, norm=False, multi_class=True)\n\n    \n#     if using_wandb:\n#         wandb_logger = WandbLogger(\n#             project=f\"GNN-Analysis-{dataset.name}\", # Project Name Wandb\n#             name=f\"{model_name}---{g_type}\",\n#             config=config,\n#             save_dir=wandb_runs_path\n#         )\n#     else:\n#         wandb_logger = None\n        \n#     f1_checkpoint_callback = ModelCheckpoint(\n#         monitor=\"val_f1_score\",\n#         mode=\"max\",\n#         filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n#         save_top_k=save_top_k,\n#         verbose=False,\n#     )\n#     early_stopping_callback = EarlyStopping(\n#         monitor=\"val_loss\",\n#         mode=\"min\",\n#         patience=early_stopping_patience,\n#         verbose=False,\n#     )\n\n#     trainer = pl.Trainer(\n#         max_epochs=max_epochs,\n#         num_sanity_val_steps=0,\n#         log_every_n_steps=0,\n#         callbacks=[\n#             f1_checkpoint_callback,\n#             early_stopping_callback\n#         ],\n#         default_root_dir=logs_folder,\n#         logger=wandb_logger if using_wandb else None,\n#     )\n\n#     trainer.fit(graph_model, datamodule=data_module)\n    \n#     test_results = []\n    \n#     for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n#         graph_model.test_prefix = f\"best_f1_{i}\"\n#         results = trainer.test(\n#             graph_model, datamodule=data_module, ckpt_path=k)\n#         test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n\n#     logs = {\n#         \"median_f1_of_best_f1\": np.median(test_results),\n#         \"max_f1_of_best_f1\": np.max(test_results),\n#         \"avg_f1_of_best_f1\": np.mean(test_results)\n#     }\n\n#     if using_wandb:\n#         wandb.log(logs)\n#         wandb.finish()\n#     else:\n#         trainer.logger.log_metrics(logs, step=trainer.global_step)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:22:28.117070Z","iopub.execute_input":"2025-10-27T14:22:28.117265Z","iopub.status.idle":"2025-10-27T14:22:28.134638Z","shell.execute_reply.started":"2025-10-27T14:22:28.117251Z","shell.execute_reply":"2025-10-27T14:22:28.133998Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>EGraphSAGE mean</h2>","metadata":{}},{"cell_type":"code","source":"# warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n\n# dataset = datasets[dataset_name]\n\n# # Hyperparameters\n\n# activation = F.relu\n# # graphs_folder = os.path.join(dataset_folder, g_type)\n# # if dataset_name == \"cic_ids_2017\":\n# #     graphs_folder = \"/kaggle/working/cic_ids_2017/flow__multi_class__n_feats__unsorted\"\n# logs_folder = os.path.join(\"logs\", dataset.name)\n# os.makedirs(logs_folder, exist_ok=True)\n# wandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\n# os.makedirs(wandb_runs_path, exist_ok=True)\n\n# labels_mapping = {0: \"Normal\", 1: \"Attack\"}\n# num_classes = 2\n# if multi_class:\n#     # with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n#     # /kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\n#     with open(\"/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\", \"rb\") as f:\n#         labels_names = pickle.load(f)\n#     labels_mapping = labels_names[0]\n# num_classes = len(labels_mapping)\n\n# dataset_kwargs = dict(\n#     use_node_features=use_centralities_nfeats,\n#     multi_class=True,\n#     using_masking=False,\n#     masked_class=2,\n#     num_workers=0,\n#     label_col=dataset.label_col,\n#     class_num_col=dataset.class_num_col,\n#     device='cuda' if torch.cuda.is_available() else \"cpu\"\n# )\n\n# data_module = GraphDataModule(\n#     graphs_folder, batch_size=1, **dataset_kwargs)\n# data_module.setup()\n\n# ndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\n# edim = next(iter(data_module.train_dataloader())).edata['h'].shape[-1]\n\n# my_models = {\n#     # \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation,\n#     #               dropout, residual, num_classes),\n#     # f\"e_graphsage_{aggregation}\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                          residual, num_classes, num_neighbors=number_neighbors, aggregation=aggregation),\n#     f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                                                          residual, num_classes, num_neighbors=None, aggregation=aggregation),\n#     # \"e_gat_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #               residual, num_classes, num_neighbors=number_neighbors),\n#     # \"e_gat_no_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes, num_neighbors=None),\n# }\n\n# criterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n\n# for model_name, model in my_models.items():\n        \n#     config = {\n#         \"run_dtime\": run_dtime,\n#         \"type\": \"GNN\",\n#         \"model_name\": model_name,\n#         \"max_epochs\": max_epochs,\n#         \"learning_rate\": learning_rate,\n#         \"weight_decay\": weight_decay,\n#         \"ndim_out\": ndim_out,\n#         \"num_layers\": num_layers,\n#         \"number_neighbors\": number_neighbors,\n#         \"activation\": activation.__name__,\n#         \"dropout\": dropout,\n#         \"residual\": residual,\n#         \"multi_class\": multi_class,\n#         \"aggregation\": aggregation,\n#         \"early_stopping_patience\": early_stopping_patience,\n#         \"use_centralities_nfeats\": use_centralities_nfeats,}\n\n#     graph_model = GraphModel(model, criterion, learning_rate, config, model_name,\n#                              labels_mapping, weight_decay=weight_decay, using_wandb=using_wandb, norm=False, multi_class=True)\n\n    \n#     if using_wandb:\n#         wandb_logger = WandbLogger(\n#             project=f\"GNN-Analysis-{dataset.name}\", # Project Name Wandb\n#             name=f\"{model_name}---{g_type}\",\n#             config=config,\n#             save_dir=wandb_runs_path\n#         )\n#     else:\n#         wandb_logger = None\n        \n#     f1_checkpoint_callback = ModelCheckpoint(\n#         monitor=\"val_f1_score\",\n#         mode=\"max\",\n#         filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n#         save_top_k=save_top_k,\n#         verbose=False,\n#     )\n#     early_stopping_callback = EarlyStopping(\n#         monitor=\"val_loss\",\n#         mode=\"min\",\n#         patience=early_stopping_patience,\n#         verbose=False,\n#     )\n\n#     trainer = pl.Trainer(\n#         max_epochs=max_epochs,\n#         num_sanity_val_steps=0,\n#         log_every_n_steps=0,\n#         callbacks=[\n#             f1_checkpoint_callback,\n#             early_stopping_callback\n#         ],\n#         default_root_dir=logs_folder,\n#         logger=wandb_logger if using_wandb else None,\n#     )\n\n#     trainer.fit(graph_model, datamodule=data_module)\n    \n#     test_results = []\n    \n#     for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n#         graph_model.test_prefix = f\"best_f1_{i}\"\n#         results = trainer.test(\n#             graph_model, datamodule=data_module, ckpt_path=k)\n#         test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n\n#     logs = {\n#         \"median_f1_of_best_f1\": np.median(test_results),\n#         \"max_f1_of_best_f1\": np.max(test_results),\n#         \"avg_f1_of_best_f1\": np.mean(test_results)\n#     }\n\n#     if using_wandb:\n#         wandb.log(logs)\n#         wandb.finish()\n#     else:\n#         trainer.logger.log_metrics(logs, step=trainer.global_step)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:22:28.135260Z","iopub.execute_input":"2025-10-27T14:22:28.135459Z","iopub.status.idle":"2025-10-27T14:22:28.151696Z","shell.execute_reply.started":"2025-10-27T14:22:28.135435Z","shell.execute_reply":"2025-10-27T14:22:28.151094Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>EGraphSAGE sum</h2>","metadata":{}},{"cell_type":"code","source":"# warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n\n# dataset = datasets[dataset_name]\n\n# # Hyperparameters\n# aggregation=\"sum\"\n\n# activation = F.relu\n# # graphs_folder = os.path.join(dataset_folder, g_type)\n# # if dataset_name == \"cic_ids_2017\":\n#     # graphs_folder = \"/kaggle/working/cic_ids_2017/flow__multi_class__n_feats__unsorted\"\n# logs_folder = os.path.join(\"logs\", dataset.name)\n# os.makedirs(logs_folder, exist_ok=True)\n# wandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\n# os.makedirs(wandb_runs_path, exist_ok=True)\n\n# labels_mapping = {0: \"Normal\", 1: \"Attack\"}\n# num_classes = 2\n# if multi_class:\n#     # with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n#     # /kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\n#     with open(\"/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\", \"rb\") as f:\n#         labels_names = pickle.load(f)\n#     labels_mapping = labels_names[0]\n# num_classes = len(labels_mapping)\n\n# dataset_kwargs = dict(\n#     use_node_features=use_centralities_nfeats,\n#     multi_class=True,\n#     using_masking=False,\n#     masked_class=2,\n#     num_workers=0,\n#     label_col=dataset.label_col,\n#     class_num_col=dataset.class_num_col,\n#     device='cuda' if torch.cuda.is_available() else \"cpu\"\n# )\n\n# data_module = GraphDataModule(\n#     graphs_folder, batch_size=1, **dataset_kwargs)\n# data_module.setup()\n\n# ndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\n# edim = next(iter(data_module.train_dataloader())).edata['h'].shape[-1]\n\n# my_models = {\n#     # \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation,\n#     #               dropout, residual, num_classes),\n#     # f\"e_graphsage_{aggregation}\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                          residual, num_classes, num_neighbors=number_neighbors, aggregation=aggregation),\n#     f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                                                          residual, num_classes, num_neighbors=None, aggregation=aggregation),\n#     # \"e_gat_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #               residual, num_classes, num_neighbors=number_neighbors),\n#     # \"e_gat_no_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes, num_neighbors=None),\n# }\n\n# criterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n\n# for model_name, model in my_models.items():\n        \n#     config = {\n#         \"run_dtime\": run_dtime,\n#         \"type\": \"GNN\",\n#         \"model_name\": model_name,\n#         \"max_epochs\": max_epochs,\n#         \"learning_rate\": learning_rate,\n#         \"weight_decay\": weight_decay,\n#         \"ndim_out\": ndim_out,\n#         \"num_layers\": num_layers,\n#         \"number_neighbors\": number_neighbors,\n#         \"activation\": activation.__name__,\n#         \"dropout\": dropout,\n#         \"residual\": residual,\n#         \"multi_class\": multi_class,\n#         \"aggregation\": aggregation,\n#         \"early_stopping_patience\": early_stopping_patience,\n#         \"use_centralities_nfeats\": use_centralities_nfeats,}\n\n#     graph_model = GraphModel(model, criterion, learning_rate, config, model_name,\n#                              labels_mapping, weight_decay=weight_decay, using_wandb=using_wandb, norm=False, multi_class=True)\n\n    \n#     if using_wandb:\n#         wandb_logger = WandbLogger(\n#             project=f\"GNN-Analysis-{dataset.name}\", # Project Name Wandb\n#             name=f\"{model_name}---{g_type}\",\n#             config=config,\n#             save_dir=wandb_runs_path\n#         )\n#     else:\n#         wandb_logger = None\n        \n#     f1_checkpoint_callback = ModelCheckpoint(\n#         monitor=\"val_f1_score\",\n#         mode=\"max\",\n#         filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n#         save_top_k=save_top_k,\n#         verbose=False,\n#     )\n#     early_stopping_callback = EarlyStopping(\n#         monitor=\"val_loss\",\n#         mode=\"min\",\n#         patience=early_stopping_patience,\n#         verbose=False,\n#     )\n\n#     trainer = pl.Trainer(\n#         max_epochs=max_epochs,\n#         num_sanity_val_steps=0,\n#         log_every_n_steps=0,\n#         callbacks=[\n#             f1_checkpoint_callback,\n#             early_stopping_callback\n#         ],\n#         default_root_dir=logs_folder,\n#         logger=wandb_logger if using_wandb else None,\n#     )\n\n#     trainer.fit(graph_model, datamodule=data_module)\n    \n#     test_results = []\n    \n#     for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n#         graph_model.test_prefix = f\"best_f1_{i}\"\n#         results = trainer.test(\n#             graph_model, datamodule=data_module, ckpt_path=k)\n#         test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n\n#     logs = {\n#         \"median_f1_of_best_f1\": np.median(test_results),\n#         \"max_f1_of_best_f1\": np.max(test_results),\n#         \"avg_f1_of_best_f1\": np.mean(test_results)\n#     }\n\n#     if using_wandb:\n#         wandb.log(logs)\n#         wandb.finish()\n#     else:\n#         trainer.logger.log_metrics(logs, step=trainer.global_step)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:22:28.152404Z","iopub.execute_input":"2025-10-27T14:22:28.152635Z","iopub.status.idle":"2025-10-27T14:22:28.170358Z","shell.execute_reply.started":"2025-10-27T14:22:28.152610Z","shell.execute_reply":"2025-10-27T14:22:28.169811Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>EGraphSAGE Sampling mean</h2>","metadata":{}},{"cell_type":"code","source":"# warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n\n# dataset = datasets[dataset_name]\n\n# # Hyperparameters\n# aggregation=\"mean\"\n\n# activation = F.relu\n# # graphs_folder = os.path.join(dataset_folder, g_type)\n# # if dataset_name == \"cic_ids_2017\":\n#     # graphs_folder = \"/kaggle/working/cic_ids_2017/flow__multi_class__n_feats__unsorted\"\n# logs_folder = os.path.join(\"logs\", dataset.name)\n# os.makedirs(logs_folder, exist_ok=True)\n# wandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\n# os.makedirs(wandb_runs_path, exist_ok=True)\n\n# labels_mapping = {0: \"Normal\", 1: \"Attack\"}\n# num_classes = 2\n# if multi_class:\n#     # with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n#     # /kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\n#     with open(\"/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\", \"rb\") as f:\n#         labels_names = pickle.load(f)\n#     labels_mapping = labels_names[0]\n# num_classes = len(labels_mapping)\n\n# dataset_kwargs = dict(\n#     use_node_features=use_centralities_nfeats,\n#     multi_class=True,\n#     using_masking=False,\n#     masked_class=2,\n#     num_workers=0,\n#     label_col=dataset.label_col,\n#     class_num_col=dataset.class_num_col,\n#     device='cuda' if torch.cuda.is_available() else \"cpu\"\n# )\n\n# data_module = GraphDataModule(\n#     graphs_folder, batch_size=1, **dataset_kwargs)\n# data_module.setup()\n\n# ndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\n# edim = next(iter(data_module.train_dataloader())).edata['h'].shape[-1]\n\n# my_models = {\n#     # \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation,\n#     #               dropout, residual, num_classes),\n#     f\"e_graphsage_{aggregation}\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                                              residual, num_classes, num_neighbors=number_neighbors, aggregation=aggregation),\n#     # f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                                      residual, num_classes, num_neighbors=None, aggregation=aggregation),\n#     # \"e_gat_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                   # residual, num_classes, num_neighbors=number_neighbors),\n#     # \"e_gat_no_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes, num_neighbors=None),\n# }\n\n# criterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n\n# for model_name, model in my_models.items():\n        \n#     config = {\n#         \"run_dtime\": run_dtime,\n#         \"type\": \"GNN\",\n#         \"model_name\": model_name,\n#         \"max_epochs\": max_epochs,\n#         \"learning_rate\": learning_rate,\n#         \"weight_decay\": weight_decay,\n#         \"ndim_out\": ndim_out,\n#         \"num_layers\": num_layers,\n#         \"number_neighbors\": number_neighbors,\n#         \"activation\": activation.__name__,\n#         \"dropout\": dropout,\n#         \"residual\": residual,\n#         \"multi_class\": multi_class,\n#         \"aggregation\": aggregation,\n#         \"early_stopping_patience\": early_stopping_patience,\n#         \"use_centralities_nfeats\": use_centralities_nfeats,}\n\n#     graph_model = GraphModel(model, criterion, learning_rate, config, model_name,\n#                              labels_mapping, weight_decay=weight_decay, using_wandb=using_wandb, norm=False, multi_class=True)\n\n    \n#     if using_wandb:\n#         wandb_logger = WandbLogger(\n#             project=f\"GNN-Analysis-{dataset.name}\", # Project Name Wandb\n#             name=f\"{model_name}---{g_type}\",\n#             config=config,\n#             save_dir=wandb_runs_path\n#         )\n#     else:\n#         wandb_logger = None\n        \n#     f1_checkpoint_callback = ModelCheckpoint(\n#         monitor=\"val_f1_score\",\n#         mode=\"max\",\n#         filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n#         save_top_k=save_top_k,\n#         verbose=False,\n#     )\n#     early_stopping_callback = EarlyStopping(\n#         monitor=\"val_loss\",\n#         mode=\"min\",\n#         patience=early_stopping_patience,\n#         verbose=False,\n#     )\n\n#     trainer = pl.Trainer(\n#         max_epochs=max_epochs,\n#         num_sanity_val_steps=0,\n#         log_every_n_steps=0,\n#         callbacks=[\n#             f1_checkpoint_callback,\n#             early_stopping_callback\n#         ],\n#         default_root_dir=logs_folder,\n#         logger=wandb_logger if using_wandb else None,\n#     )\n\n#     trainer.fit(graph_model, datamodule=data_module)\n    \n#     test_results = []\n    \n#     for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n#         graph_model.test_prefix = f\"best_f1_{i}\"\n#         results = trainer.test(\n#             graph_model, datamodule=data_module, ckpt_path=k)\n#         test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n\n#     logs = {\n#         \"median_f1_of_best_f1\": np.median(test_results),\n#         \"max_f1_of_best_f1\": np.max(test_results),\n#         \"avg_f1_of_best_f1\": np.mean(test_results)\n#     }\n\n#     if using_wandb:\n#         wandb.log(logs)\n#         wandb.finish()\n#     else:\n#         trainer.logger.log_metrics(logs, step=trainer.global_step)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:22:28.171043Z","iopub.execute_input":"2025-10-27T14:22:28.171273Z","iopub.status.idle":"2025-10-27T14:22:28.185790Z","shell.execute_reply.started":"2025-10-27T14:22:28.171258Z","shell.execute_reply":"2025-10-27T14:22:28.185180Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>EGraphSAGE Sampling sum</h2>","metadata":{}},{"cell_type":"code","source":"# warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n\n# dataset = datasets[dataset_name]\n\n# # Hyperparameters\n# aggregation=\"sum\"\n\n# activation = F.relu\n# # graphs_folder = os.path.join(dataset_folder, g_type)\n# # if dataset_name == \"cic_ids_2017\":\n#     # graphs_folder = \"/kaggle/working/cic_ids_2017/flow__multi_class__n_feats__unsorted\"\n# logs_folder = os.path.join(\"logs\", dataset.name)\n# os.makedirs(logs_folder, exist_ok=True)\n# wandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\n# os.makedirs(wandb_runs_path, exist_ok=True)\n\n# labels_mapping = {0: \"Normal\", 1: \"Attack\"}\n# num_classes = 2\n# if multi_class:\n#     # with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n#     # /kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\n#     with open(\"/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\", \"rb\") as f:\n#         labels_names = pickle.load(f)\n#     labels_mapping = labels_names[0]\n# num_classes = len(labels_mapping)\n\n# dataset_kwargs = dict(\n#     use_node_features=use_centralities_nfeats,\n#     multi_class=True,\n#     using_masking=False,\n#     masked_class=2,\n#     num_workers=0,\n#     label_col=dataset.label_col,\n#     class_num_col=dataset.class_num_col,\n#     device='cuda' if torch.cuda.is_available() else \"cpu\"\n# )\n\n# data_module = GraphDataModule(\n#     graphs_folder, batch_size=1, **dataset_kwargs)\n# data_module.setup()\n\n# ndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\n# edim = next(iter(data_module.train_dataloader())).edata['h'].shape[-1]\n\n# my_models = {\n#     # \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation,\n#     #               dropout, residual, num_classes),\n#     f\"e_graphsage_{aggregation}\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                                              residual, num_classes, num_neighbors=number_neighbors, aggregation=aggregation),\n#     # f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                                      residual, num_classes, num_neighbors=None, aggregation=aggregation),\n#     # \"e_gat_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                   # residual, num_classes, num_neighbors=number_neighbors),\n#     # \"e_gat_no_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes, num_neighbors=None),\n# }\n\n# criterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n\n# for model_name, model in my_models.items():\n        \n#     config = {\n#         \"run_dtime\": run_dtime,\n#         \"type\": \"GNN\",\n#         \"model_name\": model_name,\n#         \"max_epochs\": max_epochs,\n#         \"learning_rate\": learning_rate,\n#         \"weight_decay\": weight_decay,\n#         \"ndim_out\": ndim_out,\n#         \"num_layers\": num_layers,\n#         \"number_neighbors\": number_neighbors,\n#         \"activation\": activation.__name__,\n#         \"dropout\": dropout,\n#         \"residual\": residual,\n#         \"multi_class\": multi_class,\n#         \"aggregation\": aggregation,\n#         \"early_stopping_patience\": early_stopping_patience,\n#         \"use_centralities_nfeats\": use_centralities_nfeats,}\n\n#     graph_model = GraphModel(model, criterion, learning_rate, config, model_name,\n#                              labels_mapping, weight_decay=weight_decay, using_wandb=using_wandb, norm=False, multi_class=True)\n\n    \n#     if using_wandb:\n#         wandb_logger = WandbLogger(\n#             project=f\"GNN-Analysis-{dataset.name}\", # Project Name Wandb\n#             name=f\"{model_name}---{g_type}\",\n#             config=config,\n#             save_dir=wandb_runs_path\n#         )\n#     else:\n#         wandb_logger = None\n        \n#     f1_checkpoint_callback = ModelCheckpoint(\n#         monitor=\"val_f1_score\",\n#         mode=\"max\",\n#         filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n#         save_top_k=save_top_k,\n#         verbose=False,\n#     )\n#     early_stopping_callback = EarlyStopping(\n#         monitor=\"val_loss\",\n#         mode=\"min\",\n#         patience=early_stopping_patience,\n#         verbose=False,\n#     )\n\n#     trainer = pl.Trainer(\n#         max_epochs=max_epochs,\n#         num_sanity_val_steps=0,\n#         log_every_n_steps=0,\n#         callbacks=[\n#             f1_checkpoint_callback,\n#             early_stopping_callback\n#         ],\n#         default_root_dir=logs_folder,\n#         logger=wandb_logger if using_wandb else None,\n#     )\n\n#     trainer.fit(graph_model, datamodule=data_module)\n    \n#     test_results = []\n    \n#     for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n#         graph_model.test_prefix = f\"best_f1_{i}\"\n#         results = trainer.test(\n#             graph_model, datamodule=data_module, ckpt_path=k)\n#         test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n\n#     logs = {\n#         \"median_f1_of_best_f1\": np.median(test_results),\n#         \"max_f1_of_best_f1\": np.max(test_results),\n#         \"avg_f1_of_best_f1\": np.mean(test_results)\n#     }\n\n#     if using_wandb:\n#         wandb.log(logs)\n#         wandb.finish()\n#     else:\n#         trainer.logger.log_metrics(logs, step=trainer.global_step)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:22:28.186521Z","iopub.execute_input":"2025-10-27T14:22:28.187067Z","iopub.status.idle":"2025-10-27T14:22:28.202407Z","shell.execute_reply.started":"2025-10-27T14:22:28.187050Z","shell.execute_reply":"2025-10-27T14:22:28.201694Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>E_GCN</h2>","metadata":{}},{"cell_type":"code","source":"# warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n\n# dataset = datasets[dataset_name]\n\n# # Hyperparameters\n# aggregation=\"mean\"\n\n# activation = F.relu\n# # graphs_folder = os.path.join(dataset_folder, g_type)\n# # if dataset_name == \"cic_ids_2017\":\n# #     graphs_folder = \"/kaggle/working/cic_ids_2017/flow__multi_class__n_feats__unsorted\"\n# logs_folder = os.path.join(\"logs\", dataset.name)\n# os.makedirs(logs_folder, exist_ok=True)\n# wandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\n# os.makedirs(wandb_runs_path, exist_ok=True)\n\n# labels_mapping = {0: \"Normal\", 1: \"Attack\"}\n# num_classes = 2\n# if multi_class:\n#     # with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n#     # /kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\n#     with open(\"/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\", \"rb\") as f:\n#         labels_names = pickle.load(f)\n#     labels_mapping = labels_names[0]\n# num_classes = len(labels_mapping)\n\n# dataset_kwargs = dict(\n#     use_node_features=use_centralities_nfeats,\n#     multi_class=True,\n#     using_masking=False,\n#     masked_class=2,\n#     num_workers=0,\n#     label_col=dataset.label_col,\n#     class_num_col=dataset.class_num_col,\n#     device='cuda' if torch.cuda.is_available() else \"cpu\"\n# )\n\n# data_module = GraphDataModule(\n#     graphs_folder, batch_size=1, **dataset_kwargs)\n# data_module.setup()\n\n# ndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\n# edim = next(iter(data_module.train_dataloader())).edata['h'].shape[-1]\n\n# my_models = {\n#     \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation,\n#                   dropout, residual, num_classes),\n#     # f\"e_graphsage_{aggregation}\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                          residual, num_classes, num_neighbors=number_neighbors, aggregation=aggregation),\n#     # f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                                      residual, num_classes, num_neighbors=None, aggregation=aggregation),\n#     # \"e_gat_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                   # residual, num_classes, num_neighbors=number_neighbors),\n#     # \"e_gat_no_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes, num_neighbors=None),\n# }\n\n# criterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n\n# for model_name, model in my_models.items():\n        \n#     config = {\n#         \"run_dtime\": run_dtime,\n#         \"type\": \"GNN\",\n#         \"model_name\": model_name,\n#         \"max_epochs\": max_epochs,\n#         \"learning_rate\": learning_rate,\n#         \"weight_decay\": weight_decay,\n#         \"ndim_out\": ndim_out,\n#         \"num_layers\": num_layers,\n#         \"number_neighbors\": number_neighbors,\n#         \"activation\": activation.__name__,\n#         \"dropout\": dropout,\n#         \"residual\": residual,\n#         \"multi_class\": multi_class,\n#         \"aggregation\": aggregation,\n#         \"early_stopping_patience\": early_stopping_patience,\n#         \"use_centralities_nfeats\": use_centralities_nfeats,}\n\n#     graph_model = GraphModel(model, criterion, learning_rate, config, model_name,\n#                              labels_mapping, weight_decay=weight_decay, using_wandb=using_wandb, norm=False, multi_class=True)\n\n    \n#     if using_wandb:\n#         wandb_logger = WandbLogger(\n#             project=f\"GNN-Analysis-{dataset.name}\", # Project Name Wandb\n#             name=f\"{model_name}---{g_type}\",\n#             config=config,\n#             save_dir=wandb_runs_path\n#         )\n#     else:\n#         wandb_logger = None\n        \n#     f1_checkpoint_callback = ModelCheckpoint(\n#         monitor=\"val_f1_score\",\n#         mode=\"max\",\n#         filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n#         save_top_k=save_top_k,\n#         verbose=False,\n#     )\n#     early_stopping_callback = EarlyStopping(\n#         monitor=\"val_loss\",\n#         mode=\"min\",\n#         patience=early_stopping_patience,\n#         verbose=False,\n#     )\n\n#     trainer = pl.Trainer(\n#         max_epochs=max_epochs,\n#         num_sanity_val_steps=0,\n#         log_every_n_steps=0,\n#         callbacks=[\n#             f1_checkpoint_callback,\n#             early_stopping_callback\n#         ],\n#         default_root_dir=logs_folder,\n#         logger=wandb_logger if using_wandb else None,\n#     )\n\n#     trainer.fit(graph_model, datamodule=data_module)\n    \n#     test_results = []\n    \n#     for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n#         graph_model.test_prefix = f\"best_f1_{i}\"\n#         results = trainer.test(\n#             graph_model, datamodule=data_module, ckpt_path=k)\n#         test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n\n#     logs = {\n#         \"median_f1_of_best_f1\": np.median(test_results),\n#         \"max_f1_of_best_f1\": np.max(test_results),\n#         \"avg_f1_of_best_f1\": np.mean(test_results)\n#     }\n\n#     if using_wandb:\n#         wandb.log(logs)\n#         wandb.finish()\n#     else:\n#         trainer.logger.log_metrics(logs, step=trainer.global_step)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T14:22:28.203229Z","iopub.execute_input":"2025-10-27T14:22:28.203513Z","iopub.status.idle":"2025-10-27T14:22:28.219477Z","shell.execute_reply.started":"2025-10-27T14:22:28.203492Z","shell.execute_reply":"2025-10-27T14:22:28.218819Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}