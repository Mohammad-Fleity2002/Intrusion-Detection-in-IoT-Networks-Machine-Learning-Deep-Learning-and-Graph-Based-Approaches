{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12479627,"sourceType":"datasetVersion","datasetId":7874205},{"sourceId":12479887,"sourceType":"datasetVersion","datasetId":7874406},{"sourceId":12516523,"sourceType":"datasetVersion","datasetId":7900575},{"sourceId":258352,"sourceType":"modelInstanceVersion","modelInstanceId":218283,"modelId":240009},{"sourceId":470644,"sourceType":"modelInstanceVersion","modelInstanceId":379672,"modelId":399557},{"sourceId":473434,"sourceType":"modelInstanceVersion","modelInstanceId":381305,"modelId":400996}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h2>CIC_TON_IOT_GNN_TRAINING flow__multi_class__n_feats__unsorted</h2>","metadata":{}},{"cell_type":"markdown","source":"<h1>Creating Graphs</h1>","metadata":{}},{"cell_type":"code","source":"!pip install powerlaw","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:17.305766Z","iopub.execute_input":"2025-09-17T10:55:17.306007Z","iopub.status.idle":"2025-09-17T10:55:20.736481Z","shell.execute_reply.started":"2025-09-17T10:55:17.305984Z","shell.execute_reply":"2025-09-17T10:55:20.735563Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: powerlaw in /usr/local/lib/python3.11/dist-packages (1.5)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from powerlaw) (1.15.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from powerlaw) (1.26.4)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from powerlaw) (3.7.2)\nRequirement already satisfied: mpmath in /usr/local/lib/python3.11/dist-packages (from powerlaw) (1.3.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw) (11.2.1)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->powerlaw) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->powerlaw) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->powerlaw) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->powerlaw) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->powerlaw) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->powerlaw) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->powerlaw) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->powerlaw) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->powerlaw) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->powerlaw) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->powerlaw) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->powerlaw) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import json\n\nwith open(\"/kaggle/input/cic-ton-iot-gnn/df_properties.json\", \"r\") as f:\n    df_properties = json.load(f)\n\nprint(\"üìò Dataset Properties:\")\nfor k, v in df_properties.items():\n    print(f\"{k}: {v}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:20.737524Z","iopub.execute_input":"2025-09-17T10:55:20.737775Z","iopub.status.idle":"2025-09-17T10:55:20.752190Z","shell.execute_reply.started":"2025-09-17T10:55:20.737751Z","shell.execute_reply":"2025-09-17T10:55:20.751347Z"}},"outputs":[{"name":"stdout","text":"üìò Dataset Properties:\nname: cic_ton_iot\nlength: 5350583\nnum_benign: 2514059\npercentage_of_benign_records: 46.98663678331875\nnum_attack: 2836524\npercentage_of_attack_records: 53.01336321668125\nattacks: ['Benign', 'mitm', 'scanning', 'dos', 'ddos', 'injection', 'password', 'backdoor', 'ransomware', 'xss']\nweak_columns: ['Pkt Len Max', 'Bwd Seg Size Avg', 'Fwd Seg Size Avg', 'Fwd URG Flags', 'Bwd Pkt Len Max', 'Bwd URG Flags', 'Fwd Pkt Len Mean', 'Pkt Len Mean', 'FIN Flag Cnt', 'Bwd Header Len', 'Flow IAT Min', 'Tot Bwd Pkts', 'Flow Duration', 'Flow IAT Max', 'TotLen Bwd Pkts', 'Bwd Pkt Len Std', 'Active Std', 'Fwd Pkts/b Avg', 'Fwd Byts/b Avg', 'Fwd IAT Mean', 'Fwd Pkt Len Std', 'Pkt Len Var', 'Fwd Blk Rate Avg', 'Subflow Bwd Pkts', 'Pkt Len Std', 'CWE Flag Count', 'Bwd PSH Flags', 'Flow Pkts/s', 'URG Flag Cnt', 'Active Mean', 'Idle Mean', 'Bwd IAT Mean', 'Bwd IAT Tot', 'Flow IAT Mean', 'Fwd IAT Tot', 'PSH Flag Cnt', 'Bwd Pkt Len Mean', 'Pkt Size Avg', 'Fwd Pkt Len Max']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pickle\n\nwith open(\"/kaggle/input/cic-ton-iot-gnn/labels_names.pkl\", \"rb\") as f:\n    labels_names = pickle.load(f)\nprint(labels_names)\nprint(\"\\nüè∑Ô∏è Label Class Mapping:\")\n# for k, v in labels_names.items():\n#     print(f\"{k}: {v}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:20.753035Z","iopub.execute_input":"2025-09-17T10:55:20.753545Z","iopub.status.idle":"2025-09-17T10:55:20.767603Z","shell.execute_reply.started":"2025-09-17T10:55:20.753527Z","shell.execute_reply":"2025-09-17T10:55:20.766914Z"}},"outputs":[{"name":"stdout","text":"[{0: 'Benign', 1: 'backdoor', 2: 'ddos', 3: 'dos', 4: 'injection', 5: 'mitm', 6: 'password', 7: 'ransomware', 8: 'scanning', 9: 'xss'}, array(['Benign', 'mitm', 'scanning', 'dos', 'ddos', 'injection',\n       'password', 'backdoor', 'ransomware', 'xss'], dtype=object)]\n\nüè∑Ô∏è Label Class Mapping:\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport pickle\nimport random\nimport socket\nimport struct\n\nimport sys\nimport pandas as pd\nsys.path.append(\"/kaggle/input/gnn-nids/pytorch/default/1\")\n\nimport networkx as nx\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom src.dataset.dataset_info import datasets\nfrom src.graph.graph_construction import create_weightless_window_graph\nfrom src.graph.graph_measures import calculate_graph_measures\nfrom src.graph.centralities import add_centralities, add_centralities_as_node_features\n# from local_variables import local_datasets_path\nlocal_datasets_path = \"/kaggle/input/\"\noriginal_datasets_files_path = \"/kaggle/input/cic-ton-iot-gnn\"\n# /kaggle/input/cic-ids-2017-gnn-2\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-09-17T10:55:20.769367Z","iopub.execute_input":"2025-09-17T10:55:20.769581Z","iopub.status.idle":"2025-09-17T10:55:22.105664Z","shell.execute_reply.started":"2025-09-17T10:55:20.769565Z","shell.execute_reply":"2025-09-17T10:55:22.105026Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"multi_class = True\n\nuse_node_features = True\n\nuse_port_in_address = False\n\ngenerated_ips = False\n\ngraph_type = \"flow\"\n# graph_type = \"window\"\n# graph_type = \"line\"\n\nwindow_size= 500\n# print(\"1. flow sorted\")\n# print(\"2. flow using node features unsorted\") \nprint(\"3. flow using node features sorted\")\n# 4. flow using port numbers sorted\n\nsort_timestamp = True\n\n# k_fold = None\n# k_fold = 5\n\nvalidation_size = 0.1\ntest_size = 0.1\n\ncn_measures = [\"betweenness\", \"degree\", \"pagerank\", \"closeness\", \"k_truss\"]\n# cn_measures = [\"betweenness\", \"degree\", \"closeness\"]\n\nnetwork_features = ['src_betweenness', 'dst_betweenness', 'src_degree', 'dst_degree', 'src_pagerank', 'dst_pagerank', 'src_closeness', 'dst_closeness', 'src_k_truss', 'dst_k_truss']\n# network_features = ['src_betweenness', 'dst_betweenness', 'src_degree', 'dst_degree', 'src_pagerank', 'dst_pagerank']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:22.106456Z","iopub.execute_input":"2025-09-17T10:55:22.106927Z","iopub.status.idle":"2025-09-17T10:55:22.112469Z","shell.execute_reply.started":"2025-09-17T10:55:22.106904Z","shell.execute_reply":"2025-09-17T10:55:22.111563Z"}},"outputs":[{"name":"stdout","text":"3. flow using node features sorted\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# name = \"cic_ton_iot_5_percent\"\nname = \"cic_ton_iot\"\n# name = \"cic_ids_2017_5_percent\"\n# name = \"cic_ids_2017\"\n# name = \"cic_bot_iot\"\n# name = \"cic_ton_iot_modified\"\n# name = \"nf_ton_iotv2_modified\"\n# name = \"ccd_inid_modified\"\n# name = \"nf_uq_nids_modified\"\n# name = \"edge_iiot\"\n# name = \"nf_cse_cic_ids2018\"\n# name = \"nf_bot_iotv2\"\n# name = \"nf_uq_nids\"\n# name = \"x_iiot\"\n\ndataset = datasets[name]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:22.113386Z","iopub.execute_input":"2025-09-17T10:55:22.113691Z","iopub.status.idle":"2025-09-17T10:55:22.131378Z","shell.execute_reply.started":"2025-09-17T10:55:22.113665Z","shell.execute_reply":"2025-09-17T10:55:22.130656Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"g_type = \"\"\nif graph_type == \"flow\":\n    g_type = \"flow\"\nelif graph_type == \"line\":\n    g_type = f\"line_graph_{window_size}\"\nelif graph_type == \"window\":\n    g_type = f\"window_graph_{window_size}\"\n    \nif multi_class:\n    g_type += \"__multi_class\"\n    \nif use_node_features:\n    g_type += \"__n_feats\"\n    \n# if k_fold:\n#     g_type += f\"__{k_fold}_fold\"\n    \nif use_port_in_address:\n    g_type += \"__ports\"\n    \nif generated_ips:\n    g_type += \"__generated_ips\"\n    \nif sort_timestamp:\n    g_type += \"__sorted\"\nelse:\n    g_type += \"__unsorted\"\n# ****************** ALTERED    \n\n\n\ndataset_path = os.path.join(local_datasets_path,name)\nfolder_path = os.path.join(dataset_path, g_type)\nif name == \"cic_ids_2017\":\n    name_ds = \"cic-ids-2017-gnn-2\"\n    dataset_path = os.path.join(local_datasets_path,name_ds)\n    # folder_path = os.path.join(local_datasets_path,name, g_type)\n    folder_path= os.path.join(\"/kaggle/working/\",name,g_type)\nif name ==\"cic_ton_iot\":\n    name_ds = \"cic-ton-iot-gnn\"\n    dataset_path = os.path.join(local_datasets_path,name_ds)\n    folder_path= os.path.join(\"/kaggle/working/\",name,g_type)\n    \nprint(folder_path)\nprint(dataset_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:22.132467Z","iopub.execute_input":"2025-09-17T10:55:22.133459Z","iopub.status.idle":"2025-09-17T10:55:22.148017Z","shell.execute_reply.started":"2025-09-17T10:55:22.133429Z","shell.execute_reply":"2025-09-17T10:55:22.147456Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/cic_ton_iot/flow__multi_class__n_feats__sorted\n/kaggle/input/cic-ton-iot-gnn\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# if name== \"cic_ids_2017\":\n#     df=pd.read_parquet(os.path.join(dataset_path, f\"{name}.parquet\"))\n# else:\ndf = pd.read_parquet(os.path.join(dataset_path, f\"{name}.parquet\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:22.148736Z","iopub.execute_input":"2025-09-17T10:55:22.149007Z","iopub.status.idle":"2025-09-17T10:55:30.114678Z","shell.execute_reply.started":"2025-09-17T10:55:22.148982Z","shell.execute_reply":"2025-09-17T10:55:30.114106Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2025-09-17T10:55:30.115457Z","iopub.execute_input":"2025-09-17T10:55:30.115689Z","iopub.status.idle":"2025-09-17T10:55:30.144001Z","shell.execute_reply.started":"2025-09-17T10:55:30.115671Z","shell.execute_reply":"2025-09-17T10:55:30.143243Z"},"trusted":true},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"   Flow ID          Src IP Src Port          Dst IP Dst Port  Protocol  \\\n0   179528   177.30.87.144      0.0     192.168.1.1      0.0       0.0   \n1   111395   167.49.176.28      0.0  50.165.192.168      0.0       0.0   \n2   987246   230.158.52.59      0.0  177.21.192.168      0.0       0.0   \n3   230384  183.68.192.168      0.0     1.1.192.168      0.0       0.0   \n4   230209  183.41.192.168      0.0     1.1.192.168      0.0       0.0   \n\n                  Timestamp  Flow Duration  Tot Fwd Pkts  Tot Bwd Pkts  ...  \\\n0 2019-04-25 17:18:52+00:00     47814343.0           5.0           0.0  ...   \n1 2019-04-25 17:18:49+00:00      2033142.0           2.0           0.0  ...   \n2 2019-04-25 17:18:37+00:00     82877133.0          14.0           0.0  ...   \n3 2019-04-25 17:18:42+00:00        24359.0           2.0           0.0  ...   \n4 2019-04-25 17:18:42+00:00     10239351.0           3.0           0.0  ...   \n\n     Active Std  Active Max  Active Min     Idle Mean      Idle Std  \\\n0  0.000000e+00   1038036.0   1038036.0  5.187256e+14  8.984590e+14   \n1  0.000000e+00         0.0         0.0  1.556177e+15  0.000000e+00   \n2  1.711593e+06   3942470.0    226402.0  1.729085e+14  5.187256e+14   \n3  0.000000e+00         0.0         0.0  1.556177e+15  0.000000e+00   \n4  0.000000e+00   4053975.0   4053975.0  7.780884e+14  1.100383e+15   \n\n       Idle Max      Idle Min  Label  Attack  Class  \n0  1.556177e+15  1.657324e+07      0  Benign      0  \n1  1.556177e+15  1.556177e+15      0  Benign      0  \n2  1.556177e+15  6.036493e+06      0  Benign      0  \n3  1.556177e+15  1.556177e+15      0  Benign      0  \n4  1.556177e+15  6.185376e+06      0  Benign      0  \n\n[5 rows x 85 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Flow ID</th>\n      <th>Src IP</th>\n      <th>Src Port</th>\n      <th>Dst IP</th>\n      <th>Dst Port</th>\n      <th>Protocol</th>\n      <th>Timestamp</th>\n      <th>Flow Duration</th>\n      <th>Tot Fwd Pkts</th>\n      <th>Tot Bwd Pkts</th>\n      <th>...</th>\n      <th>Active Std</th>\n      <th>Active Max</th>\n      <th>Active Min</th>\n      <th>Idle Mean</th>\n      <th>Idle Std</th>\n      <th>Idle Max</th>\n      <th>Idle Min</th>\n      <th>Label</th>\n      <th>Attack</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>179528</td>\n      <td>177.30.87.144</td>\n      <td>0.0</td>\n      <td>192.168.1.1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-04-25 17:18:52+00:00</td>\n      <td>47814343.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000e+00</td>\n      <td>1038036.0</td>\n      <td>1038036.0</td>\n      <td>5.187256e+14</td>\n      <td>8.984590e+14</td>\n      <td>1.556177e+15</td>\n      <td>1.657324e+07</td>\n      <td>0</td>\n      <td>Benign</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>111395</td>\n      <td>167.49.176.28</td>\n      <td>0.0</td>\n      <td>50.165.192.168</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-04-25 17:18:49+00:00</td>\n      <td>2033142.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000e+00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.556177e+15</td>\n      <td>0.000000e+00</td>\n      <td>1.556177e+15</td>\n      <td>1.556177e+15</td>\n      <td>0</td>\n      <td>Benign</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>987246</td>\n      <td>230.158.52.59</td>\n      <td>0.0</td>\n      <td>177.21.192.168</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-04-25 17:18:37+00:00</td>\n      <td>82877133.0</td>\n      <td>14.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.711593e+06</td>\n      <td>3942470.0</td>\n      <td>226402.0</td>\n      <td>1.729085e+14</td>\n      <td>5.187256e+14</td>\n      <td>1.556177e+15</td>\n      <td>6.036493e+06</td>\n      <td>0</td>\n      <td>Benign</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>230384</td>\n      <td>183.68.192.168</td>\n      <td>0.0</td>\n      <td>1.1.192.168</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-04-25 17:18:42+00:00</td>\n      <td>24359.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000e+00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.556177e+15</td>\n      <td>0.000000e+00</td>\n      <td>1.556177e+15</td>\n      <td>1.556177e+15</td>\n      <td>0</td>\n      <td>Benign</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>230209</td>\n      <td>183.41.192.168</td>\n      <td>0.0</td>\n      <td>1.1.192.168</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-04-25 17:18:42+00:00</td>\n      <td>10239351.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000e+00</td>\n      <td>4053975.0</td>\n      <td>4053975.0</td>\n      <td>7.780884e+14</td>\n      <td>1.100383e+15</td>\n      <td>1.556177e+15</td>\n      <td>6.185376e+06</td>\n      <td>0</td>\n      <td>Benign</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows √ó 85 columns</p>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"cols_to_norm = list(set(list(df.columns))  - set(list([dataset.label_col, dataset.class_num_col])) - set(dataset.drop_columns)  - set(dataset.weak_columns))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:30.144818Z","iopub.execute_input":"2025-09-17T10:55:30.145474Z","iopub.status.idle":"2025-09-17T10:55:30.149135Z","shell.execute_reply.started":"2025-09-17T10:55:30.145450Z","shell.execute_reply":"2025-09-17T10:55:30.148466Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"df[dataset.label_col].value_counts()","metadata":{"execution":{"iopub.status.busy":"2025-09-17T10:55:30.149854Z","iopub.execute_input":"2025-09-17T10:55:30.150158Z","iopub.status.idle":"2025-09-17T10:55:30.193251Z","shell.execute_reply.started":"2025-09-17T10:55:30.150139Z","shell.execute_reply":"2025-09-17T10:55:30.192650Z"},"trusted":true},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Label\n1    2836524\n0    2514059\nName: count, dtype: int64"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"if generated_ips:\n    df[dataset.src_ip_col] = df[dataset.src_ip_col].apply(lambda x: socket.inet_ntoa(struct.pack('>I', random.randint(0xac100001, 0xac1f0001))))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:30.193891Z","iopub.execute_input":"2025-09-17T10:55:30.194146Z","iopub.status.idle":"2025-09-17T10:55:30.210674Z","shell.execute_reply.started":"2025-09-17T10:55:30.194089Z","shell.execute_reply":"2025-09-17T10:55:30.210132Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"if sort_timestamp:\n    # df[dataset.timestamp_col] = pd.to_datetime(df[dataset.timestamp_col].str.strip(), format=dataset.timestamp_format)\n    df.sort_values(dataset.timestamp_col, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:30.213280Z","iopub.execute_input":"2025-09-17T10:55:30.213472Z","iopub.status.idle":"2025-09-17T10:55:33.716932Z","shell.execute_reply.started":"2025-09-17T10:55:30.213457Z","shell.execute_reply":"2025-09-17T10:55:33.716150Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"if use_port_in_address:\n    df[dataset.src_port_col] = df[dataset.src_port_col].astype(float).astype(int).astype(str) # to remove the decimal point\n    df[dataset.src_ip_col] = df[dataset.src_ip_col] + ':' + df[dataset.src_port_col]\n\n    df[dataset.dst_port_col] = df[dataset.dst_port_col].astype(float).astype(int).astype(str) # to remove the decimal point\n    df[dataset.dst_ip_col] = df[dataset.dst_ip_col] + ':' + df[dataset.dst_port_col]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:33.718057Z","iopub.execute_input":"2025-09-17T10:55:33.718365Z","iopub.status.idle":"2025-09-17T10:55:33.723056Z","shell.execute_reply.started":"2025-09-17T10:55:33.718339Z","shell.execute_reply":"2025-09-17T10:55:33.722541Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"df.head(20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:33.723762Z","iopub.execute_input":"2025-09-17T10:55:33.724033Z","iopub.status.idle":"2025-09-17T10:55:33.770501Z","shell.execute_reply.started":"2025-09-17T10:55:33.724005Z","shell.execute_reply":"2025-09-17T10:55:33.769751Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"        Flow ID           Src IP Src Port         Dst IP Dst Port  Protocol  \\\n562313   179528    177.30.87.144      0.0    192.168.1.1      0.0       0.0   \n562315  1006517   244.15.103.115      0.0   192.168.1.79      0.0       0.0   \n562314   179528    177.30.87.144      0.0    192.168.1.1      0.0       0.0   \n562317  1048404   41.238.224.122      0.0  192.168.1.190      0.0       0.0   \n562316   179528    177.30.87.144      0.0    192.168.1.1      0.0       0.0   \n562319  1006517   244.15.103.115      0.0   192.168.1.79      0.0       0.0   \n562318   179528    177.30.87.144      0.0    192.168.1.1      0.0       0.0   \n562321   179528    177.30.87.144      0.0    192.168.1.1      0.0       0.0   \n562320  1006517   244.15.103.115      0.0   192.168.1.79      0.0       0.0   \n563028   107367   164.91.192.168      0.0    1.152.3.122      0.0       0.0   \n562903   107091   164.42.192.168      0.0    1.152.3.122      0.0       0.0   \n562322  1006517   244.15.103.115      0.0   192.168.1.79      0.0       0.0   \n562917   102764  161.241.192.168      0.0    1.152.3.122      0.0       0.0   \n562323   179528    177.30.87.144      0.0    192.168.1.1      0.0       0.0   \n562966   102117  161.130.192.168      0.0    1.152.3.122      0.0       0.0   \n562324  1006517   244.15.103.115      0.0   192.168.1.79      0.0       0.0   \n562934    68919   140.85.192.168      0.0  1.152.192.168      0.0       0.0   \n562325   179528    177.30.87.144      0.0    192.168.1.1      0.0       0.0   \n562326  1006517   244.15.103.115      0.0   192.168.1.79      0.0       0.0   \n562327   179528    177.30.87.144      0.0    192.168.1.1      0.0       0.0   \n\n                       Timestamp  Flow Duration  Tot Fwd Pkts  Tot Bwd Pkts  \\\n562313 2019-02-04 19:53:30+00:00    115205004.0           3.0           0.0   \n562315 2019-02-04 19:54:38+00:00      6039467.0           3.0           0.0   \n562314 2019-02-04 19:55:58+00:00     65536962.0           3.0           0.0   \n562317 2019-02-04 19:57:37+00:00     90177027.0           3.0           0.0   \n562316 2019-02-04 19:58:04+00:00    111616761.0           3.0           0.0   \n562319 2019-02-04 19:59:07+00:00     40255700.0           2.0           0.0   \n562318 2019-02-04 20:00:37+00:00     73428081.0           3.0           0.0   \n562321 2019-02-04 20:03:01+00:00     56417287.0           3.0           0.0   \n562320 2019-02-04 20:03:26+00:00     98506703.0           7.0           0.0   \n563028 2019-02-04 20:03:43+00:00     43687115.0           2.0           0.0   \n562903 2019-02-04 20:03:59+00:00     44672952.0           2.0           0.0   \n562322 2019-02-04 20:06:52+00:00    100557678.0           6.0           0.0   \n562917 2019-02-04 20:06:56+00:00     14504914.0           2.0           0.0   \n562323 2019-02-04 20:07:12+00:00    113764805.0           5.0           0.0   \n562966 2019-02-04 20:07:31+00:00     13460934.0           2.0           0.0   \n562324 2019-02-04 20:09:19+00:00    104447217.0           4.0           0.0   \n562934 2019-02-04 20:09:26+00:00     16157462.0           2.0           0.0   \n562325 2019-02-04 20:09:41+00:00    103939761.0           3.0           0.0   \n562326 2019-02-04 20:11:41+00:00     70250783.0           2.0           0.0   \n562327 2019-02-04 20:11:59+00:00    104244396.0           5.0           0.0   \n\n        ...  Active Std  Active Max  Active Min     Idle Mean      Idle Std  \\\n562313  ...         0.0         0.0         0.0  5.180663e+14  8.973171e+14   \n562315  ...         0.0         0.0         0.0  1.554199e+15  0.000000e+00   \n562314  ...         0.0         0.0         0.0  5.180663e+14  8.973172e+14   \n562317  ...         0.0         0.0         0.0  5.180664e+14  8.973172e+14   \n562316  ...         0.0         0.0         0.0  5.180664e+14  8.973172e+14   \n562319  ...         0.0         0.0         0.0  7.770996e+14  1.098985e+15   \n562318  ...         0.0         0.0         0.0  5.180664e+14  8.973173e+14   \n562321  ...         0.0         0.0         0.0  5.180665e+14  8.973174e+14   \n562320  ...         0.0   1942261.0   1942261.0  2.590333e+14  6.344992e+14   \n563028  ...         0.0         0.0         0.0  7.770997e+14  1.098985e+15   \n562903  ...         0.0         0.0         0.0  7.770997e+14  1.098985e+15   \n562322  ...         0.0   3993329.0   3993329.0  3.108399e+14  6.950592e+14   \n562917  ...         0.0         0.0         0.0  7.770998e+14  1.098985e+15   \n562323  ...         0.0      1503.0      1503.0  3.885499e+14  7.770998e+14   \n562966  ...         0.0         0.0         0.0  7.770998e+14  1.098985e+15   \n562324  ...         0.0         0.0         0.0  3.885500e+14  7.770999e+14   \n562934  ...         0.0         0.0         0.0  7.770999e+14  1.098985e+15   \n562325  ...         0.0         0.0         0.0  5.180666e+14  8.973176e+14   \n562326  ...         0.0         0.0         0.0  7.771000e+14  1.098985e+15   \n562327  ...         0.0         0.0         0.0  3.108400e+14  6.950593e+14   \n\n            Idle Max      Idle Min  Label  Attack  Class  \n562313  1.554199e+15  1.300883e+07      0  Benign      0  \n562315  1.554199e+15  1.554199e+15      0  Benign      0  \n562314  1.554199e+15  2.170914e+07      0  Benign      0  \n562317  1.554199e+15  1.516871e+07      0  Benign      0  \n562316  1.554199e+15  5.447900e+07      0  Benign      0  \n562319  1.554199e+15  4.025570e+07      0  Benign      0  \n562318  1.554199e+15  1.914878e+07      0  Benign      0  \n562321  1.554199e+15  2.334359e+07      0  Benign      0  \n562320  1.554199e+15  6.040497e+06      0  Benign      0  \n563028  1.554199e+15  4.368712e+07      0  Benign      0  \n562903  1.554199e+15  4.467295e+07      0  Benign      0  \n562322  1.554200e+15  6.033677e+06      0  Benign      0  \n562917  1.554200e+15  1.450491e+07      0  Benign      0  \n562323  1.554200e+15  1.443224e+07      0  Benign      0  \n562966  1.554200e+15  1.346093e+07      0  Benign      0  \n562324  1.554200e+15  1.003317e+07      0  Benign      0  \n562934  1.554200e+15  1.615746e+07      0  Benign      0  \n562325  1.554200e+15  3.522726e+07      0  Benign      0  \n562326  1.554200e+15  7.025078e+07      0  Benign      0  \n562327  1.554200e+15  1.689920e+07      0  Benign      0  \n\n[20 rows x 85 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Flow ID</th>\n      <th>Src IP</th>\n      <th>Src Port</th>\n      <th>Dst IP</th>\n      <th>Dst Port</th>\n      <th>Protocol</th>\n      <th>Timestamp</th>\n      <th>Flow Duration</th>\n      <th>Tot Fwd Pkts</th>\n      <th>Tot Bwd Pkts</th>\n      <th>...</th>\n      <th>Active Std</th>\n      <th>Active Max</th>\n      <th>Active Min</th>\n      <th>Idle Mean</th>\n      <th>Idle Std</th>\n      <th>Idle Max</th>\n      <th>Idle Min</th>\n      <th>Label</th>\n      <th>Attack</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>562313</th>\n      <td>179528</td>\n      <td>177.30.87.144</td>\n      <td>0.0</td>\n      <td>192.168.1.1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-02-04 19:53:30+00:00</td>\n      <td>115205004.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5.180663e+14</td>\n      <td>8.973171e+14</td>\n      <td>1.554199e+15</td>\n      <td>1.300883e+07</td>\n      <td>0</td>\n      <td>Benign</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>562315</th>\n      <td>1006517</td>\n      <td>244.15.103.115</td>\n      <td>0.0</td>\n      <td>192.168.1.79</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-02-04 19:54:38+00:00</td>\n      <td>6039467.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.554199e+15</td>\n      <td>0.000000e+00</td>\n      <td>1.554199e+15</td>\n      <td>1.554199e+15</td>\n      <td>0</td>\n      <td>Benign</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>562314</th>\n      <td>179528</td>\n      <td>177.30.87.144</td>\n      <td>0.0</td>\n      <td>192.168.1.1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-02-04 19:55:58+00:00</td>\n      <td>65536962.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5.180663e+14</td>\n      <td>8.973172e+14</td>\n      <td>1.554199e+15</td>\n      <td>2.170914e+07</td>\n      <td>0</td>\n      <td>Benign</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>562317</th>\n      <td>1048404</td>\n      <td>41.238.224.122</td>\n      <td>0.0</td>\n      <td>192.168.1.190</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-02-04 19:57:37+00:00</td>\n      <td>90177027.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5.180664e+14</td>\n      <td>8.973172e+14</td>\n      <td>1.554199e+15</td>\n      <td>1.516871e+07</td>\n      <td>0</td>\n      <td>Benign</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>562316</th>\n      <td>179528</td>\n      <td>177.30.87.144</td>\n      <td>0.0</td>\n      <td>192.168.1.1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-02-04 19:58:04+00:00</td>\n      <td>111616761.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5.180664e+14</td>\n      <td>8.973172e+14</td>\n      <td>1.554199e+15</td>\n      <td>5.447900e+07</td>\n      <td>0</td>\n      <td>Benign</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>562319</th>\n      <td>1006517</td>\n      <td>244.15.103.115</td>\n      <td>0.0</td>\n      <td>192.168.1.79</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-02-04 19:59:07+00:00</td>\n      <td>40255700.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7.770996e+14</td>\n      <td>1.098985e+15</td>\n      <td>1.554199e+15</td>\n      <td>4.025570e+07</td>\n      <td>0</td>\n      <td>Benign</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>562318</th>\n      <td>179528</td>\n      <td>177.30.87.144</td>\n      <td>0.0</td>\n      <td>192.168.1.1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-02-04 20:00:37+00:00</td>\n      <td>73428081.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5.180664e+14</td>\n      <td>8.973173e+14</td>\n      <td>1.554199e+15</td>\n      <td>1.914878e+07</td>\n      <td>0</td>\n      <td>Benign</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>562321</th>\n      <td>179528</td>\n      <td>177.30.87.144</td>\n      <td>0.0</td>\n      <td>192.168.1.1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-02-04 20:03:01+00:00</td>\n      <td>56417287.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5.180665e+14</td>\n      <td>8.973174e+14</td>\n      <td>1.554199e+15</td>\n      <td>2.334359e+07</td>\n      <td>0</td>\n      <td>Benign</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>562320</th>\n      <td>1006517</td>\n      <td>244.15.103.115</td>\n      <td>0.0</td>\n      <td>192.168.1.79</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-02-04 20:03:26+00:00</td>\n      <td>98506703.0</td>\n      <td>7.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1942261.0</td>\n      <td>1942261.0</td>\n      <td>2.590333e+14</td>\n      <td>6.344992e+14</td>\n      <td>1.554199e+15</td>\n      <td>6.040497e+06</td>\n      <td>0</td>\n      <td>Benign</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>563028</th>\n      <td>107367</td>\n      <td>164.91.192.168</td>\n      <td>0.0</td>\n      <td>1.152.3.122</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-02-04 20:03:43+00:00</td>\n      <td>43687115.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7.770997e+14</td>\n      <td>1.098985e+15</td>\n      <td>1.554199e+15</td>\n      <td>4.368712e+07</td>\n      <td>0</td>\n      <td>Benign</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>562903</th>\n      <td>107091</td>\n      <td>164.42.192.168</td>\n      <td>0.0</td>\n      <td>1.152.3.122</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-02-04 20:03:59+00:00</td>\n      <td>44672952.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7.770997e+14</td>\n      <td>1.098985e+15</td>\n      <td>1.554199e+15</td>\n      <td>4.467295e+07</td>\n      <td>0</td>\n      <td>Benign</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>562322</th>\n      <td>1006517</td>\n      <td>244.15.103.115</td>\n      <td>0.0</td>\n      <td>192.168.1.79</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-02-04 20:06:52+00:00</td>\n      <td>100557678.0</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>3993329.0</td>\n      <td>3993329.0</td>\n      <td>3.108399e+14</td>\n      <td>6.950592e+14</td>\n      <td>1.554200e+15</td>\n      <td>6.033677e+06</td>\n      <td>0</td>\n      <td>Benign</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>562917</th>\n      <td>102764</td>\n      <td>161.241.192.168</td>\n      <td>0.0</td>\n      <td>1.152.3.122</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-02-04 20:06:56+00:00</td>\n      <td>14504914.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7.770998e+14</td>\n      <td>1.098985e+15</td>\n      <td>1.554200e+15</td>\n      <td>1.450491e+07</td>\n      <td>0</td>\n      <td>Benign</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>562323</th>\n      <td>179528</td>\n      <td>177.30.87.144</td>\n      <td>0.0</td>\n      <td>192.168.1.1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-02-04 20:07:12+00:00</td>\n      <td>113764805.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1503.0</td>\n      <td>1503.0</td>\n      <td>3.885499e+14</td>\n      <td>7.770998e+14</td>\n      <td>1.554200e+15</td>\n      <td>1.443224e+07</td>\n      <td>0</td>\n      <td>Benign</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>562966</th>\n      <td>102117</td>\n      <td>161.130.192.168</td>\n      <td>0.0</td>\n      <td>1.152.3.122</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-02-04 20:07:31+00:00</td>\n      <td>13460934.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7.770998e+14</td>\n      <td>1.098985e+15</td>\n      <td>1.554200e+15</td>\n      <td>1.346093e+07</td>\n      <td>0</td>\n      <td>Benign</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>562324</th>\n      <td>1006517</td>\n      <td>244.15.103.115</td>\n      <td>0.0</td>\n      <td>192.168.1.79</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-02-04 20:09:19+00:00</td>\n      <td>104447217.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.885500e+14</td>\n      <td>7.770999e+14</td>\n      <td>1.554200e+15</td>\n      <td>1.003317e+07</td>\n      <td>0</td>\n      <td>Benign</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>562934</th>\n      <td>68919</td>\n      <td>140.85.192.168</td>\n      <td>0.0</td>\n      <td>1.152.192.168</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-02-04 20:09:26+00:00</td>\n      <td>16157462.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7.770999e+14</td>\n      <td>1.098985e+15</td>\n      <td>1.554200e+15</td>\n      <td>1.615746e+07</td>\n      <td>0</td>\n      <td>Benign</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>562325</th>\n      <td>179528</td>\n      <td>177.30.87.144</td>\n      <td>0.0</td>\n      <td>192.168.1.1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-02-04 20:09:41+00:00</td>\n      <td>103939761.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5.180666e+14</td>\n      <td>8.973176e+14</td>\n      <td>1.554200e+15</td>\n      <td>3.522726e+07</td>\n      <td>0</td>\n      <td>Benign</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>562326</th>\n      <td>1006517</td>\n      <td>244.15.103.115</td>\n      <td>0.0</td>\n      <td>192.168.1.79</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-02-04 20:11:41+00:00</td>\n      <td>70250783.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7.771000e+14</td>\n      <td>1.098985e+15</td>\n      <td>1.554200e+15</td>\n      <td>7.025078e+07</td>\n      <td>0</td>\n      <td>Benign</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>562327</th>\n      <td>179528</td>\n      <td>177.30.87.144</td>\n      <td>0.0</td>\n      <td>192.168.1.1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-02-04 20:11:59+00:00</td>\n      <td>104244396.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.108400e+14</td>\n      <td>6.950593e+14</td>\n      <td>1.554200e+15</td>\n      <td>1.689920e+07</td>\n      <td>0</td>\n      <td>Benign</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>20 rows √ó 85 columns</p>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"print(dataset.class_num_col)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:33.771378Z","iopub.execute_input":"2025-09-17T10:55:33.771705Z","iopub.status.idle":"2025-09-17T10:55:33.785876Z","shell.execute_reply.started":"2025-09-17T10:55:33.771644Z","shell.execute_reply":"2025-09-17T10:55:33.785135Z"}},"outputs":[{"name":"stdout","text":"Class\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"if multi_class:\n    y = df[dataset.class_num_col]\nelse:\n    y = df[dataset.label_col]\n\nif sort_timestamp:\n    X_tr, X_test, y_tr, y_test = train_test_split(\n        df, y, test_size=test_size)\n    \n    X_train, X_val, y_train, y_val = train_test_split(\n        X_tr, y_tr, test_size=validation_size)\nelse:\n    X_tr, X_test, y_tr, y_test = train_test_split(\n        df, y, test_size=test_size, random_state=13, stratify=y)\n    \n    X_train, X_val, y_train, y_val = train_test_split(\n        X_tr, y_tr, test_size=validation_size, random_state=13, stratify=y_tr)\n\ndel df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:33.786654Z","iopub.execute_input":"2025-09-17T10:55:33.786927Z","iopub.status.idle":"2025-09-17T10:55:51.641449Z","shell.execute_reply.started":"2025-09-17T10:55:33.786904Z","shell.execute_reply":"2025-09-17T10:55:51.640807Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"if graph_type == \"line\" and use_node_features:\n    add_centralities(df = X_train, new_path=None, graph_path=None, dataset=dataset, cn_measures=cn_measures, network_features=network_features, create_using=nx.MultiDiGraph())\n    add_centralities(df = X_val, new_path=None, graph_path=None, dataset=dataset, cn_measures=cn_measures, network_features=network_features, create_using=nx.MultiDiGraph())\n    add_centralities(df = X_test, new_path=None, graph_path=None, dataset=dataset, cn_measures=cn_measures, network_features=network_features, create_using=nx.MultiDiGraph())\n    cols_to_norm = list(set(cols_to_norm) | set(network_features))\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:51.642192Z","iopub.execute_input":"2025-09-17T10:55:51.642383Z","iopub.status.idle":"2025-09-17T10:55:51.647415Z","shell.execute_reply.started":"2025-09-17T10:55:51.642368Z","shell.execute_reply":"2025-09-17T10:55:51.646746Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"scaler = StandardScaler()\n\nX_train[cols_to_norm] = scaler.fit_transform(X_train[cols_to_norm])\nX_train['h'] = X_train[ cols_to_norm ].values.tolist()\n\ncols_to_drop = list(set(list(X_train.columns)) - set(list([dataset.label_col, dataset.src_ip_col, dataset.dst_ip_col, dataset.class_num_col, 'h'])))\nX_train.drop(cols_to_drop, axis=1, inplace=True)\n\nX_val[cols_to_norm] = scaler.transform(X_val[cols_to_norm])\nX_val['h'] = X_val[ cols_to_norm ].values.tolist()\nX_val.drop(cols_to_drop, axis=1, inplace=True)\n\nX_test[cols_to_norm] = scaler.transform(X_test[cols_to_norm])\nX_test['h'] = X_test[ cols_to_norm ].values.tolist()\nX_test.drop(cols_to_drop, axis=1, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:51.648226Z","iopub.execute_input":"2025-09-17T10:55:51.648455Z","iopub.status.idle":"2025-09-17T10:56:27.912386Z","shell.execute_reply.started":"2025-09-17T10:55:51.648433Z","shell.execute_reply":"2025-09-17T10:56:27.911507Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"if graph_type == \"window\" or graph_type == \"line\":\n\n    create_weightless_window_graph(\n        df=X_train,\n        dataset=dataset,\n        window_size=window_size,\n        line_graph=graph_type == \"line\",\n        folder_path=os.path.join(folder_path, \"training\"),\n        edge_attr= ['h', dataset.label_col, dataset.class_num_col],\n        file_type=\"pkl\")\n    \n    create_weightless_window_graph(\n        df=X_val,\n        dataset=dataset,\n        window_size=window_size,\n        line_graph=graph_type == \"line\",\n        folder_path=os.path.join(folder_path, \"validation\"),\n        edge_attr= ['h', dataset.label_col, dataset.class_num_col],\n        file_type=\"pkl\")\n    \n    create_weightless_window_graph(\n        df=X_test,\n        dataset=dataset,\n        window_size=window_size,\n        line_graph=graph_type == \"line\",\n        folder_path=os.path.join(folder_path, \"testing\"),\n        edge_attr= ['h', dataset.label_col, dataset.class_num_col],\n        file_type=\"pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:56:27.914683Z","iopub.execute_input":"2025-09-17T10:56:27.914967Z","iopub.status.idle":"2025-09-17T10:56:27.920480Z","shell.execute_reply.started":"2025-09-17T10:56:27.914941Z","shell.execute_reply":"2025-09-17T10:56:27.919865Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"if graph_type == \"flow\":\n\tos.makedirs(folder_path, exist_ok=True)\n\tprint(f\"==>> X_train.shape: {X_train.shape}\")\n\tprint(f\"==>> X_val.shape: {X_val.shape}\")\n\tprint(f\"==>> X_test.shape: {X_test.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-09-17T10:56:27.921183Z","iopub.execute_input":"2025-09-17T10:56:27.921661Z","iopub.status.idle":"2025-09-17T10:56:27.963719Z","shell.execute_reply.started":"2025-09-17T10:56:27.921633Z","shell.execute_reply":"2025-09-17T10:56:27.963137Z"},"trusted":true},"outputs":[{"name":"stdout","text":"==>> X_train.shape: (4333971, 5)\n==>> X_val.shape: (481553, 5)\n==>> X_test.shape: (535059, 5)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"if graph_type == \"flow\":\n    graph_name = \"training_graph\"\n\n    G = nx.from_pandas_edgelist(X_train, dataset.src_ip_col, dataset.dst_ip_col, ['h',dataset.label_col, dataset.class_num_col], create_using=nx.MultiDiGraph())\n    \n    if use_node_features:\n        add_centralities_as_node_features(df=None, G=G, graph_path=None, dataset=dataset, cn_measures=cn_measures)\n        \n        for node in G.nodes():\n            centralities = []\n            for centrality in cn_measures:\n                centralities.append(G.nodes[node].get(centrality, 0)) # Default to 0 if missing\n                \n                # Combine features into a single vector\n            n_feats = np.array(centralities, dtype=np.float32)\n            \n            # Add the new feature to the node\n            G.nodes[node][\"n_feats\"] = n_feats\n            \n    # get netowrk properties\n    graph_measures = calculate_graph_measures(G, f\"{folder_path}/{graph_name}_measures.json\", verbose=True)\n    print(f\"==>> graph_measures: {graph_measures}\")\n\n    # graph_measures = calculate_graph_measures(nx.DiGraph(G), \"datasets/\" + name + \"/training_graph_simple_measures.json\", verbose=True)\n    # print(f\"==>> graph_measures: {graph_measures}\")\n\n    with open(f\"{folder_path}/{graph_name}.pkl\", \"wb\") as f:\n        pickle.dump(G, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:56:27.964392Z","iopub.execute_input":"2025-09-17T10:56:27.964636Z","iopub.status.idle":"2025-09-17T11:11:39.152300Z","shell.execute_reply.started":"2025-09-17T10:56:27.964621Z","shell.execute_reply":"2025-09-17T11:11:39.151413Z"}},"outputs":[{"name":"stdout","text":"calculated betweenness\ncalculated degree\ncalculated closeness\ncalculated pagerank\ncalculated k_truss\n==>> calculated degrees, in 0.2526018190001196 seconds\n==>> graph_measures: {'number_of_nodes': 132311, 'number_of_edges': 4333971, 'max_degree': 1674190, 'avg_degree': 65.51187731934608, 'density': 0.0002475696369108385}\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"if graph_type == \"flow\":\n    graph_name = \"validation_graph\"\n\n    G = nx.from_pandas_edgelist(X_val, dataset.src_ip_col, dataset.dst_ip_col, ['h',dataset.label_col, dataset.class_num_col], create_using=nx.MultiDiGraph())\n    \n    if use_node_features:\n        add_centralities_as_node_features(df=None, G=G, graph_path=None, dataset=dataset, cn_measures=cn_measures)\n        \n        for node in G.nodes():\n            centralities = []\n            for centrality in cn_measures:\n                centralities.append(G.nodes[node].get(centrality, 0)) # Default to 0 if missing\n                \n                # Combine features into a single vector\n            n_feats = np.array(centralities, dtype=np.float32)\n            \n            # Add the new feature to the node\n            G.nodes[node][\"n_feats\"] = n_feats\n            \n    # get netowrk properties\n    graph_measures = calculate_graph_measures(G, f\"{folder_path}/{graph_name}_measures.json\", verbose=True)\n    print(f\"==>> graph_measures: {graph_measures}\")\n\n    # graph_measures = calculate_graph_measures(nx.DiGraph(G), \"datasets/\" + name + \"/training_graph_simple_measures.json\", verbose=True)\n    # print(f\"==>> graph_measures: {graph_measures}\")\n\n    with open(f\"{folder_path}/{graph_name}.pkl\", \"wb\") as f:\n        pickle.dump(G, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:11:39.153205Z","iopub.execute_input":"2025-09-17T11:11:39.153482Z","iopub.status.idle":"2025-09-17T11:12:10.335973Z","shell.execute_reply.started":"2025-09-17T11:11:39.153454Z","shell.execute_reply":"2025-09-17T11:12:10.335161Z"}},"outputs":[{"name":"stdout","text":"calculated betweenness\ncalculated degree\ncalculated closeness\ncalculated pagerank\ncalculated k_truss\n==>> calculated degrees, in 0.04900716699921759 seconds\n==>> graph_measures: {'number_of_nodes': 37769, 'number_of_edges': 481553, 'max_degree': 186329, 'avg_degree': 25.499907331409357, 'density': 0.0003375861487424454}\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"if graph_type == \"flow\":\n    graph_name = \"testing_graph\"\n    \n    G = nx.from_pandas_edgelist(X_test, dataset.src_ip_col, dataset.dst_ip_col, ['h', dataset.label_col, dataset.class_num_col], create_using=nx.MultiDiGraph())\n    \n    if use_node_features:\n        add_centralities_as_node_features(df=None, G=G, graph_path=None, dataset=dataset, cn_measures=cn_measures)\n        \n        for node in G.nodes():\n            centralities = []\n            for centrality in cn_measures:\n                centralities.append(G.nodes[node].get(centrality, 0)) # Default to 0 if missing\n                \n                # Combine features into a single vector\n            n_feats = np.array(centralities, dtype=np.float32)\n            \n            # Add the new feature to the node\n            G.nodes[node][\"n_feats\"] = n_feats\n            \n    graph_measures = calculate_graph_measures(G, f\"{folder_path}/{graph_name}_measures.json\", verbose=True)\n    print(f\"==>> graph_measures: {graph_measures}\")\n    \n    # graph_measures = calculate_graph_measures(nx.DiGraph(G_test), \"datasets/\" + name + \"/testing_graph_simple_measures.json\", verbose=True)\n    # print(f\"==>> graph_measures: {graph_measures}\")\n    \n    with open(f\"{folder_path}/{graph_name}.pkl\", \"wb\") as f:\n        pickle.dump(G, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:12:10.336878Z","iopub.execute_input":"2025-09-17T11:12:10.337184Z","iopub.status.idle":"2025-09-17T11:12:51.904721Z","shell.execute_reply.started":"2025-09-17T11:12:10.337158Z","shell.execute_reply":"2025-09-17T11:12:51.904117Z"}},"outputs":[{"name":"stdout","text":"calculated betweenness\ncalculated degree\ncalculated closeness\ncalculated pagerank\ncalculated k_truss\n==>> calculated degrees, in 0.059191026000007696 seconds\n==>> graph_measures: {'number_of_nodes': 41240, 'number_of_edges': 535059, 'max_degree': 206269, 'avg_degree': 25.94854510184287, 'density': 0.00031461171587384356}\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import json\n\nprint(\"-------------------------- SAVING VARIABLES -------------------------- \")\ndata = {\n    \"dataset_path\": dataset_path,\n    \"folder_path\": folder_path,\n    \"name\": name,\n    \"g_type\": g_type\n}\n\n# Save to JSON\nwith open(\"config.json\", \"w\") as f:\n    json.dump(data, f, indent=4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:12:51.905497Z","iopub.execute_input":"2025-09-17T11:12:51.905726Z","iopub.status.idle":"2025-09-17T11:12:51.911293Z","shell.execute_reply.started":"2025-09-17T11:12:51.905709Z","shell.execute_reply":"2025-09-17T11:12:51.910409Z"}},"outputs":[{"name":"stdout","text":"-------------------------- SAVING VARIABLES -------------------------- \n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"print(G)\nprint(y)\nprint(X_train, X_val, y_train, y_val)\ndel G\ndel y \ndel X_train, X_val, y_train, y_val","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:12:51.912210Z","iopub.execute_input":"2025-09-17T11:12:51.912501Z","iopub.status.idle":"2025-09-17T11:12:55.270622Z","shell.execute_reply.started":"2025-09-17T11:12:51.912473Z","shell.execute_reply":"2025-09-17T11:12:55.269981Z"}},"outputs":[{"name":"stdout","text":"MultiDiGraph with 41240 nodes and 535059 edges\n562313    0\n562315    0\n562314    0\n562317    0\n562316    0\n         ..\n291672    0\n329941    0\n329974    0\n341650    0\n334132    0\nName: Class, Length: 5350583, dtype: int64\n                Src IP         Dst IP  Label  Class  \\\n2119601   192.168.1.39  192.168.1.190      1      9   \n179170   26.99.192.168   1.169.74.125      0      0   \n653771   192.168.1.193   192.168.1.33      0      0   \n4655754  192.168.1.195   192.168.1.39      0      0   \n1725979   192.168.1.36  192.168.1.152      1      9   \n...                ...            ...    ...    ...   \n1398075   192.168.1.36  52.28.231.150      0      0   \n436919   192.168.1.193   192.168.1.33      1      1   \n4973201   192.168.1.35  192.168.1.190      1      6   \n2925346   192.168.1.35  192.168.1.195      1      6   \n903134    192.168.1.32  192.168.1.195      1      6   \n\n                                                         h  \n2119601  [0.0015098238038789129, -0.031140547671802212,...  \n179170   [0.001500240423299663, -0.03177462197291609, -...  \n653771   [0.0015002757863276307, -0.01358667433961119, ...  \n4655754  [0.001500240423299663, -0.03177462197291609, -...  \n1725979  [0.001500240423299663, -0.03177462197291609, -...  \n...                                                    ...  \n1398075  [0.0015089220466657364, -0.027745654240126955,...  \n436919   [0.0015062344565401902, -0.025363020727639164,...  \n4973201  [0.001500240423299663, -0.031774528629212144, ...  \n2925346  [0.00150036419389755, -0.027140282846510738, -...  \n903134   [0.0015004172384395016, -0.028833166831536753,...  \n\n[4333971 rows x 5 columns]                 Src IP           Dst IP  Label  Class  \\\n4402272   192.168.1.35      192.168.1.1      1      9   \n4003970  192.168.1.190     192.168.1.35      0      0   \n4246791  192.168.1.190     192.168.1.39      0      0   \n580195   192.168.1.190  205.251.194.154      0      0   \n3199926  192.168.1.190     192.168.1.35      0      0   \n...                ...              ...    ...    ...   \n2531645  192.168.1.190     192.168.1.35      0      0   \n3413268  192.168.1.195     192.168.1.39      0      0   \n1843423  192.168.1.190     192.168.1.32      0      0   \n5002472   192.168.1.35    192.168.1.190      1      9   \n5166267   192.168.1.35   18.194.169.124      1      9   \n\n                                                         h  \n4402272  [0.001535957081547052, -0.03173449750236181, -...  \n4003970  [0.001500240423299663, -0.03177462197291609, -...  \n4246791  [0.001500240423299663, -0.03177462197291609, -...  \n580195   [0.001500240423299663, -0.03177462197291609, -...  \n3199926  [0.001500240423299663, -0.03177462197291609, -...  \n...                                                    ...  \n2531645  [0.001500240423299663, -0.03177462197291609, -...  \n3413268  [0.001500240423299663, -0.03177462197291609, -...  \n1843423  [0.00150306946553708, -0.031292758174004336, -...  \n5002472  [0.001541243854228225, -0.030853112867649562, ...  \n5166267  [0.0015002934678416146, -0.02805765102050359, ...  \n\n[481553 rows x 5 columns] 2119601    9\n179170     0\n653771     0\n4655754    0\n1725979    9\n          ..\n1398075    0\n436919     1\n4973201    6\n2925346    6\n903134     6\nName: Class, Length: 4333971, dtype: int64 4402272    9\n4003970    0\n4246791    0\n580195     0\n3199926    0\n          ..\n2531645    0\n3413268    0\n1843423    0\n5002472    9\n5166267    9\nName: Class, Length: 481553, dtype: int64\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"print(scaler)\ndel scaler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:12:55.271278Z","iopub.execute_input":"2025-09-17T11:12:55.271478Z","iopub.status.idle":"2025-09-17T11:12:55.293989Z","shell.execute_reply.started":"2025-09-17T11:12:55.271462Z","shell.execute_reply":"2025-09-17T11:12:55.293465Z"}},"outputs":[{"name":"stdout","text":"StandardScaler()\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import psutil\n\nram_gb = psutil.virtual_memory().used / 1e9\nprint(f\"Current RAM usage: {ram_gb:.2f} GB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:12:55.294719Z","iopub.execute_input":"2025-09-17T11:12:55.294943Z","iopub.status.idle":"2025-09-17T11:12:55.299486Z","shell.execute_reply.started":"2025-09-17T11:12:55.294921Z","shell.execute_reply":"2025-09-17T11:12:55.298857Z"}},"outputs":[{"name":"stdout","text":"Current RAM usage: 17.04 GB\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"import gc\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:12:55.300213Z","iopub.execute_input":"2025-09-17T11:12:55.300458Z","iopub.status.idle":"2025-09-17T11:12:56.058251Z","shell.execute_reply.started":"2025-09-17T11:12:55.300441Z","shell.execute_reply":"2025-09-17T11:12:56.057495Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"625960"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"print(\"-------------------------- CLEARING THE MEMORY -------------------------- \")\nprint(\"Clear all variables (like restarting kernel)\")\n%reset -f\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:12:56.059196Z","iopub.execute_input":"2025-09-17T11:12:56.059529Z","iopub.status.idle":"2025-09-17T11:12:57.421819Z","shell.execute_reply.started":"2025-09-17T11:12:56.059503Z","shell.execute_reply":"2025-09-17T11:12:57.421268Z"}},"outputs":[{"name":"stdout","text":"-------------------------- CLEARING THE MEMORY -------------------------- \nClear all variables (like restarting kernel)\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"import psutil\nram_gb = psutil.virtual_memory().used / 1e9\nprint(f\"Current RAM usage: {ram_gb:.2f} GB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:12:57.422536Z","iopub.execute_input":"2025-09-17T11:12:57.422770Z","iopub.status.idle":"2025-09-17T11:12:57.426884Z","shell.execute_reply.started":"2025-09-17T11:12:57.422746Z","shell.execute_reply":"2025-09-17T11:12:57.426134Z"}},"outputs":[{"name":"stdout","text":"Current RAM usage: 5.35 GB\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"<h1>TRAINNING</h1>","metadata":{}},{"cell_type":"code","source":"print(\"-------------------------- RETRIVING VARIABLES --------------------------\")\nimport json\nwith open(\"config.json\", \"r\") as f:\n    config = json.load(f)\n\n# Access the values\ndataset_path = config[\"dataset_path\"]\nfolder_path = config[\"folder_path\"]\nname = config[\"name\"]\ng_type = config[\"g_type\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:12:57.427621Z","iopub.execute_input":"2025-09-17T11:12:57.428080Z","iopub.status.idle":"2025-09-17T11:12:57.450165Z","shell.execute_reply.started":"2025-09-17T11:12:57.428057Z","shell.execute_reply":"2025-09-17T11:12:57.449555Z"}},"outputs":[{"name":"stdout","text":"-------------------------- RETRIVING VARIABLES --------------------------\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"print(dataset_path)\nprint(folder_path)\nprint(name)\nprint(g_type)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:12:57.450929Z","iopub.execute_input":"2025-09-17T11:12:57.451200Z","iopub.status.idle":"2025-09-17T11:12:57.470248Z","shell.execute_reply.started":"2025-09-17T11:12:57.451172Z","shell.execute_reply":"2025-09-17T11:12:57.469153Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/cic-ton-iot-gnn\n/kaggle/working/cic_ton_iot/flow__multi_class__n_feats__sorted\ncic_ton_iot\nflow__multi_class__n_feats__sorted\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"dataset_name = name\ndataset_folder = dataset_path\ngraph_folder=folder_path\nmodel_path = \"/kaggle/input/gnn-nids-wandb/pytorch/default/6/\"\n\nusing_wandb = False\n# save_top_k = 5\nsave_top_k = 1\n# save_top_k = 4 save best 4 test model from the traininng based on f1_score \n# early_stopping_patience = max_epochs = 20\nearly_stopping_patience = max_epochs = 500\n# early_stopping_patience = 200 all - epochs = 500 - lr = 0.005\nlearning_rate = 0.005\nweight_decay = 0.01\nndim_out = [32, 32]\nnum_layers = 2\nnumber_neighbors = [25, 10]\ndropout = 0.5\nresidual = True\nmulti_class = True\naggregation = \"mean\"\n# aggregation = \"sum\"\n\nuse_centralities_nfeats = False\n\n# g_type = \"flow\"\n\n# if multi_class:\n#     g_type += \"__multi_class\"\n\n# if use_centralities_nfeats:\n#     g_type += \"__n_feats\"\n\n# g_type += \"__unsorted\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:12:57.471240Z","iopub.execute_input":"2025-09-17T11:12:57.471480Z","iopub.status.idle":"2025-09-17T11:12:57.486523Z","shell.execute_reply.started":"2025-09-17T11:12:57.471464Z","shell.execute_reply":"2025-09-17T11:12:57.485949Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"pip uninstall -y torch torchvision torchaudio torchdata pytorch-lightning --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:12:57.487236Z","iopub.execute_input":"2025-09-17T11:12:57.487479Z","iopub.status.idle":"2025-09-17T11:13:05.740989Z","shell.execute_reply.started":"2025-09-17T11:12:57.487456Z","shell.execute_reply":"2025-09-17T11:13:05.740049Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping torchdata as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"pip install torch==2.1.0+cu118 torchvision==0.16.0+cu118 torchaudio==2.1.0+cu118 -f https://download.pytorch.org/whl/torch_stable.html --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:13:05.742332Z","iopub.execute_input":"2025-09-17T11:13:05.742664Z","iopub.status.idle":"2025-09-17T11:14:36.221946Z","shell.execute_reply.started":"2025-09-17T11:13:05.742628Z","shell.execute_reply":"2025-09-17T11:14:36.221058Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"import torch\nprint(\"torch version:\", torch.__version__)\nprint(\"CUDA available?\", torch.cuda.is_available())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:14:36.223112Z","iopub.execute_input":"2025-09-17T11:14:36.223374Z","iopub.status.idle":"2025-09-17T11:14:39.468721Z","shell.execute_reply.started":"2025-09-17T11:14:36.223348Z","shell.execute_reply":"2025-09-17T11:14:39.468053Z"}},"outputs":[{"name":"stdout","text":"torch version: 2.1.0+cu118\nCUDA available? True\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"pip install dgl -f https://data.dgl.ai/wheels/torch-2.1/cu118/repo.html --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:14:39.469496Z","iopub.execute_input":"2025-09-17T11:14:39.470012Z","iopub.status.idle":"2025-09-17T11:14:42.752760Z","shell.execute_reply.started":"2025-09-17T11:14:39.469993Z","shell.execute_reply":"2025-09-17T11:14:42.751933Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"!apt update -qq 2>/dev/null >/dev/null;\n!apt install -y libcusparse11 -qq 2>/dev/null >/dev/null;","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:14:42.753776Z","iopub.execute_input":"2025-09-17T11:14:42.754070Z","iopub.status.idle":"2025-09-17T11:14:50.372306Z","shell.execute_reply.started":"2025-09-17T11:14:42.754044Z","shell.execute_reply":"2025-09-17T11:14:50.371267Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"pip install --upgrade -q pytorch-lightning==2.5.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:14:50.379746Z","iopub.execute_input":"2025-09-17T11:14:50.379997Z","iopub.status.idle":"2025-09-17T11:14:54.356915Z","shell.execute_reply.started":"2025-09-17T11:14:50.379975Z","shell.execute_reply":"2025-09-17T11:14:54.356139Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"!pip install --upgrade -q wandb==0.19.6","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:14:54.357884Z","iopub.execute_input":"2025-09-17T11:14:54.358125Z","iopub.status.idle":"2025-09-17T11:15:00.565221Z","shell.execute_reply.started":"2025-09-17T11:14:54.358084Z","shell.execute_reply":"2025-09-17T11:15:00.564337Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# import wandb\n# wandb.login(key=\"c20e6d52d61e51ff11f4391b5870097badb9092f\")\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"mohammad_wandb_secret\")\n\nwandb.login(key=secret_value_0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:15:00.566354Z","iopub.execute_input":"2025-09-17T11:15:00.566633Z","iopub.status.idle":"2025-09-17T11:15:09.609077Z","shell.execute_reply.started":"2025-09-17T11:15:00.566605Z","shell.execute_reply":"2025-09-17T11:15:09.608364Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohammad-fleity\u001b[0m (\u001b[33mmohammad-fleity-lebanese-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"import os\nimport json\nimport pickle\nimport time\nimport timeit\nimport importlib.util\nimport sys\nimport random\nimport numpy as np\nimport csv\nos.environ[\"DGLBACKEND\"] = \"pytorch\"\n\nimport torch.nn as nn\nimport torch\nimport warnings\nimport wandb\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\nseed = 42  # or any constant value\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\nrun_dtime = time.strftime(\"%Y%m%d-%H%M%S\")\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:15:09.610043Z","iopub.execute_input":"2025-09-17T11:15:09.610581Z","iopub.status.idle":"2025-09-17T11:15:14.997158Z","shell.execute_reply.started":"2025-09-17T11:15:09.610561Z","shell.execute_reply":"2025-09-17T11:15:14.996428Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/gnn-nids/pytorch/default/1/time_efficiency.ipynb\n/kaggle/input/gnn-nids/pytorch/default/1/.gitignore\n/kaggle/input/gnn-nids/pytorch/default/1/prepare_graph_files.ipynb\n/kaggle/input/gnn-nids/pytorch/default/1/main.py\n/kaggle/input/gnn-nids/pytorch/default/1/README.md\n/kaggle/input/gnn-nids/pytorch/default/1/dataset_properties.ipynb\n/kaggle/input/gnn-nids/pytorch/default/1/pre_processing.ipynb\n/kaggle/input/gnn-nids/pytorch/default/1/concat_properties.py\n/kaggle/input/gnn-nids/pytorch/default/1/requirements.txt\n/kaggle/input/gnn-nids/pytorch/default/1/graph_properties.ipynb\n/kaggle/input/gnn-nids/pytorch/default/1/results_analysis.ipynb\n/kaggle/input/gnn-nids/pytorch/default/1/testing_dfs/cic_ton_iot_5_percent.parquet\n/kaggle/input/gnn-nids/pytorch/default/1/testing_dfs/cic_ids_2017_5_percent.parquet\n/kaggle/input/gnn-nids/pytorch/default/1/src/lightning_model.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/utils.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/models.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/__init__.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/lightning_data.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/models old.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/models v2.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/models edge_update.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/dataset/dataset_measures.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/dataset/dataset_utils.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/dataset/features_analysis.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/dataset/ecdf.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/dataset/__init__.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/dataset/dataset_info.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/graph/graph_construction.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/graph/graph_measures.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/graph/centralities.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/graph/graph_utils.py\n/kaggle/input/gnn-nids/pytorch/default/1/src/graph/__init__.py\n/kaggle/input/cic-ids-2017-gnn-2/df_properties.json\n/kaggle/input/cic-ids-2017-gnn-2/cic_ids_2017.parquet\n/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\n/kaggle/input/cic-ids-20117/df_properties.json\n/kaggle/input/cic-ids-20117/labels_names.pkl\n/kaggle/input/cic-ton-iot-gnn/cic_ton_iot.parquet\n/kaggle/input/cic-ton-iot-gnn/df_properties.json\n/kaggle/input/cic-ton-iot-gnn/labels_names.pkl\n/kaggle/input/gnn-nids-wandb/pytorch/default/6/lightning_model.py\n/kaggle/input/gnn-nids-wandb/pytorch/default/6/models.py\n/kaggle/input/gnn-nids-wandb/pytorch/default/6/lightning_data.py\n/kaggle/input/gnn-nids-wandb/pytorch/default/6/dataset_info.py\n/kaggle/input/githubrepofiles/pytorch/default/1/time_efficiency.ipynb\n/kaggle/input/githubrepofiles/pytorch/default/1/.gitignore\n/kaggle/input/githubrepofiles/pytorch/default/1/prepare_graph_files.ipynb\n/kaggle/input/githubrepofiles/pytorch/default/1/main.py\n/kaggle/input/githubrepofiles/pytorch/default/1/README.md\n/kaggle/input/githubrepofiles/pytorch/default/1/dataset_properties.ipynb\n/kaggle/input/githubrepofiles/pytorch/default/1/pre_processing.ipynb\n/kaggle/input/githubrepofiles/pytorch/default/1/concat_properties.py\n/kaggle/input/githubrepofiles/pytorch/default/1/requirements.txt\n/kaggle/input/githubrepofiles/pytorch/default/1/graph_properties.ipynb\n/kaggle/input/githubrepofiles/pytorch/default/1/results_analysis.ipynb\n/kaggle/input/githubrepofiles/pytorch/default/1/testing_dfs/cic_ton_iot_5_percent.parquet\n/kaggle/input/githubrepofiles/pytorch/default/1/testing_dfs/cic_ids_2017_5_percent.parquet\n/kaggle/input/githubrepofiles/pytorch/default/1/src/lightning_model.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/utils.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/models.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/__init__.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/lightning_data.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/models old.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/models v2.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/models edge_update.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/dataset/dataset_measures.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/dataset/dataset_utils.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/dataset/features_analysis.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/dataset/ecdf.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/dataset/__init__.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/dataset/dataset_info.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/graph/graph_construction.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/graph/graph_measures.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/graph/centralities.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/graph/graph_utils.py\n/kaggle/input/githubrepofiles/pytorch/default/1/src/graph/__init__.py\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:15:14.997964Z","iopub.execute_input":"2025-09-17T11:15:14.998867Z","iopub.status.idle":"2025-09-17T11:15:15.032980Z","shell.execute_reply.started":"2025-09-17T11:15:14.998845Z","shell.execute_reply":"2025-09-17T11:15:15.032416Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"# import kagglehub\n\n# # Download latest version\n# path = kagglehub.model_download(\"mortadaphdtermos/gnn-nids-wandb/pyTorch/default\")\n\n# print(\"Path to model files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:15:15.033672Z","iopub.execute_input":"2025-09-17T11:15:15.033902Z","iopub.status.idle":"2025-09-17T11:15:15.037200Z","shell.execute_reply.started":"2025-09-17T11:15:15.033885Z","shell.execute_reply":"2025-09-17T11:15:15.036413Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"def add_lib(module_name, path):\n    spec = importlib.util.spec_from_file_location(module_name, path)\n    dataset_info = importlib.util.module_from_spec(spec)\n    sys.modules[module_name] = dataset_info\n    spec.loader.exec_module(dataset_info)\n\nadd_lib(\"dataset_info\", model_path + \"dataset_info.py\")\nadd_lib(\"lightning_data\", model_path + \"lightning_data.py\")\nadd_lib(\"lightning_model\", model_path + \"lightning_model.py\")\nadd_lib(\"models\", model_path + \"models.py\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:15:15.037890Z","iopub.execute_input":"2025-09-17T11:15:15.038086Z","iopub.status.idle":"2025-09-17T11:15:15.566045Z","shell.execute_reply.started":"2025-09-17T11:15:15.038072Z","shell.execute_reply":"2025-09-17T11:15:15.565450Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"from models import EGAT, EGCN, EGRAPHSAGE\nfrom lightning_model import GraphModel\nfrom lightning_data import GraphDataModule\nfrom dataset_info import datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:15:15.566772Z","iopub.execute_input":"2025-09-17T11:15:15.567022Z","iopub.status.idle":"2025-09-17T11:15:15.570759Z","shell.execute_reply.started":"2025-09-17T11:15:15.566995Z","shell.execute_reply":"2025-09-17T11:15:15.570034Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"print(os.path.join(dataset_folder, \"labels_names.pkl\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:15:15.571618Z","iopub.execute_input":"2025-09-17T11:15:15.572223Z","iopub.status.idle":"2025-09-17T11:15:15.588274Z","shell.execute_reply.started":"2025-09-17T11:15:15.572205Z","shell.execute_reply":"2025-09-17T11:15:15.587724Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/cic-ton-iot-gnn/labels_names.pkl\n","output_type":"stream"}],"execution_count":48},{"cell_type":"markdown","source":"<h2>E_GAT</h2>","metadata":{}},{"cell_type":"code","source":"# print(\"----------------------- TRAINING E_GAT --------------------------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:15:15.589154Z","iopub.execute_input":"2025-09-17T11:15:15.589400Z","iopub.status.idle":"2025-09-17T11:15:15.605763Z","shell.execute_reply.started":"2025-09-17T11:15:15.589383Z","shell.execute_reply":"2025-09-17T11:15:15.605060Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n\ndataset = datasets[dataset_name]\n\n# Hyperparameters\n\nactivation = F.relu\ngraphs_folder=graph_folder\n# graphs_folder = os.path.join(dataset_folder, g_type)\n# if dataset_name == \"cic_ids_2017\":\n#     graphs_folder = \"/kaggle/working/cic_ids_2017/flow__multi_class__n_feats__unsorted\"\nlogs_folder = os.path.join(\"logs\", dataset.name)\nos.makedirs(logs_folder, exist_ok=True)\nwandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\nos.makedirs(wandb_runs_path, exist_ok=True)\n\nlabels_mapping = {0: \"Normal\", 1: \"Attack\"}\nnum_classes = 2\nif multi_class:\n    with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n    # /kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\n    # with open(\"/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\", \"rb\") as f:\n        labels_names = pickle.load(f)\n    labels_mapping = labels_names[0]\nnum_classes = len(labels_mapping)\n\ndataset_kwargs = dict(\n    use_node_features=use_centralities_nfeats,\n    multi_class=True,\n    using_masking=False,\n    masked_class=2,\n    num_workers=0,\n    label_col=dataset.label_col,\n    class_num_col=dataset.class_num_col,\n    device='cuda' if torch.cuda.is_available() else \"cpu\"\n)\n\ndata_module = GraphDataModule(\n    graphs_folder, batch_size=1, **dataset_kwargs)\ndata_module.setup()\n\nndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\nedim = next(iter(data_module.train_dataloader())).edata['h'].shape[-1]\n\n# my_models = {\n#     # \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation,\n#     #               dropout, residual, num_classes),\n#     # f\"e_graphsage_{aggregation}\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                          residual, num_classes, num_neighbors=number_neighbors, aggregation=aggregation),\n#     # f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                                      residual, num_classes, num_neighbors=None, aggregation=aggregation),\n#     # \"e_gat_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                   # residual, num_classes, num_neighbors=number_neighbors),\n#     \"e_gat_no_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes, num_neighbors=None),\n# }\nmy_models={\n    \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes),\n    f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,residual, num_classes, num_neighbors=None, aggregation=aggregation),\n    f\"e_graphsage_{aggregation}\": EGRAPHSAGE(\n        ndim, edim, ndim_out, num_layers, activation, dropout,\n        residual, num_classes, num_neighbors=number_neighbors,\n        aggregation=aggregation\n    ),\n    f\"e_graphsage_sum_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,residual, num_classes, num_neighbors=None, aggregation=\"sum\"),\n    f\"e_graphsage_sum\": EGRAPHSAGE(\n        ndim, edim, ndim_out, num_layers, activation, dropout,\n        residual, num_classes, num_neighbors=number_neighbors,\n        aggregation=\"sum\"\n    ),\n    \"e_gat_sampling\": EGAT(\n        ndim, edim, ndim_out, num_layers, activation, dropout,\n        residual, num_classes, num_neighbors=number_neighbors\n    ),\n    \"e_gat_no_sampling\": EGAT(\n        ndim, edim, ndim_out, num_layers, activation, dropout,\n        residual, num_classes, num_neighbors=None\n    ),\n}\n\ncriterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n\nfor model_name, model in my_models.items():\n    print(\"----------------------- TRAINING \",model_name,\" --------------------------\")\n    config = {\n        \"run_dtime\": run_dtime,\n        \"type\": \"GNN\",\n        \"model_name\": model_name,\n        \"max_epochs\": max_epochs,\n        \"learning_rate\": learning_rate,\n        \"weight_decay\": weight_decay,\n        \"ndim_out\": ndim_out,\n        \"num_layers\": num_layers,\n        \"number_neighbors\": number_neighbors,\n        \"activation\": activation.__name__,\n        \"dropout\": dropout,\n        \"residual\": residual,\n        \"multi_class\": multi_class,\n        \"aggregation\": aggregation,\n        \"early_stopping_patience\": early_stopping_patience,\n        \"use_centralities_nfeats\": use_centralities_nfeats,}\n\n    process = psutil.Process(os.getpid())  # current process\n    start_time = time.time()\n    start_mem = process.memory_info().rss / (1024 ** 2)  # in MB\n    graph_model = GraphModel(model, criterion, learning_rate, config, model_name,\n                             labels_mapping, weight_decay=weight_decay, using_wandb=using_wandb, norm=False, multi_class=True)\n\n    \n    if using_wandb:\n        wandb_logger = WandbLogger(\n            project=f\"GNN-Analysis-{dataset.name}\", # Project Name Wandb\n            name=f\"{model_name}---{g_type}\",\n            config=config,\n            save_dir=wandb_runs_path\n        )\n    else:\n        wandb_logger = None\n        \n    f1_checkpoint_callback = ModelCheckpoint(\n        monitor=\"val_f1_score\",\n        mode=\"max\",\n        filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n        save_top_k=save_top_k,\n        verbose=False,\n    )\n    early_stopping_callback = EarlyStopping(\n        monitor=\"val_loss\",\n        mode=\"min\",\n        patience=early_stopping_patience,\n        verbose=False,\n    )\n\n\n    trainer = pl.Trainer(\n        max_epochs=max_epochs,\n        num_sanity_val_steps=0,\n        log_every_n_steps=0,\n        callbacks=[\n            f1_checkpoint_callback,\n            early_stopping_callback\n        ],\n        default_root_dir=logs_folder,\n        logger=wandb_logger if using_wandb else None,\n    )\n\n    trainer.fit(graph_model, datamodule=data_module)\n    \n    test_results = []\n    \n    # for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n    #     graph_model.test_prefix = f\"best_f1_{i}\"\n    #     results = trainer.test(\n    #         graph_model, datamodule=data_module, ckpt_path=k)\n    #     test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n\n    # logs = {\n    #     \"median_f1_of_best_f1\": np.median(test_results),\n    #     \"max_f1_of_best_f1\": np.max(test_results),\n    #     \"avg_f1_of_best_f1\": np.mean(test_results)\n    # }\n\n    # if using_wandb:\n    #     wandb.log(logs)\n    #     wandb.finish()\n\n    print(process)\n    for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n        graph_model.test_prefix = f\"best_f1_{i}\"\n        results = trainer.test(graph_model, datamodule=data_module, ckpt_path=k)\n        test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n\n        print(f\"\\n--- Test Run {i+1} ---\")\n        print(\"graph_model: \", graph_model)\n        print(\"data_module: \", data_module)\n        print(\"results: \", results)\n        logs = {\n        \"median_f1_of_best_f1\": np.median(test_results),\n        \"max_f1_of_best_f1\": np.max(test_results),\n        \"avg_f1_of_best_f1\": np.mean(test_results),\n            \n    }\n    print(\"test_results: \", test_results)\n    end_time = time.time()\n    end_mem = process.memory_info().rss / (1024 ** 2)  # in MB\n    \n    elapsed_time = end_time - start_time\n    mem_used = end_mem - start_mem\n    print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n    print(f\"Memory used: {mem_used:.2f} MB (current RSS: {end_mem:.2f} MB)\")\n    metric_dir= \"/kaggle/working/temp\"\n    os.makedirs(metric_dir, exist_ok=True)\n    \n    csv_path = os.path.join(metric_dir, f\"{model_name}_metrics.csv\")\n    \n    with open(csv_path, mode=\"w\", newline=\"\") as f:\n        writer = csv.writer(f)\n        writer.writerow([\"TEST_ACCURACY\", test_results[0]])\n        writer.writerow([\"MEMORY_USED\", f\"{mem_used:.2f} MB\"])\n        writer.writerow([\"TIME_CONSUMED\", f\"{elapsed_time:.2f} s.\"])\n    \n    print(f\"Metrics saved to {csv_path}\")\n\n\n    if using_wandb:\n        wandb.log(logs)\n        wandb.finish()\n    else:\n        trainer.logger.log_metrics(logs, step=trainer.global_step)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\n\n# labels_map = {\n#     \"0\": \"BENIGN\",\n#     \"1\": \"Bot\",\n#     \"2\": \"DDoS\",\n#     \"3\": \"DoS GoldenEye\",\n#     \"4\": \"DoS Hulk\",\n#     \"5\": \"DoS Slowhttptest\",\n#     \"6\": \"DoS slowloris\",\n#     \"7\": \"FTP-Patator\",\n#     \"8\": \"Heartbleed\",\n#     \"9\": \"Infiltration\",\n#     \"10\": \"PortScan\",\n#     \"11\": \"SSH-Patator\",\n#     \"12\": \"Web Attack \\ufffd Brute Force\",\n#     \"13\": \"Web Attack \\ufffd Sql Injection\",\n#     \"14\": \"Web Attack \\ufffd XSS\"\n# }\nlabels_map = {\n    \"0\": \"Benign\",\n    \"1\": \"backdoor\",\n    \"2\": \"ddos\",\n    \"3\": \"dos\",\n    \"4\": \"injection\",\n    \"5\": \"mitm\",\n    \"6\": \"password\",\n    \"7\": \"ransomware\",\n    \"8\": \"scanning\",\n    \"9\": \"xss\"\n}\ndef json_to_csv_per_file(json_folder, output_folder):\n    os.makedirs(output_folder, exist_ok=True)\n\n    for filename in os.listdir(json_folder):\n        if filename.endswith(\".json\"):\n            filepath = os.path.join(json_folder, filename)\n            with open(filepath, \"r\") as f:\n                data = json.load(f)\n\n            rows = []\n            # --- Per-class metrics ---\n            classification_report=data[\"classification_report\"]\n            # print(classification_report)\n            if \"results_fpr_fnr\" in data and \"per_class\" in data[\"results_fpr_fnr\"]:\n                # print(data[\"results_fpr_fnr\"])\n                for cls, metrics in data[\"results_fpr_fnr\"][\"per_class\"].items():\n                    attack=labels_map.get(cls,cls)\n                    print(f\"{attack}\")\n                    metrics_d=classification_report.get(attack)\n                    row = {\n                        \"ATTACK TYPE\": attack,\n                        \"precision\": metrics_d.get(\"precision\"),\n                        \"recall\": metrics_d.get(\"recall\"),\n                        \"f1_score\": metrics_d.get(\"f1-score\"),\n                        \"support\": metrics_d.get(\"support\"),\n                        \"FPR\": metrics.get(\"FPR\"),\n                        \"FNR\": metrics.get(\"FNR\"),\n                    }\n                    # print()\n                    # print(metrics_d.get(\"precision\"))\n                    # print()\n                    rows.append(row)\n\n            df_classes = pd.DataFrame(rows)\n            weighted_avg=classification_report['weighted avg']\n            # print(data)\n            # --- Global / test metrics ---\n            global_metrics = {\n                \"test_weighted_f1\": data.get(\"test_weighted_f1\"),\n                'test_accuracy':data.get('accuracy'),\n                'precision': weighted_avg['precision'],\n                'recall': weighted_avg['recall'], \n                'support': weighted_avg['support'],\n                # \"test_fpr\": data.get(\"test_fpr\"),\n                # \"test_fnr\": data.get(\"test_fnr\"),\n                \"global_FPR\": data[\"results_fpr_fnr\"][\"global\"].get(\"FPR\") if \"results_fpr_fnr\" in data and \"global\" in data[\"results_fpr_fnr\"] else None,\n                \"global_FNR\": data[\"results_fpr_fnr\"][\"global\"].get(\"FNR\") if \"results_fpr_fnr\" in data and \"global\" in data[\"results_fpr_fnr\"] else None,\n                \"time_consumption\":None,\n                \"memory_consumption\":None,\n            }\n            print(global_metrics)\n            df_global = pd.DataFrame([global_metrics])\n\n            # --- Save CSV ---\n            csv_output = os.path.join(output_folder, filename.replace(\".json\", \".csv\"))\n            with open(csv_output, \"w\") as f:\n                f.write(\"### Per-class Metrics\\n\")\n            df_classes.to_csv(csv_output, mode=\"a\", index=False)\n            with open(csv_output, \"a\") as f:\n                f.write(\"\\n### Global Metrics\\n\")\n            df_global.to_csv(csv_output, mode=\"a\", index=False)\n\n            print(f\"Saved {csv_output}\")\njson_folder = \"/kaggle/working/temp/\"\noutput_folder = \"/kaggle/working/temp/csv_results\"\njson_to_csv_per_file(json_folder, output_folder)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T14:12:07.284Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>E_GAT Sampling</h2>","metadata":{}},{"cell_type":"code","source":"# print(\"----------------------- TRAINING E_GAT Sampling--------------------------\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T14:12:07.284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n\n# dataset = datasets[dataset_name]\n\n# # Hyperparameters\n\n# activation = F.relu\n# graphs_folder = graph_folder\n# if dataset_name == \"cic_ids_2017\":\n#     graphs_folder = \"/kaggle/working/cic_ids_2017/flow__multi_class__n_feats__unsorted\"\n# logs_folder = os.path.join(\"logs\", dataset.name)\n# os.makedirs(logs_folder, exist_ok=True)\n# wandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\n# os.makedirs(wandb_runs_path, exist_ok=True)\n\n# labels_mapping = {0: \"Normal\", 1: \"Attack\"}\n# num_classes = 2\n# if multi_class:\n#     with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n#     # /kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\n#     # with open(\"/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\", \"rb\") as f:\n#         labels_names = pickle.load(f)\n#     labels_mapping = labels_names[0]\n# num_classes = len(labels_mapping)\n\n# dataset_kwargs = dict(\n#     use_node_features=use_centralities_nfeats,\n#     multi_class=True,\n#     using_masking=False,\n#     masked_class=2,\n#     num_workers=0,\n#     label_col=dataset.label_col,\n#     class_num_col=dataset.class_num_col,\n#     device='cuda' if torch.cuda.is_available() else \"cpu\"\n# )\n\n# data_module = GraphDataModule(\n#     graphs_folder, batch_size=1, **dataset_kwargs)\n# data_module.setup()\n\n# ndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\n# edim = next(iter(data_module.train_dataloader())).edata['h'].shape[-1]\n\n# my_models = {\n#     # \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation,\n#     #               dropout, residual, num_classes),\n#     # f\"e_graphsage_{aggregation}\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                          residual, num_classes, num_neighbors=number_neighbors, aggregation=aggregation),\n#     # f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                                      residual, num_classes, num_neighbors=None, aggregation=aggregation),\n#     \"e_gat_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                   residual, num_classes, num_neighbors=number_neighbors),\n#     # \"e_gat_no_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes, num_neighbors=None),\n# }\n\n# criterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n\n# for model_name, model in my_models.items():\n        \n#     config = {\n#         \"run_dtime\": run_dtime,\n#         \"type\": \"GNN\",\n#         \"model_name\": model_name,\n#         \"max_epochs\": max_epochs,\n#         \"learning_rate\": learning_rate,\n#         \"weight_decay\": weight_decay,\n#         \"ndim_out\": ndim_out,\n#         \"num_layers\": num_layers,\n#         \"number_neighbors\": number_neighbors,\n#         \"activation\": activation.__name__,\n#         \"dropout\": dropout,\n#         \"residual\": residual,\n#         \"multi_class\": multi_class,\n#         \"aggregation\": aggregation,\n#         \"early_stopping_patience\": early_stopping_patience,\n#         \"use_centralities_nfeats\": use_centralities_nfeats,}\n\n#     graph_model = GraphModel(model, criterion, learning_rate, config, model_name,\n#                              labels_mapping, weight_decay=weight_decay, using_wandb=using_wandb, norm=False, multi_class=True)\n\n    \n#     if using_wandb:\n#         wandb_logger = WandbLogger(\n#             project=f\"GNN-Analysis-{dataset.name}\", # Project Name Wandb\n#             name=f\"{model_name}---{g_type}\",\n#             config=config,\n#             save_dir=wandb_runs_path\n#         )\n#     else:\n#         wandb_logger = None\n        \n#     f1_checkpoint_callback = ModelCheckpoint(\n#         monitor=\"val_f1_score\",\n#         mode=\"max\",\n#         filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n#         save_top_k=save_top_k,\n#         verbose=False,\n#     )\n#     early_stopping_callback = EarlyStopping(\n#         monitor=\"val_loss\",\n#         mode=\"min\",\n#         patience=early_stopping_patience,\n#         verbose=False,\n#     )\n\n#     trainer = pl.Trainer(\n#         max_epochs=max_epochs,\n#         num_sanity_val_steps=0,\n#         log_every_n_steps=0,\n#         callbacks=[\n#             f1_checkpoint_callback,\n#             early_stopping_callback\n#         ],\n#         default_root_dir=logs_folder,\n#         logger=wandb_logger if using_wandb else None,\n#     )\n\n#     trainer.fit(graph_model, datamodule=data_module)\n    \n#     test_results = []\n    \n#     for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n#         graph_model.test_prefix = f\"best_f1_{i}\"\n#         results = trainer.test(\n#             graph_model, datamodule=data_module, ckpt_path=k)\n#         test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n\n#     logs = {\n#         \"median_f1_of_best_f1\": np.median(test_results),\n#         \"max_f1_of_best_f1\": np.max(test_results),\n#         \"avg_f1_of_best_f1\": np.mean(test_results)\n#     }\n\n#     if using_wandb:\n#         wandb.log(logs)\n#         wandb.finish()\n#     else:\n#         trainer.logger.log_metrics(logs, step=trainer.global_step)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T14:12:07.284Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>EGraphSAGE mean</h2>","metadata":{}},{"cell_type":"code","source":"# print(\"----------------------- TRAINING EGraphSAGE mean --------------------------\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T14:12:07.284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n\n# dataset = datasets[dataset_name]\n\n# # Hyperparameters\n\n# activation = F.relu\n# graphs_folder = graph_folder\n# if dataset_name == \"cic_ids_2017\":\n#     graphs_folder = \"/kaggle/working/cic_ids_2017/flow__multi_class__n_feats__unsorted\"\n# logs_folder = os.path.join(\"logs\", dataset.name)\n# os.makedirs(logs_folder, exist_ok=True)\n# wandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\n# os.makedirs(wandb_runs_path, exist_ok=True)\n\n# labels_mapping = {0: \"Normal\", 1: \"Attack\"}\n# num_classes = 2\n# if multi_class:\n#     with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n#     # /kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\n#     # with open(\"/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\", \"rb\") as f:\n#         labels_names = pickle.load(f)\n#     labels_mapping = labels_names[0]\n# num_classes = len(labels_mapping)\n\n# dataset_kwargs = dict(\n#     use_node_features=use_centralities_nfeats,\n#     multi_class=True,\n#     using_masking=False,\n#     masked_class=2,\n#     num_workers=0,\n#     label_col=dataset.label_col,\n#     class_num_col=dataset.class_num_col,\n#     device='cuda' if torch.cuda.is_available() else \"cpu\"\n# )\n\n# data_module = GraphDataModule(\n#     graphs_folder, batch_size=1, **dataset_kwargs)\n# data_module.setup()\n\n# ndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\n# edim = next(iter(data_module.train_dataloader())).edata['h'].shape[-1]\n\n# my_models = {\n#     # \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation,\n#     #               dropout, residual, num_classes),\n#     # f\"e_graphsage_{aggregation}\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                          residual, num_classes, num_neighbors=number_neighbors, aggregation=aggregation),\n#     f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                                                          residual, num_classes, num_neighbors=None, aggregation=aggregation),\n#     # \"e_gat_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #               residual, num_classes, num_neighbors=number_neighbors),\n#     # \"e_gat_no_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes, num_neighbors=None),\n# }\n\n# criterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n\n# for model_name, model in my_models.items():\n        \n#     config = {\n#         \"run_dtime\": run_dtime,\n#         \"type\": \"GNN\",\n#         \"model_name\": model_name,\n#         \"max_epochs\": max_epochs,\n#         \"learning_rate\": learning_rate,\n#         \"weight_decay\": weight_decay,\n#         \"ndim_out\": ndim_out,\n#         \"num_layers\": num_layers,\n#         \"number_neighbors\": number_neighbors,\n#         \"activation\": activation.__name__,\n#         \"dropout\": dropout,\n#         \"residual\": residual,\n#         \"multi_class\": multi_class,\n#         \"aggregation\": aggregation,\n#         \"early_stopping_patience\": early_stopping_patience,\n#         \"use_centralities_nfeats\": use_centralities_nfeats,}\n\n#     graph_model = GraphModel(model, criterion, learning_rate, config, model_name,\n#                              labels_mapping, weight_decay=weight_decay, using_wandb=using_wandb, norm=False, multi_class=True)\n\n    \n#     if using_wandb:\n#         wandb_logger = WandbLogger(\n#             project=f\"GNN-Analysis-{dataset.name}\", # Project Name Wandb\n#             name=f\"{model_name}---{g_type}\",\n#             config=config,\n#             save_dir=wandb_runs_path\n#         )\n#     else:\n#         wandb_logger = None\n        \n#     f1_checkpoint_callback = ModelCheckpoint(\n#         monitor=\"val_f1_score\",\n#         mode=\"max\",\n#         filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n#         save_top_k=save_top_k,\n#         verbose=False,\n#     )\n#     early_stopping_callback = EarlyStopping(\n#         monitor=\"val_loss\",\n#         mode=\"min\",\n#         patience=early_stopping_patience,\n#         verbose=False,\n#     )\n\n#     trainer = pl.Trainer(\n#         max_epochs=max_epochs,\n#         num_sanity_val_steps=0,\n#         log_every_n_steps=0,\n#         callbacks=[\n#             f1_checkpoint_callback,\n#             early_stopping_callback\n#         ],\n#         default_root_dir=logs_folder,\n#         logger=wandb_logger if using_wandb else None,\n#     )\n\n#     trainer.fit(graph_model, datamodule=data_module)\n    \n#     test_results = []\n    \n#     for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n#         graph_model.test_prefix = f\"best_f1_{i}\"\n#         results = trainer.test(\n#             graph_model, datamodule=data_module, ckpt_path=k)\n#         test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n\n#     logs = {\n#         \"median_f1_of_best_f1\": np.median(test_results),\n#         \"max_f1_of_best_f1\": np.max(test_results),\n#         \"avg_f1_of_best_f1\": np.mean(test_results)\n#     }\n\n#     if using_wandb:\n#         wandb.log(logs)\n#         wandb.finish()\n#     else:\n#         trainer.logger.log_metrics(logs, step=trainer.global_step)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T14:12:07.284Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>EGraphSAGE sum</h2>","metadata":{}},{"cell_type":"code","source":"# print(\"----------------------- TRAINING EGraphSAGE sum --------------------------\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T14:12:07.284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n\n# dataset = datasets[dataset_name]\n\n# # Hyperparameters\n# aggregation=\"sum\"\n\n# activation = F.relu\n# graphs_folder = graph_folder\n# if dataset_name == \"cic_ids_2017\":\n#     graphs_folder = \"/kaggle/working/cic_ids_2017/flow__multi_class__n_feats__unsorted\"\n# logs_folder = os.path.join(\"logs\", dataset.name)\n# os.makedirs(logs_folder, exist_ok=True)\n# wandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\n# os.makedirs(wandb_runs_path, exist_ok=True)\n\n# labels_mapping = {0: \"Normal\", 1: \"Attack\"}\n# num_classes = 2\n# if multi_class:\n#     with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n#     # /kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\n#     # with open(\"/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\", \"rb\") as f:\n#         labels_names = pickle.load(f)\n#     labels_mapping = labels_names[0]\n# num_classes = len(labels_mapping)\n\n# dataset_kwargs = dict(\n#     use_node_features=use_centralities_nfeats,\n#     multi_class=True,\n#     using_masking=False,\n#     masked_class=2,\n#     num_workers=0,\n#     label_col=dataset.label_col,\n#     class_num_col=dataset.class_num_col,\n#     device='cuda' if torch.cuda.is_available() else \"cpu\"\n# )\n\n# data_module = GraphDataModule(\n#     graphs_folder, batch_size=1, **dataset_kwargs)\n# data_module.setup()\n\n# ndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\n# edim = next(iter(data_module.train_dataloader())).edata['h'].shape[-1]\n\n# my_models = {\n#     # \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation,\n#     #               dropout, residual, num_classes),\n#     # f\"e_graphsage_{aggregation}\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                          residual, num_classes, num_neighbors=number_neighbors, aggregation=aggregation),\n#     f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                                                          residual, num_classes, num_neighbors=None, aggregation=aggregation),\n#     # \"e_gat_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #               residual, num_classes, num_neighbors=number_neighbors),\n#     # \"e_gat_no_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes, num_neighbors=None),\n# }\n\n# criterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n\n# for model_name, model in my_models.items():\n        \n#     config = {\n#         \"run_dtime\": run_dtime,\n#         \"type\": \"GNN\",\n#         \"model_name\": model_name,\n#         \"max_epochs\": max_epochs,\n#         \"learning_rate\": learning_rate,\n#         \"weight_decay\": weight_decay,\n#         \"ndim_out\": ndim_out,\n#         \"num_layers\": num_layers,\n#         \"number_neighbors\": number_neighbors,\n#         \"activation\": activation.__name__,\n#         \"dropout\": dropout,\n#         \"residual\": residual,\n#         \"multi_class\": multi_class,\n#         \"aggregation\": aggregation,\n#         \"early_stopping_patience\": early_stopping_patience,\n#         \"use_centralities_nfeats\": use_centralities_nfeats,}\n\n#     graph_model = GraphModel(model, criterion, learning_rate, config, model_name,\n#                              labels_mapping, weight_decay=weight_decay, using_wandb=using_wandb, norm=False, multi_class=True)\n\n    \n#     if using_wandb:\n#         wandb_logger = WandbLogger(\n#             project=f\"GNN-Analysis-{dataset.name}\", # Project Name Wandb\n#             name=f\"{model_name}---{g_type}\",\n#             config=config,\n#             save_dir=wandb_runs_path\n#         )\n#     else:\n#         wandb_logger = None\n        \n#     f1_checkpoint_callback = ModelCheckpoint(\n#         monitor=\"val_f1_score\",\n#         mode=\"max\",\n#         filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n#         save_top_k=save_top_k,\n#         verbose=False,\n#     )\n#     early_stopping_callback = EarlyStopping(\n#         monitor=\"val_loss\",\n#         mode=\"min\",\n#         patience=early_stopping_patience,\n#         verbose=False,\n#     )\n\n#     trainer = pl.Trainer(\n#         max_epochs=max_epochs,\n#         num_sanity_val_steps=0,\n#         log_every_n_steps=0,\n#         callbacks=[\n#             f1_checkpoint_callback,\n#             early_stopping_callback\n#         ],\n#         default_root_dir=logs_folder,\n#         logger=wandb_logger if using_wandb else None,\n#     )\n\n#     trainer.fit(graph_model, datamodule=data_module)\n    \n#     test_results = []\n    \n#     for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n#         graph_model.test_prefix = f\"best_f1_{i}\"\n#         results = trainer.test(\n#             graph_model, datamodule=data_module, ckpt_path=k)\n#         test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n\n#     logs = {\n#         \"median_f1_of_best_f1\": np.median(test_results),\n#         \"max_f1_of_best_f1\": np.max(test_results),\n#         \"avg_f1_of_best_f1\": np.mean(test_results)\n#     }\n\n#     if using_wandb:\n#         wandb.log(logs)\n#         wandb.finish()\n#     else:\n#         trainer.logger.log_metrics(logs, step=trainer.global_step)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T14:12:07.284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" # wandb.finish()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T14:12:07.284Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>EGraphSAGE Sampling mean</h2>","metadata":{}},{"cell_type":"code","source":"# print(\"----------------------- TRAINING EGraphSAGE Sampling mean --------------------------\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T14:12:07.284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n\n# dataset = datasets[dataset_name]\n\n# # Hyperparameters\n# aggregation=\"mean\"\n\n# activation = F.relu\n# graphs_folder = graph_folder\n# if dataset_name == \"cic_ids_2017\":\n#     graphs_folder = \"/kaggle/working/cic_ids_2017/flow__multi_class__n_feats__unsorted\"\n# logs_folder = os.path.join(\"logs\", dataset.name)\n# os.makedirs(logs_folder, exist_ok=True)\n# wandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\n# os.makedirs(wandb_runs_path, exist_ok=True)\n\n# labels_mapping = {0: \"Normal\", 1: \"Attack\"}\n# num_classes = 2\n# if multi_class:\n#     with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n#     # /kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\n#     # with open(\"/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\", \"rb\") as f:\n#         labels_names = pickle.load(f)\n#     labels_mapping = labels_names[0]\n# num_classes = len(labels_mapping)\n\n# dataset_kwargs = dict(\n#     use_node_features=use_centralities_nfeats,\n#     multi_class=True,\n#     using_masking=False,\n#     masked_class=2,\n#     num_workers=0,\n#     label_col=dataset.label_col,\n#     class_num_col=dataset.class_num_col,\n#     device='cuda' if torch.cuda.is_available() else \"cpu\"\n# )\n\n# data_module = GraphDataModule(\n#     graphs_folder, batch_size=1, **dataset_kwargs)\n# data_module.setup()\n\n# ndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\n# edim = next(iter(data_module.train_dataloader())).edata['h'].shape[-1]\n\n# my_models = {\n#     # \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation,\n#     #               dropout, residual, num_classes),\n#     f\"e_graphsage_{aggregation}\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                                              residual, num_classes, num_neighbors=number_neighbors, aggregation=aggregation),\n#     # f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                                      residual, num_classes, num_neighbors=None, aggregation=aggregation),\n#     # \"e_gat_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                   # residual, num_classes, num_neighbors=number_neighbors),\n#     # \"e_gat_no_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes, num_neighbors=None),\n# }\n\n# criterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n\n# for model_name, model in my_models.items():\n        \n#     config = {\n#         \"run_dtime\": run_dtime,\n#         \"type\": \"GNN\",\n#         \"model_name\": model_name,\n#         \"max_epochs\": max_epochs,\n#         \"learning_rate\": learning_rate,\n#         \"weight_decay\": weight_decay,\n#         \"ndim_out\": ndim_out,\n#         \"num_layers\": num_layers,\n#         \"number_neighbors\": number_neighbors,\n#         \"activation\": activation.__name__,\n#         \"dropout\": dropout,\n#         \"residual\": residual,\n#         \"multi_class\": multi_class,\n#         \"aggregation\": aggregation,\n#         \"early_stopping_patience\": early_stopping_patience,\n#         \"use_centralities_nfeats\": use_centralities_nfeats,}\n\n#     graph_model = GraphModel(model, criterion, learning_rate, config, model_name,\n#                              labels_mapping, weight_decay=weight_decay, using_wandb=using_wandb, norm=False, multi_class=True)\n\n    \n#     if using_wandb:\n#         wandb_logger = WandbLogger(\n#             project=f\"GNN-Analysis-{dataset.name}\", # Project Name Wandb\n#             name=f\"{model_name}---{g_type}\",\n#             config=config,\n#             save_dir=wandb_runs_path\n#         )\n#     else:\n#         wandb_logger = None\n        \n#     f1_checkpoint_callback = ModelCheckpoint(\n#         monitor=\"val_f1_score\",\n#         mode=\"max\",\n#         filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n#         save_top_k=save_top_k,\n#         verbose=False,\n#     )\n#     early_stopping_callback = EarlyStopping(\n#         monitor=\"val_loss\",\n#         mode=\"min\",\n#         patience=early_stopping_patience,\n#         verbose=False,\n#     )\n\n#     trainer = pl.Trainer(\n#         max_epochs=max_epochs,\n#         num_sanity_val_steps=0,\n#         log_every_n_steps=0,\n#         callbacks=[\n#             f1_checkpoint_callback,\n#             early_stopping_callback\n#         ],\n#         default_root_dir=logs_folder,\n#         logger=wandb_logger if using_wandb else None,\n#     )\n\n#     trainer.fit(graph_model, datamodule=data_module)\n    \n#     test_results = []\n    \n#     for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n#         graph_model.test_prefix = f\"best_f1_{i}\"\n#         results = trainer.test(\n#             graph_model, datamodule=data_module, ckpt_path=k)\n#         test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n\n#     logs = {\n#         \"median_f1_of_best_f1\": np.median(test_results),\n#         \"max_f1_of_best_f1\": np.max(test_results),\n#         \"avg_f1_of_best_f1\": np.mean(test_results)\n#     }\n\n#     if using_wandb:\n#         wandb.log(logs)\n#         wandb.finish()\n#     else:\n#         trainer.logger.log_metrics(logs, step=trainer.global_step)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T14:12:07.284Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>EGraphSAGE Sampling sum</h2>","metadata":{}},{"cell_type":"code","source":"# print(\"----------------------- TRAINING EGraphSAGE Sampling sum --------------------------\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T14:12:07.284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n\n# dataset = datasets[dataset_name]\n\n# # Hyperparameters\n# aggregation=\"sum\"\n\n# activation = F.relu\n# graphs_folder = graph_folder\n# if dataset_name == \"cic_ids_2017\":\n#     graphs_folder = \"/kaggle/working/cic_ids_2017/flow__multi_class__n_feats__unsorted\"\n# logs_folder = os.path.join(\"logs\", dataset.name)\n# os.makedirs(logs_folder, exist_ok=True)\n# wandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\n# os.makedirs(wandb_runs_path, exist_ok=True)\n\n# labels_mapping = {0: \"Normal\", 1: \"Attack\"}\n# num_classes = 2\n# if multi_class:\n#     with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n#     # /kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\n#     # with open(\"/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\", \"rb\") as f:\n#         labels_names = pickle.load(f)\n#     labels_mapping = labels_names[0]\n# num_classes = len(labels_mapping)\n\n# dataset_kwargs = dict(\n#     use_node_features=use_centralities_nfeats,\n#     multi_class=True,\n#     using_masking=False,\n#     masked_class=2,\n#     num_workers=0,\n#     label_col=dataset.label_col,\n#     class_num_col=dataset.class_num_col,\n#     device='cuda' if torch.cuda.is_available() else \"cpu\"\n# )\n\n# data_module = GraphDataModule(\n#     graphs_folder, batch_size=1, **dataset_kwargs)\n# data_module.setup()\n\n# ndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\n# edim = next(iter(data_module.train_dataloader())).edata['h'].shape[-1]\n\n# my_models = {\n#     # \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation,\n#     #               dropout, residual, num_classes),\n#     f\"e_graphsage_{aggregation}\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                                              residual, num_classes, num_neighbors=number_neighbors, aggregation=aggregation),\n#     # f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                                      residual, num_classes, num_neighbors=None, aggregation=aggregation),\n#     # \"e_gat_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                   # residual, num_classes, num_neighbors=number_neighbors),\n#     # \"e_gat_no_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes, num_neighbors=None),\n# }\n\n# criterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n\n# for model_name, model in my_models.items():\n        \n#     config = {\n#         \"run_dtime\": run_dtime,\n#         \"type\": \"GNN\",\n#         \"model_name\": model_name,\n#         \"max_epochs\": max_epochs,\n#         \"learning_rate\": learning_rate,\n#         \"weight_decay\": weight_decay,\n#         \"ndim_out\": ndim_out,\n#         \"num_layers\": num_layers,\n#         \"number_neighbors\": number_neighbors,\n#         \"activation\": activation.__name__,\n#         \"dropout\": dropout,\n#         \"residual\": residual,\n#         \"multi_class\": multi_class,\n#         \"aggregation\": aggregation,\n#         \"early_stopping_patience\": early_stopping_patience,\n#         \"use_centralities_nfeats\": use_centralities_nfeats,}\n\n#     graph_model = GraphModel(model, criterion, learning_rate, config, model_name,\n#                              labels_mapping, weight_decay=weight_decay, using_wandb=using_wandb, norm=False, multi_class=True)\n\n    \n#     if using_wandb:\n#         wandb_logger = WandbLogger(\n#             project=f\"GNN-Analysis-{dataset.name}\", # Project Name Wandb\n#             name=f\"{model_name}---{g_type}\",\n#             config=config,\n#             save_dir=wandb_runs_path\n#         )\n#     else:\n#         wandb_logger = None\n        \n#     f1_checkpoint_callback = ModelCheckpoint(\n#         monitor=\"val_f1_score\",\n#         mode=\"max\",\n#         filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n#         save_top_k=save_top_k,\n#         verbose=False,\n#     )\n#     early_stopping_callback = EarlyStopping(\n#         monitor=\"val_loss\",\n#         mode=\"min\",\n#         patience=early_stopping_patience,\n#         verbose=False,\n#     )\n\n#     trainer = pl.Trainer(\n#         max_epochs=max_epochs,\n#         num_sanity_val_steps=0,\n#         log_every_n_steps=0,\n#         callbacks=[\n#             f1_checkpoint_callback,\n#             early_stopping_callback\n#         ],\n#         default_root_dir=logs_folder,\n#         logger=wandb_logger if using_wandb else None,\n#     )\n\n#     trainer.fit(graph_model, datamodule=data_module)\n    \n#     test_results = []\n    \n#     for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n#         graph_model.test_prefix = f\"best_f1_{i}\"\n#         results = trainer.test(\n#             graph_model, datamodule=data_module, ckpt_path=k)\n#         test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n\n#     logs = {\n#         \"median_f1_of_best_f1\": np.median(test_results),\n#         \"max_f1_of_best_f1\": np.max(test_results),\n#         \"avg_f1_of_best_f1\": np.mean(test_results)\n#     }\n\n#     if using_wandb:\n#         wandb.log(logs)\n#         wandb.finish()\n#     else:\n#         trainer.logger.log_metrics(logs, step=trainer.global_step)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T14:12:07.285Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>E_GCN</h2>","metadata":{}},{"cell_type":"code","source":"# print(\"----------------------- TRAINING E_GCN --------------------------\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T14:12:07.285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n\n# dataset = datasets[dataset_name]\n\n# # Hyperparameters\n# aggregation=\"mean\"\n\n# activation = F.relu\n# graphs_folder = graph_folder\n# if dataset_name == \"cic_ids_2017\":\n#     graphs_folder = \"/kaggle/working/cic_ids_2017/flow__multi_class__n_feats__unsorted\"\n# logs_folder = os.path.join(\"logs\", dataset.name)\n# os.makedirs(logs_folder, exist_ok=True)\n# wandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\n# os.makedirs(wandb_runs_path, exist_ok=True)\n\n# labels_mapping = {0: \"Normal\", 1: \"Attack\"}\n# num_classes = 2\n# if multi_class:\n#     with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n#     # /kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\n#     # with open(\"/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\", \"rb\") as f:\n#         labels_names = pickle.load(f)\n#     labels_mapping = labels_names[0]\n# num_classes = len(labels_mapping)\n\n# dataset_kwargs = dict(\n#     use_node_features=use_centralities_nfeats,\n#     multi_class=True,\n#     using_masking=False,\n#     masked_class=2,\n#     num_workers=0,\n#     label_col=dataset.label_col,\n#     class_num_col=dataset.class_num_col,\n#     device='cuda' if torch.cuda.is_available() else \"cpu\"\n# )\n\n# data_module = GraphDataModule(\n#     graphs_folder, batch_size=1, **dataset_kwargs)\n# data_module.setup()\n\n# ndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\n# edim = next(iter(data_module.train_dataloader())).edata['h'].shape[-1]\n\n# my_models = {\n#     \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation,\n#                   dropout, residual, num_classes),\n#     # f\"e_graphsage_{aggregation}\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                          residual, num_classes, num_neighbors=number_neighbors, aggregation=aggregation),\n#     # f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                                      residual, num_classes, num_neighbors=None, aggregation=aggregation),\n#     # \"e_gat_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                   # residual, num_classes, num_neighbors=number_neighbors),\n#     # \"e_gat_no_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes, num_neighbors=None),\n# }\n\n# criterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n\n# for model_name, model in my_models.items():\n        \n#     config = {\n#         \"run_dtime\": run_dtime,\n#         \"type\": \"GNN\",\n#         \"model_name\": model_name,\n#         \"max_epochs\": max_epochs,\n#         \"learning_rate\": learning_rate,\n#         \"weight_decay\": weight_decay,\n#         \"ndim_out\": ndim_out,\n#         \"num_layers\": num_layers,\n#         \"number_neighbors\": number_neighbors,\n#         \"activation\": activation.__name__,\n#         \"dropout\": dropout,\n#         \"residual\": residual,\n#         \"multi_class\": multi_class,\n#         \"aggregation\": aggregation,\n#         \"early_stopping_patience\": early_stopping_patience,\n#         \"use_centralities_nfeats\": use_centralities_nfeats,}\n\n#     graph_model = GraphModel(model, criterion, learning_rate, config, model_name,\n#                              labels_mapping, weight_decay=weight_decay, using_wandb=using_wandb, norm=False, multi_class=True)\n\n    \n#     if using_wandb:\n#         wandb_logger = WandbLogger(\n#             project=f\"GNN-Analysis-{dataset.name}\", # Project Name Wandb\n#             name=f\"{model_name}---{g_type}\",\n#             config=config,\n#             save_dir=wandb_runs_path\n#         )\n#     else:\n#         wandb_logger = None\n        \n#     f1_checkpoint_callback = ModelCheckpoint(\n#         monitor=\"val_f1_score\",\n#         mode=\"max\",\n#         filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n#         save_top_k=save_top_k,\n#         verbose=False,\n#     )\n#     early_stopping_callback = EarlyStopping(\n#         monitor=\"val_loss\",\n#         mode=\"min\",\n#         patience=early_stopping_patience,\n#         verbose=False,\n#     )\n\n#     trainer = pl.Trainer(\n#         max_epochs=max_epochs,\n#         num_sanity_val_steps=0,\n#         log_every_n_steps=0,\n#         callbacks=[\n#             f1_checkpoint_callback,\n#             early_stopping_callback\n#         ],\n#         default_root_dir=logs_folder,\n#         logger=wandb_logger if using_wandb else None,\n#     )\n\n#     trainer.fit(graph_model, datamodule=data_module)\n    \n#     test_results = []\n    \n#     for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n#         graph_model.test_prefix = f\"best_f1_{i}\"\n#         results = trainer.test(\n#             graph_model, datamodule=data_module, ckpt_path=k)\n#         test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n\n#     logs = {\n#         \"median_f1_of_best_f1\": np.median(test_results),\n#         \"max_f1_of_best_f1\": np.max(test_results),\n#         \"avg_f1_of_best_f1\": np.mean(test_results)\n#     }\n\n#     if using_wandb:\n#         wandb.log(logs)\n#         wandb.finish()\n#     else:\n#         trainer.logger.log_metrics(logs, step=trainer.global_step)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T14:12:07.285Z"}},"outputs":[],"execution_count":null}]}