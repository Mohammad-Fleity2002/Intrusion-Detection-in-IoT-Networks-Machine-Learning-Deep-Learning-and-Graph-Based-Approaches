{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12479627,"sourceType":"datasetVersion","datasetId":7874205},{"sourceId":12479887,"sourceType":"datasetVersion","datasetId":7874406},{"sourceId":12516523,"sourceType":"datasetVersion","datasetId":7900575},{"sourceId":258352,"sourceType":"modelInstanceVersion","modelInstanceId":218283,"modelId":240009},{"sourceId":470644,"sourceType":"modelInstanceVersion","modelInstanceId":379672,"modelId":399557},{"sourceId":473434,"sourceType":"modelInstanceVersion","modelInstanceId":381305,"modelId":400996}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h2>CIC_TON_IOT_GNN_TRAINING flow__multi_class__n_feats__unsorted</h2>","metadata":{}},{"cell_type":"markdown","source":"<h1>Creating Graphs</h1>","metadata":{}},{"cell_type":"code","source":"!pip install powerlaw","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:17.305766Z","iopub.execute_input":"2025-09-17T10:55:17.306007Z","iopub.status.idle":"2025-09-17T10:55:20.736481Z","shell.execute_reply.started":"2025-09-17T10:55:17.305984Z","shell.execute_reply":"2025-09-17T10:55:20.735563Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import json\n\n# with open(\"/kaggle/input/cic-ton-iot-gnn/df_properties.json\", \"r\") as f:\n#     df_properties = json.load(f)\n\n# print(\"üìò Dataset Properties:\")\n# for k, v in df_properties.items():\n#     print(f\"{k}: {v}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:20.737524Z","iopub.execute_input":"2025-09-17T10:55:20.737775Z","iopub.status.idle":"2025-09-17T10:55:20.752190Z","shell.execute_reply.started":"2025-09-17T10:55:20.737751Z","shell.execute_reply":"2025-09-17T10:55:20.751347Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pickle\n\n# with open(\"/kaggle/input/cic-ton-iot-gnn/labels_names.pkl\", \"rb\") as f:\n#     labels_names = pickle.load(f)\n# print(labels_names)\n# print(\"\\nüè∑Ô∏è Label Class Mapping:\")\n# # for k, v in labels_names.items():\n# #     print(f\"{k}: {v}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:20.753035Z","iopub.execute_input":"2025-09-17T10:55:20.753545Z","iopub.status.idle":"2025-09-17T10:55:20.767603Z","shell.execute_reply.started":"2025-09-17T10:55:20.753527Z","shell.execute_reply":"2025-09-17T10:55:20.766914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pickle\nimport random\nimport socket\nimport struct\nimport json\nimport sys\nimport pandas as pd\nsys.path.append(\"/kaggle/input/gnn-nids/pytorch/default/1\")\n\nimport networkx as nx\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom src.dataset.dataset_info import datasets\nfrom src.graph.graph_construction import create_weightless_window_graph\nfrom src.graph.graph_measures import calculate_graph_measures\nfrom src.graph.centralities import add_centralities, add_centralities_as_node_features\n# from local_variables import local_datasets_path\nlocal_datasets_path = \"/kaggle/input/\"\noriginal_datasets_files_path = \"/kaggle/input/cic-ton-iot-gnn\"\n# /kaggle/input/cic-ids-2017-gnn-2\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-09-17T10:55:20.769367Z","iopub.execute_input":"2025-09-17T10:55:20.769581Z","iopub.status.idle":"2025-09-17T10:55:22.105664Z","shell.execute_reply.started":"2025-09-17T10:55:20.769565Z","shell.execute_reply":"2025-09-17T10:55:22.105026Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"multi_class = True\n\nuse_node_features = False    #------- False == No node feature used\n\nuse_port_in_address = False\n\ngenerated_ips = False\n\ngraph_type = \"flow\"\n# graph_type = \"window\"\n# graph_type = \"line\"\n\n# window_size= 400\nwindow_size= 500\n# print(\"1. flow sorted\")\n# print(\"2. flow using node features unsorted\") \n# print(\"3. flow using node features sorted\")\nsort_timestamp = True      # --------- sorting timstamp\n\n# k_fold = None\n# k_fold = 5\n\nvalidation_size = 0.1\ntest_size = 0.1\n\ncn_measures = [\"betweenness\", \"degree\", \"pagerank\", \"closeness\", \"k_truss\"]\n# cn_measures = [\"betweenness\", \"degree\", \"closeness\"]\n\nnetwork_features = ['src_betweenness', 'dst_betweenness', 'src_degree', 'dst_degree', 'src_pagerank', 'dst_pagerank', 'src_closeness', 'dst_closeness', 'src_k_truss', 'dst_k_truss']\n# network_features = ['src_betweenness', 'dst_betweenness', 'src_degree', 'dst_degree', 'src_pagerank', 'dst_pagerank']\nGTP=graph_type+\" \"\nif use_node_features:\n    GTP+=\"using NODE FEATURES \"\nif sort_timestamp:\n    GTP+=\"Sorted \"\nelse:\n    GTP+=\"Unsorted.\"\nprint(GTP)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:22.106456Z","iopub.execute_input":"2025-09-17T10:55:22.106927Z","iopub.status.idle":"2025-09-17T10:55:22.112469Z","shell.execute_reply.started":"2025-09-17T10:55:22.106904Z","shell.execute_reply":"2025-09-17T10:55:22.111563Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# name = \"cic_ton_iot_5_percent\"\nname = \"cic_ton_iot\"\n# name = \"cic_ids_2017_5_percent\"\n# name = \"cic_ids_2017\"\n# name = \"cic_bot_iot\"\n# name = \"cic_ton_iot_modified\"\n# name = \"nf_ton_iotv2_modified\"\n# name = \"ccd_inid_modified\"\n# name = \"nf_uq_nids_modified\"\n# name = \"edge_iiot\"\n# name = \"nf_cse_cic_ids2018\"\n# name = \"nf_bot_iotv2\"\n# name = \"nf_uq_nids\"\n# name = \"x_iiot\"\n\ndataset = datasets[name]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:22.113386Z","iopub.execute_input":"2025-09-17T10:55:22.113691Z","iopub.status.idle":"2025-09-17T10:55:22.131378Z","shell.execute_reply.started":"2025-09-17T10:55:22.113665Z","shell.execute_reply":"2025-09-17T10:55:22.130656Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"g_type = \"\"\nif graph_type == \"flow\":\n    g_type = \"flow\"\nelif graph_type == \"line\":\n    g_type = f\"line_graph_{window_size}\"\nelif graph_type == \"window\":\n    g_type = f\"window_graph_{window_size}\"\n    \nif multi_class:\n    g_type += \"__multi_class\"\n    \nif use_node_features:\n    g_type += \"__n_feats\"\n    \n# if k_fold:\n#     g_type += f\"__{k_fold}_fold\"\n    \nif use_port_in_address:\n    g_type += \"__ports\"\n    \nif generated_ips:\n    g_type += \"__generated_ips\"\n    \nif sort_timestamp:\n    g_type += \"__sorted\"\nelse:\n    g_type += \"__unsorted\"\n# ****************** ALTERED    \n\n\n\ndataset_path = os.path.join(local_datasets_path,name)\nfolder_path = os.path.join(dataset_path, g_type)\nif name == \"cic_ids_2017\":\n    name_ds = \"cic-ids-2017-gnn-2\"\n    dataset_path = os.path.join(local_datasets_path,name_ds)\n    # folder_path = os.path.join(local_datasets_path,name, g_type)\n    folder_path= os.path.join(\"/kaggle/working/\",name,g_type)\nif name ==\"cic_ton_iot\":\n    name_ds = \"cic-ton-iot-gnn\"\n    dataset_path = os.path.join(local_datasets_path,name_ds)\n    folder_path= os.path.join(\"/kaggle/working/\",name,g_type)\n    \nprint(folder_path)\nprint(dataset_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:22.132467Z","iopub.execute_input":"2025-09-17T10:55:22.133459Z","iopub.status.idle":"2025-09-17T10:55:22.148017Z","shell.execute_reply.started":"2025-09-17T10:55:22.133429Z","shell.execute_reply":"2025-09-17T10:55:22.147456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# if name== \"cic_ids_2017\":\n#     df=pd.read_parquet(os.path.join(dataset_path, f\"{name}.parquet\"))\n# else:\ndf = pd.read_parquet(os.path.join(dataset_path, f\"{name}.parquet\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:22.148736Z","iopub.execute_input":"2025-09-17T10:55:22.149007Z","iopub.status.idle":"2025-09-17T10:55:30.114678Z","shell.execute_reply.started":"2025-09-17T10:55:22.148982Z","shell.execute_reply":"2025-09-17T10:55:30.114106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df.head()","metadata":{"execution":{"iopub.status.busy":"2025-09-17T10:55:30.115457Z","iopub.execute_input":"2025-09-17T10:55:30.115689Z","iopub.status.idle":"2025-09-17T10:55:30.144001Z","shell.execute_reply.started":"2025-09-17T10:55:30.115671Z","shell.execute_reply":"2025-09-17T10:55:30.143243Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cols_to_norm = list(set(list(df.columns))  - set(list([dataset.label_col, dataset.class_num_col])) - set(dataset.drop_columns)  - set(dataset.weak_columns))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:30.144818Z","iopub.execute_input":"2025-09-17T10:55:30.145474Z","iopub.status.idle":"2025-09-17T10:55:30.149135Z","shell.execute_reply.started":"2025-09-17T10:55:30.145450Z","shell.execute_reply":"2025-09-17T10:55:30.148466Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df[dataset.label_col].value_counts()","metadata":{"execution":{"iopub.status.busy":"2025-09-17T10:55:30.149854Z","iopub.execute_input":"2025-09-17T10:55:30.150158Z","iopub.status.idle":"2025-09-17T10:55:30.193251Z","shell.execute_reply.started":"2025-09-17T10:55:30.150139Z","shell.execute_reply":"2025-09-17T10:55:30.192650Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if generated_ips:\n    df[dataset.src_ip_col] = df[dataset.src_ip_col].apply(lambda x: socket.inet_ntoa(struct.pack('>I', random.randint(0xac100001, 0xac1f0001))))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:30.193891Z","iopub.execute_input":"2025-09-17T10:55:30.194146Z","iopub.status.idle":"2025-09-17T10:55:30.210674Z","shell.execute_reply.started":"2025-09-17T10:55:30.194089Z","shell.execute_reply":"2025-09-17T10:55:30.210132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if sort_timestamp:\n    # df[dataset.timestamp_col] = pd.to_datetime(df[dataset.timestamp_col].str.strip(), format=dataset.timestamp_format)\n    df.sort_values(dataset.timestamp_col, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:30.213280Z","iopub.execute_input":"2025-09-17T10:55:30.213472Z","iopub.status.idle":"2025-09-17T10:55:33.716932Z","shell.execute_reply.started":"2025-09-17T10:55:30.213457Z","shell.execute_reply":"2025-09-17T10:55:33.716150Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if use_port_in_address:\n    df[dataset.src_port_col] = df[dataset.src_port_col].astype(float).astype(int).astype(str) # to remove the decimal point\n    df[dataset.src_ip_col] = df[dataset.src_ip_col] + ':' + df[dataset.src_port_col]\n\n    df[dataset.dst_port_col] = df[dataset.dst_port_col].astype(float).astype(int).astype(str) # to remove the decimal point\n    df[dataset.dst_ip_col] = df[dataset.dst_ip_col] + ':' + df[dataset.dst_port_col]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:33.718057Z","iopub.execute_input":"2025-09-17T10:55:33.718365Z","iopub.status.idle":"2025-09-17T10:55:33.723056Z","shell.execute_reply.started":"2025-09-17T10:55:33.718339Z","shell.execute_reply":"2025-09-17T10:55:33.722541Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df.head(20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:33.723762Z","iopub.execute_input":"2025-09-17T10:55:33.724033Z","iopub.status.idle":"2025-09-17T10:55:33.770501Z","shell.execute_reply.started":"2025-09-17T10:55:33.724005Z","shell.execute_reply":"2025-09-17T10:55:33.769751Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(dataset.class_num_col)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:33.771378Z","iopub.execute_input":"2025-09-17T10:55:33.771705Z","iopub.status.idle":"2025-09-17T10:55:33.785876Z","shell.execute_reply.started":"2025-09-17T10:55:33.771644Z","shell.execute_reply":"2025-09-17T10:55:33.785135Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if multi_class:\n    y = df[dataset.class_num_col]\nelse:\n    y = df[dataset.label_col]\n\nif sort_timestamp:\n    X_tr, X_test, y_tr, y_test = train_test_split(\n        df, y, test_size=test_size)\n    \n    X_train, X_val, y_train, y_val = train_test_split(\n        X_tr, y_tr, test_size=validation_size)\nelse:\n    X_tr, X_test, y_tr, y_test = train_test_split(\n        df, y, test_size=test_size, random_state=13, stratify=y)\n    \n    X_train, X_val, y_train, y_val = train_test_split(\n        X_tr, y_tr, test_size=validation_size, random_state=13, stratify=y_tr)\n\ndel df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:33.786654Z","iopub.execute_input":"2025-09-17T10:55:33.786927Z","iopub.status.idle":"2025-09-17T10:55:51.641449Z","shell.execute_reply.started":"2025-09-17T10:55:33.786904Z","shell.execute_reply":"2025-09-17T10:55:51.640807Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if graph_type == \"line\" and use_node_features:\n    add_centralities(df = X_train, new_path=None, graph_path=None, dataset=dataset, cn_measures=cn_measures, network_features=network_features, create_using=nx.MultiDiGraph())\n    add_centralities(df = X_val, new_path=None, graph_path=None, dataset=dataset, cn_measures=cn_measures, network_features=network_features, create_using=nx.MultiDiGraph())\n    add_centralities(df = X_test, new_path=None, graph_path=None, dataset=dataset, cn_measures=cn_measures, network_features=network_features, create_using=nx.MultiDiGraph())\n    cols_to_norm = list(set(cols_to_norm) | set(network_features))\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:51.642192Z","iopub.execute_input":"2025-09-17T10:55:51.642383Z","iopub.status.idle":"2025-09-17T10:55:51.647415Z","shell.execute_reply.started":"2025-09-17T10:55:51.642368Z","shell.execute_reply":"2025-09-17T10:55:51.646746Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaler = StandardScaler()\n\nX_train[cols_to_norm] = scaler.fit_transform(X_train[cols_to_norm])\nX_train['h'] = X_train[ cols_to_norm ].values.tolist()\n\ncols_to_drop = list(set(list(X_train.columns)) - set(list([dataset.label_col, dataset.src_ip_col, dataset.dst_ip_col, dataset.class_num_col, 'h'])))\nX_train.drop(cols_to_drop, axis=1, inplace=True)\n\nX_val[cols_to_norm] = scaler.transform(X_val[cols_to_norm])\nX_val['h'] = X_val[ cols_to_norm ].values.tolist()\nX_val.drop(cols_to_drop, axis=1, inplace=True)\n\nX_test[cols_to_norm] = scaler.transform(X_test[cols_to_norm])\nX_test['h'] = X_test[ cols_to_norm ].values.tolist()\nX_test.drop(cols_to_drop, axis=1, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:55:51.648226Z","iopub.execute_input":"2025-09-17T10:55:51.648455Z","iopub.status.idle":"2025-09-17T10:56:27.912386Z","shell.execute_reply.started":"2025-09-17T10:55:51.648433Z","shell.execute_reply":"2025-09-17T10:56:27.911507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if graph_type == \"window\" or graph_type == \"line\":\n\n    create_weightless_window_graph(\n        df=X_train,\n        dataset=dataset,\n        window_size=window_size,\n        line_graph=graph_type == \"line\",\n        folder_path=os.path.join(folder_path, \"training\"),\n        edge_attr= ['h', dataset.label_col, dataset.class_num_col],\n        file_type=\"pkl\")\n    \n    create_weightless_window_graph(\n        df=X_val,\n        dataset=dataset,\n        window_size=window_size,\n        line_graph=graph_type == \"line\",\n        folder_path=os.path.join(folder_path, \"validation\"),\n        edge_attr= ['h', dataset.label_col, dataset.class_num_col],\n        file_type=\"pkl\")\n    \n    create_weightless_window_graph(\n        df=X_test,\n        dataset=dataset,\n        window_size=window_size,\n        line_graph=graph_type == \"line\",\n        folder_path=os.path.join(folder_path, \"testing\"),\n        edge_attr= ['h', dataset.label_col, dataset.class_num_col],\n        file_type=\"pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:56:27.914683Z","iopub.execute_input":"2025-09-17T10:56:27.914967Z","iopub.status.idle":"2025-09-17T10:56:27.920480Z","shell.execute_reply.started":"2025-09-17T10:56:27.914941Z","shell.execute_reply":"2025-09-17T10:56:27.919865Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if graph_type == \"flow\":\n\tos.makedirs(folder_path, exist_ok=True)\n\tprint(f\"==>> X_train.shape: {X_train.shape}\")\n\tprint(f\"==>> X_val.shape: {X_val.shape}\")\n\tprint(f\"==>> X_test.shape: {X_test.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-09-17T10:56:27.921183Z","iopub.execute_input":"2025-09-17T10:56:27.921661Z","iopub.status.idle":"2025-09-17T10:56:27.963719Z","shell.execute_reply.started":"2025-09-17T10:56:27.921633Z","shell.execute_reply":"2025-09-17T10:56:27.963137Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if graph_type == \"flow\":\n    graph_name = \"training_graph\"\n\n    G = nx.from_pandas_edgelist(X_train, dataset.src_ip_col, dataset.dst_ip_col, ['h',dataset.label_col, dataset.class_num_col], create_using=nx.MultiDiGraph())\n    \n    if use_node_features:\n        add_centralities_as_node_features(df=None, G=G, graph_path=None, dataset=dataset, cn_measures=cn_measures)\n        \n        for node in G.nodes():\n            centralities = []\n            for centrality in cn_measures:\n                centralities.append(G.nodes[node].get(centrality, 0)) # Default to 0 if missing\n                \n                # Combine features into a single vector\n            n_feats = np.array(centralities, dtype=np.float32)\n            \n            # Add the new feature to the node\n            G.nodes[node][\"n_feats\"] = n_feats\n            \n    # get netowrk properties\n    graph_measures = calculate_graph_measures(G, f\"{folder_path}/{graph_name}_measures.json\", verbose=True)\n    print(f\"==>> graph_measures: {graph_measures}\")\n\n    # graph_measures = calculate_graph_measures(nx.DiGraph(G), \"datasets/\" + name + \"/training_graph_simple_measures.json\", verbose=True)\n    # print(f\"==>> graph_measures: {graph_measures}\")\n\n    with open(f\"{folder_path}/{graph_name}.pkl\", \"wb\") as f:\n        pickle.dump(G, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:56:27.964392Z","iopub.execute_input":"2025-09-17T10:56:27.964636Z","iopub.status.idle":"2025-09-17T11:11:39.152300Z","shell.execute_reply.started":"2025-09-17T10:56:27.964621Z","shell.execute_reply":"2025-09-17T11:11:39.151413Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if graph_type == \"flow\":\n    graph_name = \"validation_graph\"\n\n    G = nx.from_pandas_edgelist(X_val, dataset.src_ip_col, dataset.dst_ip_col, ['h',dataset.label_col, dataset.class_num_col], create_using=nx.MultiDiGraph())\n    \n    if use_node_features:\n        add_centralities_as_node_features(df=None, G=G, graph_path=None, dataset=dataset, cn_measures=cn_measures)\n        \n        for node in G.nodes():\n            centralities = []\n            for centrality in cn_measures:\n                centralities.append(G.nodes[node].get(centrality, 0)) # Default to 0 if missing\n                \n                # Combine features into a single vector\n            n_feats = np.array(centralities, dtype=np.float32)\n            \n            # Add the new feature to the node\n            G.nodes[node][\"n_feats\"] = n_feats\n            \n    # get netowrk properties\n    graph_measures = calculate_graph_measures(G, f\"{folder_path}/{graph_name}_measures.json\", verbose=True)\n    print(f\"==>> graph_measures: {graph_measures}\")\n\n    # graph_measures = calculate_graph_measures(nx.DiGraph(G), \"datasets/\" + name + \"/training_graph_simple_measures.json\", verbose=True)\n    # print(f\"==>> graph_measures: {graph_measures}\")\n\n    with open(f\"{folder_path}/{graph_name}.pkl\", \"wb\") as f:\n        pickle.dump(G, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:11:39.153205Z","iopub.execute_input":"2025-09-17T11:11:39.153482Z","iopub.status.idle":"2025-09-17T11:12:10.335973Z","shell.execute_reply.started":"2025-09-17T11:11:39.153454Z","shell.execute_reply":"2025-09-17T11:12:10.335161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if graph_type == \"flow\":\n    graph_name = \"testing_graph\"\n    \n    G = nx.from_pandas_edgelist(X_test, dataset.src_ip_col, dataset.dst_ip_col, ['h', dataset.label_col, dataset.class_num_col], create_using=nx.MultiDiGraph())\n    \n    if use_node_features:\n        add_centralities_as_node_features(df=None, G=G, graph_path=None, dataset=dataset, cn_measures=cn_measures)\n        \n        for node in G.nodes():\n            centralities = []\n            for centrality in cn_measures:\n                centralities.append(G.nodes[node].get(centrality, 0)) # Default to 0 if missing\n                \n                # Combine features into a single vector\n            n_feats = np.array(centralities, dtype=np.float32)\n            \n            # Add the new feature to the node\n            G.nodes[node][\"n_feats\"] = n_feats\n            \n    graph_measures = calculate_graph_measures(G, f\"{folder_path}/{graph_name}_measures.json\", verbose=True)\n    print(f\"==>> graph_measures: {graph_measures}\")\n    \n    # graph_measures = calculate_graph_measures(nx.DiGraph(G_test), \"datasets/\" + name + \"/testing_graph_simple_measures.json\", verbose=True)\n    # print(f\"==>> graph_measures: {graph_measures}\")\n    \n    with open(f\"{folder_path}/{graph_name}.pkl\", \"wb\") as f:\n        pickle.dump(G, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:12:10.336878Z","iopub.execute_input":"2025-09-17T11:12:10.337184Z","iopub.status.idle":"2025-09-17T11:12:51.904721Z","shell.execute_reply.started":"2025-09-17T11:12:10.337158Z","shell.execute_reply":"2025-09-17T11:12:51.904117Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\nprint(\"-------------------------- SAVING VARIABLES -------------------------- \")\ndata = {\n    \"dataset_path\": dataset_path,\n    \"folder_path\": folder_path,\n    \"name\": name,\n    \"g_type\": g_type\n}\n\n# Save to JSON\nwith open(\"config.json\", \"w\") as f:\n    json.dump(data, f, indent=4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:12:51.905497Z","iopub.execute_input":"2025-09-17T11:12:51.905726Z","iopub.status.idle":"2025-09-17T11:12:51.911293Z","shell.execute_reply.started":"2025-09-17T11:12:51.905709Z","shell.execute_reply":"2025-09-17T11:12:51.910409Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(G)\nprint(y)\nprint(X_train, X_val, y_train, y_val)\ndel G\ndel y \ndel X_train, X_val, y_train, y_val","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:12:51.912210Z","iopub.execute_input":"2025-09-17T11:12:51.912501Z","iopub.status.idle":"2025-09-17T11:12:55.270622Z","shell.execute_reply.started":"2025-09-17T11:12:51.912473Z","shell.execute_reply":"2025-09-17T11:12:55.269981Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(scaler)\ndel scaler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:12:55.271278Z","iopub.execute_input":"2025-09-17T11:12:55.271478Z","iopub.status.idle":"2025-09-17T11:12:55.293989Z","shell.execute_reply.started":"2025-09-17T11:12:55.271462Z","shell.execute_reply":"2025-09-17T11:12:55.293465Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import psutil\n\nram_gb = psutil.virtual_memory().used / 1e9\nprint(f\"Current RAM usage: {ram_gb:.2f} GB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:12:55.294719Z","iopub.execute_input":"2025-09-17T11:12:55.294943Z","iopub.status.idle":"2025-09-17T11:12:55.299486Z","shell.execute_reply.started":"2025-09-17T11:12:55.294921Z","shell.execute_reply":"2025-09-17T11:12:55.298857Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:12:55.300213Z","iopub.execute_input":"2025-09-17T11:12:55.300458Z","iopub.status.idle":"2025-09-17T11:12:56.058251Z","shell.execute_reply.started":"2025-09-17T11:12:55.300441Z","shell.execute_reply":"2025-09-17T11:12:56.057495Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"-------------------------- CLEARING THE MEMORY -------------------------- \")\nprint(\"Clear all variables (like restarting kernel)\")\n%reset -f\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:12:56.059196Z","iopub.execute_input":"2025-09-17T11:12:56.059529Z","iopub.status.idle":"2025-09-17T11:12:57.421819Z","shell.execute_reply.started":"2025-09-17T11:12:56.059503Z","shell.execute_reply":"2025-09-17T11:12:57.421268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import psutil\nram_gb = psutil.virtual_memory().used / 1e9\nprint(f\"Current RAM usage: {ram_gb:.2f} GB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:12:57.422536Z","iopub.execute_input":"2025-09-17T11:12:57.422770Z","iopub.status.idle":"2025-09-17T11:12:57.426884Z","shell.execute_reply.started":"2025-09-17T11:12:57.422746Z","shell.execute_reply":"2025-09-17T11:12:57.426134Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h1>TRAINNING</h1>","metadata":{}},{"cell_type":"code","source":"print(\"-------------------------- RETRIVING VARIABLES --------------------------\")\nimport json\nwith open(\"config.json\", \"r\") as f:\n    config = json.load(f)\n\n# Access the values\ndataset_path = config[\"dataset_path\"]\nfolder_path = config[\"folder_path\"]\nname = config[\"name\"]\ng_type = config[\"g_type\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:12:57.427621Z","iopub.execute_input":"2025-09-17T11:12:57.428080Z","iopub.status.idle":"2025-09-17T11:12:57.450165Z","shell.execute_reply.started":"2025-09-17T11:12:57.428057Z","shell.execute_reply":"2025-09-17T11:12:57.449555Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(dataset_path)\nprint(folder_path)\nprint(name)\nprint(g_type)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:12:57.450929Z","iopub.execute_input":"2025-09-17T11:12:57.451200Z","iopub.status.idle":"2025-09-17T11:12:57.470248Z","shell.execute_reply.started":"2025-09-17T11:12:57.451172Z","shell.execute_reply":"2025-09-17T11:12:57.469153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_name = name\ndataset_folder = dataset_path\ngraph_folder=folder_path\nmodel_path = \"/kaggle/input/gnn-nids-wandb/pytorch/default/6/\"\n\nusing_wandb = False\n# save_top_k = 5\nsave_top_k = 1\n# save_top_k = 4 save best 4 test model from the traininng based on f1_score \n# early_stopping_patience = max_epochs = 20\nearly_stopping_patience = max_epochs = 500\n# early_stopping_patience = 200 all - epochs = 500 - lr = 0.005\nlearning_rate = 0.005\nweight_decay = 0.01\nndim_out = [32, 32]\nnum_layers = 2\nnumber_neighbors = [25, 10]\ndropout = 0.5\nresidual = True\nmulti_class = True\naggregation = \"mean\"\n# aggregation = \"sum\"\n\nuse_centralities_nfeats = False\n\n# g_type = \"flow\"\n\n# if multi_class:\n#     g_type += \"__multi_class\"\n\n# if use_centralities_nfeats:\n#     g_type += \"__n_feats\"\n\n# g_type += \"__unsorted\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:12:57.471240Z","iopub.execute_input":"2025-09-17T11:12:57.471480Z","iopub.status.idle":"2025-09-17T11:12:57.486523Z","shell.execute_reply.started":"2025-09-17T11:12:57.471464Z","shell.execute_reply":"2025-09-17T11:12:57.485949Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip uninstall -y torch torchvision torchaudio torchdata pytorch-lightning --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:12:57.487236Z","iopub.execute_input":"2025-09-17T11:12:57.487479Z","iopub.status.idle":"2025-09-17T11:13:05.740989Z","shell.execute_reply.started":"2025-09-17T11:12:57.487456Z","shell.execute_reply":"2025-09-17T11:13:05.740049Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install torch==2.1.0+cu118 torchvision==0.16.0+cu118 torchaudio==2.1.0+cu118 -f https://download.pytorch.org/whl/torch_stable.html --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:13:05.742332Z","iopub.execute_input":"2025-09-17T11:13:05.742664Z","iopub.status.idle":"2025-09-17T11:14:36.221946Z","shell.execute_reply.started":"2025-09-17T11:13:05.742628Z","shell.execute_reply":"2025-09-17T11:14:36.221058Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(\"torch version:\", torch.__version__)\nprint(\"CUDA available?\", torch.cuda.is_available())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:14:36.223112Z","iopub.execute_input":"2025-09-17T11:14:36.223374Z","iopub.status.idle":"2025-09-17T11:14:39.468721Z","shell.execute_reply.started":"2025-09-17T11:14:36.223348Z","shell.execute_reply":"2025-09-17T11:14:39.468053Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install dgl -f https://data.dgl.ai/wheels/torch-2.1/cu118/repo.html --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:14:39.469496Z","iopub.execute_input":"2025-09-17T11:14:39.470012Z","iopub.status.idle":"2025-09-17T11:14:42.752760Z","shell.execute_reply.started":"2025-09-17T11:14:39.469993Z","shell.execute_reply":"2025-09-17T11:14:42.751933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!apt update -qq 2>/dev/null >/dev/null;\n!apt install -y libcusparse11 -qq 2>/dev/null >/dev/null;","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:14:42.753776Z","iopub.execute_input":"2025-09-17T11:14:42.754070Z","iopub.status.idle":"2025-09-17T11:14:50.372306Z","shell.execute_reply.started":"2025-09-17T11:14:42.754044Z","shell.execute_reply":"2025-09-17T11:14:50.371267Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install --upgrade -q pytorch-lightning==2.5.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:14:50.379746Z","iopub.execute_input":"2025-09-17T11:14:50.379997Z","iopub.status.idle":"2025-09-17T11:14:54.356915Z","shell.execute_reply.started":"2025-09-17T11:14:50.379975Z","shell.execute_reply":"2025-09-17T11:14:54.356139Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade -q wandb==0.19.6","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:14:54.357884Z","iopub.execute_input":"2025-09-17T11:14:54.358125Z","iopub.status.idle":"2025-09-17T11:15:00.565221Z","shell.execute_reply.started":"2025-09-17T11:14:54.358084Z","shell.execute_reply":"2025-09-17T11:15:00.564337Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import wandb\n# wandb.login(key=\"c20e6d52d61e51ff11f4391b5870097badb9092f\")\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"mohammad_wandb_secret\")\n\nwandb.login(key=secret_value_0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:15:00.566354Z","iopub.execute_input":"2025-09-17T11:15:00.566633Z","iopub.status.idle":"2025-09-17T11:15:09.609077Z","shell.execute_reply.started":"2025-09-17T11:15:00.566605Z","shell.execute_reply":"2025-09-17T11:15:09.608364Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport pickle\nimport time\nimport timeit\nimport importlib.util\nimport sys\nimport random\nimport numpy as np\nimport csv\nos.environ[\"DGLBACKEND\"] = \"pytorch\"\nimport pandas as pd\n\nimport torch.nn as nn\nimport torch\nimport warnings\nimport wandb\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\nseed = 42  # or any constant value\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\nrun_dtime = time.strftime(\"%Y%m%d-%H%M%S\")\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:15:09.610043Z","iopub.execute_input":"2025-09-17T11:15:09.610581Z","iopub.status.idle":"2025-09-17T11:15:14.997158Z","shell.execute_reply.started":"2025-09-17T11:15:09.610561Z","shell.execute_reply":"2025-09-17T11:15:14.996428Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:15:14.997964Z","iopub.execute_input":"2025-09-17T11:15:14.998867Z","iopub.status.idle":"2025-09-17T11:15:15.032980Z","shell.execute_reply.started":"2025-09-17T11:15:14.998845Z","shell.execute_reply":"2025-09-17T11:15:15.032416Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import kagglehub\n\n# # Download latest version\n# path = kagglehub.model_download(\"mortadaphdtermos/gnn-nids-wandb/pyTorch/default\")\n\n# print(\"Path to model files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:15:15.033672Z","iopub.execute_input":"2025-09-17T11:15:15.033902Z","iopub.status.idle":"2025-09-17T11:15:15.037200Z","shell.execute_reply.started":"2025-09-17T11:15:15.033885Z","shell.execute_reply":"2025-09-17T11:15:15.036413Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_lib(module_name, path):\n    spec = importlib.util.spec_from_file_location(module_name, path)\n    dataset_info = importlib.util.module_from_spec(spec)\n    sys.modules[module_name] = dataset_info\n    spec.loader.exec_module(dataset_info)\n\nadd_lib(\"dataset_info\", model_path + \"dataset_info.py\")\nadd_lib(\"lightning_data\", model_path + \"lightning_data.py\")\nadd_lib(\"lightning_model\", model_path + \"lightning_model.py\")\nadd_lib(\"models\", model_path + \"models.py\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:15:15.037890Z","iopub.execute_input":"2025-09-17T11:15:15.038086Z","iopub.status.idle":"2025-09-17T11:15:15.566045Z","shell.execute_reply.started":"2025-09-17T11:15:15.038072Z","shell.execute_reply":"2025-09-17T11:15:15.565450Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from models import EGAT, EGCN, EGRAPHSAGE\nfrom lightning_model import GraphModel\nfrom lightning_data import GraphDataModule\nfrom dataset_info import datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:15:15.566772Z","iopub.execute_input":"2025-09-17T11:15:15.567022Z","iopub.status.idle":"2025-09-17T11:15:15.570759Z","shell.execute_reply.started":"2025-09-17T11:15:15.566995Z","shell.execute_reply":"2025-09-17T11:15:15.570034Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(os.path.join(dataset_folder, \"labels_names.pkl\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:15:15.571618Z","iopub.execute_input":"2025-09-17T11:15:15.572223Z","iopub.status.idle":"2025-09-17T11:15:15.588274Z","shell.execute_reply.started":"2025-09-17T11:15:15.572205Z","shell.execute_reply":"2025-09-17T11:15:15.587724Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>E_GAT</h2>","metadata":{}},{"cell_type":"code","source":"# print(\"----------------------- TRAINING E_GAT --------------------------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:15:15.589154Z","iopub.execute_input":"2025-09-17T11:15:15.589400Z","iopub.status.idle":"2025-09-17T11:15:15.605763Z","shell.execute_reply.started":"2025-09-17T11:15:15.589383Z","shell.execute_reply":"2025-09-17T11:15:15.605060Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import threading, time, psutil\n\ntry:\n    import pynvml\n    pynvml.nvmlInit()\n    GPU_AVAILABLE = True\nexcept:\n    GPU_AVAILABLE = False\n\ndef monitor_resources(process, peaks, interval=0.2):\n    \"\"\"Monitor CPU + GPU memory usage in a background thread.\"\"\"\n    peaks[\"cpu\"] = 0.0\n    peaks[\"gpu\"] = 0.0\n    \n    handle = None\n    if GPU_AVAILABLE:\n        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n    \n    while getattr(threading.current_thread(), \"running\", True):\n        try:\n            cpu_mem = process.memory_info().rss / (1024 ** 2)\n            gpu_mem = 0.0\n            if GPU_AVAILABLE:\n                gpu_mem = pynvml.nvmlDeviceGetMemoryInfo(handle).used / (1024 ** 2)\n            peaks[\"cpu\"] = max(peaks[\"cpu\"], cpu_mem)\n            peaks[\"gpu\"] = max(peaks[\"gpu\"], gpu_mem)\n        except psutil.NoSuchProcess:\n            break\n        time.sleep(interval)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n\ndataset = datasets[dataset_name]\n\n# Hyperparameters\n\nactivation = F.relu\ngraphs_folder=graph_folder\n# graphs_folder = os.path.join(dataset_folder, g_type)\n# if dataset_name == \"cic_ids_2017\":\n#     graphs_folder = \"/kaggle/working/cic_ids_2017/flow__multi_class__n_feats__unsorted\"\nlogs_folder = os.path.join(\"logs\", dataset.name)\nos.makedirs(logs_folder, exist_ok=True)\nwandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\nos.makedirs(wandb_runs_path, exist_ok=True)\n\nlabels_mapping = {0: \"Normal\", 1: \"Attack\"}\nnum_classes = 2\nif multi_class:\n    with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n    # /kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\n    # with open(\"/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\", \"rb\") as f:\n        labels_names = pickle.load(f)\n    labels_mapping = labels_names[0]\nnum_classes = len(labels_mapping)\n\ndataset_kwargs = dict(\n    use_node_features=use_centralities_nfeats,\n    multi_class=True,\n    using_masking=False,\n    masked_class=2,\n    num_workers=0,\n    label_col=dataset.label_col,\n    class_num_col=dataset.class_num_col,\n    device='cuda' if torch.cuda.is_available() else \"cpu\"\n)\n\ndata_module = GraphDataModule(\n    graphs_folder, batch_size=1, **dataset_kwargs)\ndata_module.setup()\n\nndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\nedim = next(iter(data_module.train_dataloader())).edata['h'].shape[-1]\n\n# my_models = {\n#     # \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation,\n#     #               dropout, residual, num_classes),\n#     # f\"e_graphsage_{aggregation}\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                          residual, num_classes, num_neighbors=number_neighbors, aggregation=aggregation),\n#     # f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                                      residual, num_classes, num_neighbors=None, aggregation=aggregation),\n#     # \"e_gat_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                   # residual, num_classes, num_neighbors=number_neighbors),\n#     \"e_gat_no_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes, num_neighbors=None),\n# }\nmy_models={\n    \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes),\n    f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,residual, num_classes, num_neighbors=None, aggregation=aggregation),\n    f\"e_graphsage_{aggregation}\": EGRAPHSAGE(\n        ndim, edim, ndim_out, num_layers, activation, dropout,\n        residual, num_classes, num_neighbors=number_neighbors,\n        aggregation=aggregation\n    ),\n    f\"e_graphsage_sum_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,residual, num_classes, num_neighbors=None, aggregation=\"sum\"),\n    f\"e_graphsage_sum\": EGRAPHSAGE(\n        ndim, edim, ndim_out, num_layers, activation, dropout,\n        residual, num_classes, num_neighbors=number_neighbors,\n        aggregation=\"sum\"\n    ),\n    \"e_gat_sampling\": EGAT(\n        ndim, edim, ndim_out, num_layers, activation, dropout,\n        residual, num_classes, num_neighbors=number_neighbors\n    ),\n    \"e_gat_no_sampling\": EGAT(\n        ndim, edim, ndim_out, num_layers, activation, dropout,\n        residual, num_classes, num_neighbors=None\n    ),\n}\n\ncriterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n\nfor model_name, model in my_models.items():\n    print(\"----------------------- PREPERATION FOR TRAINING \",model_name,\" --------------------------\")\n    config = {\n        \"run_dtime\": run_dtime,\n        \"type\": \"GNN\",\n        \"model_name\": model_name,\n        \"max_epochs\": max_epochs,\n        \"learning_rate\": learning_rate,\n        \"weight_decay\": weight_decay,\n        \"ndim_out\": ndim_out,\n        \"num_layers\": num_layers,\n        \"number_neighbors\": number_neighbors,\n        \"activation\": activation.__name__,\n        \"dropout\": dropout,\n        \"residual\": residual,\n        \"multi_class\": multi_class,\n        \"aggregation\": aggregation,\n        \"early_stopping_patience\": early_stopping_patience,\n        \"use_centralities_nfeats\": use_centralities_nfeats,}\n\n    # process = psutil.Process(os.getpid())  # current process\n    # start_time = time.time()\n    # start_mem = process.memory_info().rss / (1024 ** 2)  # in MB\n    graph_model = GraphModel(model, criterion, learning_rate, config, model_name,\n                             labels_mapping, weight_decay=weight_decay, using_wandb=using_wandb, norm=False, multi_class=True)\n\n    \n    if using_wandb:\n        wandb_logger = WandbLogger(\n            project=f\"GNN-Analysis-{dataset.name}\", # Project Name Wandb\n            name=f\"{model_name}---{g_type}\",\n            config=config,\n            save_dir=wandb_runs_path\n        )\n    else:\n        wandb_logger = None\n        \n    f1_checkpoint_callback = ModelCheckpoint(\n        monitor=\"val_f1_score\",\n        mode=\"max\",\n        filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n        save_top_k=save_top_k,\n        verbose=False,\n    )\n    early_stopping_callback = EarlyStopping(\n        monitor=\"val_loss\",\n        mode=\"min\",\n        patience=early_stopping_patience,\n        verbose=False,\n    )\n\n\n    trainer = pl.Trainer(\n        max_epochs=max_epochs,\n        num_sanity_val_steps=0,\n        log_every_n_steps=0,\n        callbacks=[\n            f1_checkpoint_callback,\n            early_stopping_callback\n        ],\n        default_root_dir=logs_folder,\n        logger=wandb_logger if using_wandb else None,\n    )\n\n    print(\"------------- start training \",model_name,\" ---------------------\")\n    \n    # Measure start time and memory\n    cpu_mem_used=gpu_mem_used=elapsed_time=0\n    process = psutil.Process(os.getpid())\n    cpu_mem_before = process.memory_info().rss / (1024 ** 2)\n    handle = None\n    if GPU_AVAILABLE:\n        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n    if GPU_AVAILABLE:\n        gpu_mem_before = pynvml.nvmlDeviceGetMemoryInfo(handle).used / (1024 ** 2)\n    \n    process = psutil.Process(os.getpid())\n    peaks = {}\n    \n    monitor_thread = threading.Thread(target=monitor_resources, args=(process, peaks))\n    monitor_thread.running = True\n    monitor_thread.start()\n    \n    start_time = time.time()\n    trainer.fit(graph_model, datamodule=data_module)\n    \n    test_results = []\n    \n\n    print(process)\n    for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n        graph_model.test_prefix = f\"best_f1_{i}\"\n        results = trainer.test(graph_model, datamodule=data_module, ckpt_path=k)\n        # End time and memory\n        elapsed_time = time.time() - start_time\n        # Stop thread safely\n        monitor_thread.running = False\n        monitor_thread.join()\n        \n        peak_cpu = peaks.get(\"cpu\", 0.0)\n        peak_gpu = peaks.get(\"gpu\", 0.0)\n        cpu_mem_used=peak_cpu-cpu_mem_before\n        gpu_mem_used=peak_gpu-gpu_mem_before\n        test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n\n        print(f\"\\n--- Test Run {i+1} ---\")\n        print(\"graph_model: \", graph_model)\n        print(\"data_module: \", data_module)\n        print(\"results: \", results)\n        logs = {\n        \"median_f1_of_best_f1\": np.median(test_results),\n        \"max_f1_of_best_f1\": np.max(test_results),\n        \"avg_f1_of_best_f1\": np.mean(test_results),\n            \n    }\n    print(\"test_results: \", test_results)\n    \n    metric_dir = \"/kaggle/working/temp\"\n    os.makedirs(metric_dir, exist_ok=True)\n    \n    json_path = os.path.join(metric_dir, f\"{model_name}_metrics.json\")\n    \n    # Create dataframe with a single row\n    df = pd.DataFrame([{\n        # \"model_name\": model_name,\n        \"TEST_ACCURACY\": test_results[0],\n        \"Time Consumption (s)\": elapsed_time,\n        # \"Memory Consumption (MB)\": memory_consumption_mb\n        \"CPU_Peak_MB\": peak_cpu,\n        \"GPU_Peak_MB\": peak_gpu,\n        \"cpu_mem_used\":cpu_mem_used,\n        \"gpu_mem_used\":gpu_mem_used,\n    }])\n    \n    # Save to JSON (records = list of dicts)\n    df.to_json(json_path, orient=\"records\", indent=4)\n    \n    print(f\"Metrics saved to {json_path}\")\n\n\n    if using_wandb:\n        wandb.log(logs)\n        wandb.finish()\n    else:\n        trainer.logger.log_metrics(logs, step=trainer.global_step)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nlabels_map = {\n    \"0\": \"Benign\",\n    \"1\": \"backdoor\",\n    \"2\": \"ddos\",\n    \"3\": \"dos\",\n    \"4\": \"injection\",\n    \"5\": \"mitm\",\n    \"6\": \"password\",\n    \"7\": \"ransomware\",\n    \"8\": \"scanning\",\n    \"9\": \"xss\"\n}\ndef json_to_csv_per_file(json_folder, output_folder):\n    os.makedirs(output_folder, exist_ok=True)\n    rows_all=[]\n    rows_m_avg=[]\n    for filename in os.listdir(json_folder):\n        if filename.endswith(\"_results.json\"):\n            no_ext = filename.replace(\"_results.json\", \"\")\n            # Remove the trailing \"_metrics\"\n            print(no_ext)\n            model_name=no_ext\n            metric_path = os.path.join(json_folder,f\"{no_ext}_metrics.json\")\n            print(metric_path)\n            filepath = os.path.join(json_folder, filename)\n            with open(filepath, \"r\") as f:\n                data = json.load(f)\n            rows = []\n            # --- Per-class metrics ---\n            classification_report=data[\"classification_report\"]\n            # print(classification_report)\n            if \"results_fpr_fnr\" in data and \"per_class\" in data[\"results_fpr_fnr\"]:\n                # print(data[\"results_fpr_fnr\"])\n                for cls, metrics in data[\"results_fpr_fnr\"][\"per_class\"].items():\n                    attack=labels_map.get(cls,cls)\n                    print(f\"{attack}\")\n                    metrics_d=classification_report.get(attack)\n                    row = {\n                        \"ATTACK TYPE\": attack,\n                        \"precision\": metrics_d.get(\"precision\"),\n                        \"recall\": metrics_d.get(\"recall\"),\n                        \"f1_score\": metrics_d.get(\"f1-score\"),\n                        \"support\": metrics_d.get(\"support\"),\n                        \"FPR\": metrics.get(\"FPR\"),\n                        \"FNR\": metrics.get(\"FNR\"),\n                    }\n                    # print(row)\n                    # print(metrics_d.get(\"precision\"))\n                    # print()\n                    rows.append(row)\n\n            df_classes = pd.DataFrame(rows)\n            weighted_avg=classification_report['weighted avg']\n            macro_avg=classification_report['macro avg']\n            # print(data)\n            # print(macro_avg)\n            # print(data['classification_report']['accuracy'])\n            # --- Global / test metrics ---\n            with open(metric_path, \"r\") as f:\n                device_data = json.load(f)\n            print(device_data)\n            elapsed_time=device_data[0][\"Time Consumption (s)\"]\n            peak_cpu=device_data[0][\"CPU_Peak_MB\"]\n            peak_gpu=device_data[0][\"GPU_Peak_MB\"]\n            cpu_mem_used=device_data[0][\"cpu_mem_used\"]\n            gpu_mem_used=device_data[0][\"gpu_mem_used\"]\n            # print(\"Memory:\", MEMORY_USED)\n            # print(\"Time:\", TIME_CONSUMED)\n            global_metrics = {\n                'model_name':model_name,\n                \"test_weighted_f1\": data.get(\"test_weighted_f1\"),\n                'test_accuracy':data['classification_report']['accuracy'],\n                'precision': weighted_avg['precision'],\n                'recall': weighted_avg['recall'], \n                'support': weighted_avg['support'],\n                # \"test_fpr\": data.get(\"test_fpr\"),\n                # \"test_fnr\": data.get(\"test_fnr\"),\n                \"global_FPR\": data[\"results_fpr_fnr\"][\"global\"].get(\"FPR\") if \"results_fpr_fnr\" in data and \"global\" in data[\"results_fpr_fnr\"] else None,\n                \"global_FNR\": data[\"results_fpr_fnr\"][\"global\"].get(\"FNR\") if \"results_fpr_fnr\" in data and \"global\" in data[\"results_fpr_fnr\"] else None,\n                # \"time_consumption\":TIME_CONSUMED,\n                # \"memory_consumption\":MEMORY_USED,\n                \"Time Consumption (s)\": elapsed_time,\n                \"CPU_Peak_MB\": peak_cpu,\n                \"GPU_Peak_MB\": peak_gpu,\n                \"cpu_mem_used\":cpu_mem_used,\n                \"gpu_mem_used\":gpu_mem_used,\n            }\n            rows_all.append(\n                {\n                'model_name':model_name,\n                \"test_weighted_f1\": data.get(\"test_weighted_f1\"),\n                'test_accuracy':data['classification_report']['accuracy'],\n                'precision': weighted_avg['precision'],\n                'recall': weighted_avg['recall'], \n                'support': weighted_avg['support'],\n                # \"test_fpr\": data.get(\"test_fpr\"),\n                # \"test_fnr\": data.get(\"test_fnr\"),\n                \"global_FPR\": data[\"results_fpr_fnr\"][\"global\"].get(\"FPR\") if \"results_fpr_fnr\" in data and \"global\" in data[\"results_fpr_fnr\"] else None,\n                \"global_FNR\": data[\"results_fpr_fnr\"][\"global\"].get(\"FNR\") if \"results_fpr_fnr\" in data and \"global\" in data[\"results_fpr_fnr\"] else None,\n                # \"time_consumption\":TIME_CONSUMED,\n                # \"memory_consumption\":MEMORY_USED,\n                \"Time Consumption (s)\": elapsed_time,\n                \"CPU_Peak_MB\": peak_cpu,\n                \"GPU_Peak_MB\": peak_gpu,\n                \"cpu_mem_used\":cpu_mem_used,\n                \"gpu_mem_used\":gpu_mem_used,\n                }\n            )\n            print(global_metrics)\n            # {'precision': 0.4393356617629928, 'recall': 0.5886290738703498, 'f1-score': 0.3932640639233801, 'support': 228621}\n            macro_metrics = {\n                'model_name':model_name,\n                'f1-score': macro_avg['f1-score'],\n                'precision': macro_avg['precision'],\n                'recall': macro_avg['recall'], \n                'support': macro_avg['support'],\n            }\n            rows_m_avg.append(\n                {\n                'model_name':model_name,\n                'f1-score': macro_avg['f1-score'],\n                'precision': macro_avg['precision'],\n                'recall': macro_avg['recall'], \n                'support': macro_avg['support'],\n            }\n            )\n            print(macro_metrics)\n            df_global = pd.DataFrame([global_metrics])\n            # --- Save CSV ---\n            csv_output = os.path.join(output_folder, filename.replace(\".json\", \".csv\"))\n            with open(csv_output, \"w\") as f:\n                f.write(\"### Per-class Metrics\\n\")\n            df_classes.to_csv(csv_output, mode=\"a\", index=False)\n            with open(csv_output, \"a\") as f:\n                f.write(\"\\n### Global Metrics\\n\")\n            df_global.to_csv(csv_output, mode=\"a\", index=False)\n\n            print(f\"Saved {csv_output}\")\n        df = pd.DataFrame(rows_all)\n    temp_dir = \"/kaggle/working/temp/csv_results/\"\n    # print(folder_path)\n    folder=os.path.join(\"/kaggle/working/\",dataset_name)\n    graph_title=folder_path.replace(f\"{folder}/\",\"\")\n    # print(graph_title)\n    # Create folder if it doesn't exist\n    if not os.path.exists(temp_dir):\n        os.makedirs(temp_dir)\n    \n    # Save CSV inside temp/\n    if dataset_name==\"cic_ton_iot\":\n        csv_path = os.path.join(temp_dir, f\"{graph_title}__GNN_BENCHMARKING_CIC_TON_IOT.csv\")\n    else:\n        csv_path = os.path.join(temp_dir, f\"{graph_title}__GNN_BENCHMARKING_CIC_IDS_2017.csv\")\n    df.to_csv(csv_path, index=False)\n    print(f\"‚úÖ CSV saved as {csv_path}\")\n    \n    df = pd.DataFrame(rows_m_avg)\n    temp_dir = \"/kaggle/working/temp/csv_results/\"\n    \n    # Create folder if it doesn't exist\n    if not os.path.exists(temp_dir):\n        os.makedirs(temp_dir)\n    \n    # Save CSV inside temp/\n    if dataset_name==\"cic_ton_iot\":\n        csv_path = os.path.join(temp_dir, f\"{graph_title}__GNN_macro_results_CIC_TON_IOT.csv\")\n    else:\n        csv_path = os.path.join(temp_dir, f\"{graph_title}__GNN_macro_results_CIC_IDS_2017.csv\")\n    df.to_csv(csv_path, index=False)\n    print(f\"‚úÖ CSV saved as {csv_path}\")\n\njson_folder = \"/kaggle/working/temp/\"\noutput_folder = \"/kaggle/working/temp/csv_results\"\njson_to_csv_per_file(json_folder, output_folder)\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T14:12:07.284Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>E_GAT Sampling</h2>","metadata":{}},{"cell_type":"code","source":"# print(\"----------------------- TRAINING E_GAT Sampling--------------------------\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T14:12:07.284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n\n# dataset = datasets[dataset_name]\n\n# # Hyperparameters\n\n# activation = F.relu\n# graphs_folder = graph_folder\n# if dataset_name == \"cic_ids_2017\":\n#     graphs_folder = \"/kaggle/working/cic_ids_2017/flow__multi_class__n_feats__unsorted\"\n# logs_folder = os.path.join(\"logs\", dataset.name)\n# os.makedirs(logs_folder, exist_ok=True)\n# wandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\n# os.makedirs(wandb_runs_path, exist_ok=True)\n\n# labels_mapping = {0: \"Normal\", 1: \"Attack\"}\n# num_classes = 2\n# if multi_class:\n#     with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n#     # /kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\n#     # with open(\"/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\", \"rb\") as f:\n#         labels_names = pickle.load(f)\n#     labels_mapping = labels_names[0]\n# num_classes = len(labels_mapping)\n\n# dataset_kwargs = dict(\n#     use_node_features=use_centralities_nfeats,\n#     multi_class=True,\n#     using_masking=False,\n#     masked_class=2,\n#     num_workers=0,\n#     label_col=dataset.label_col,\n#     class_num_col=dataset.class_num_col,\n#     device='cuda' if torch.cuda.is_available() else \"cpu\"\n# )\n\n# data_module = GraphDataModule(\n#     graphs_folder, batch_size=1, **dataset_kwargs)\n# data_module.setup()\n\n# ndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\n# edim = next(iter(data_module.train_dataloader())).edata['h'].shape[-1]\n\n# my_models = {\n#     # \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation,\n#     #               dropout, residual, num_classes),\n#     # f\"e_graphsage_{aggregation}\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                          residual, num_classes, num_neighbors=number_neighbors, aggregation=aggregation),\n#     # f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                                      residual, num_classes, num_neighbors=None, aggregation=aggregation),\n#     \"e_gat_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                   residual, num_classes, num_neighbors=number_neighbors),\n#     # \"e_gat_no_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes, num_neighbors=None),\n# }\n\n# criterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n\n# for model_name, model in my_models.items():\n        \n#     config = {\n#         \"run_dtime\": run_dtime,\n#         \"type\": \"GNN\",\n#         \"model_name\": model_name,\n#         \"max_epochs\": max_epochs,\n#         \"learning_rate\": learning_rate,\n#         \"weight_decay\": weight_decay,\n#         \"ndim_out\": ndim_out,\n#         \"num_layers\": num_layers,\n#         \"number_neighbors\": number_neighbors,\n#         \"activation\": activation.__name__,\n#         \"dropout\": dropout,\n#         \"residual\": residual,\n#         \"multi_class\": multi_class,\n#         \"aggregation\": aggregation,\n#         \"early_stopping_patience\": early_stopping_patience,\n#         \"use_centralities_nfeats\": use_centralities_nfeats,}\n\n#     graph_model = GraphModel(model, criterion, learning_rate, config, model_name,\n#                              labels_mapping, weight_decay=weight_decay, using_wandb=using_wandb, norm=False, multi_class=True)\n\n    \n#     if using_wandb:\n#         wandb_logger = WandbLogger(\n#             project=f\"GNN-Analysis-{dataset.name}\", # Project Name Wandb\n#             name=f\"{model_name}---{g_type}\",\n#             config=config,\n#             save_dir=wandb_runs_path\n#         )\n#     else:\n#         wandb_logger = None\n        \n#     f1_checkpoint_callback = ModelCheckpoint(\n#         monitor=\"val_f1_score\",\n#         mode=\"max\",\n#         filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n#         save_top_k=save_top_k,\n#         verbose=False,\n#     )\n#     early_stopping_callback = EarlyStopping(\n#         monitor=\"val_loss\",\n#         mode=\"min\",\n#         patience=early_stopping_patience,\n#         verbose=False,\n#     )\n\n#     trainer = pl.Trainer(\n#         max_epochs=max_epochs,\n#         num_sanity_val_steps=0,\n#         log_every_n_steps=0,\n#         callbacks=[\n#             f1_checkpoint_callback,\n#             early_stopping_callback\n#         ],\n#         default_root_dir=logs_folder,\n#         logger=wandb_logger if using_wandb else None,\n#     )\n\n#     trainer.fit(graph_model, datamodule=data_module)\n    \n#     test_results = []\n    \n#     for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n#         graph_model.test_prefix = f\"best_f1_{i}\"\n#         results = trainer.test(\n#             graph_model, datamodule=data_module, ckpt_path=k)\n#         test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n\n#     logs = {\n#         \"median_f1_of_best_f1\": np.median(test_results),\n#         \"max_f1_of_best_f1\": np.max(test_results),\n#         \"avg_f1_of_best_f1\": np.mean(test_results)\n#     }\n\n#     if using_wandb:\n#         wandb.log(logs)\n#         wandb.finish()\n#     else:\n#         trainer.logger.log_metrics(logs, step=trainer.global_step)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T14:12:07.284Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>EGraphSAGE mean</h2>","metadata":{}},{"cell_type":"code","source":"# print(\"----------------------- TRAINING EGraphSAGE mean --------------------------\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T14:12:07.284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n\n# dataset = datasets[dataset_name]\n\n# # Hyperparameters\n\n# activation = F.relu\n# graphs_folder = graph_folder\n# if dataset_name == \"cic_ids_2017\":\n#     graphs_folder = \"/kaggle/working/cic_ids_2017/flow__multi_class__n_feats__unsorted\"\n# logs_folder = os.path.join(\"logs\", dataset.name)\n# os.makedirs(logs_folder, exist_ok=True)\n# wandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\n# os.makedirs(wandb_runs_path, exist_ok=True)\n\n# labels_mapping = {0: \"Normal\", 1: \"Attack\"}\n# num_classes = 2\n# if multi_class:\n#     with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n#     # /kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\n#     # with open(\"/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\", \"rb\") as f:\n#         labels_names = pickle.load(f)\n#     labels_mapping = labels_names[0]\n# num_classes = len(labels_mapping)\n\n# dataset_kwargs = dict(\n#     use_node_features=use_centralities_nfeats,\n#     multi_class=True,\n#     using_masking=False,\n#     masked_class=2,\n#     num_workers=0,\n#     label_col=dataset.label_col,\n#     class_num_col=dataset.class_num_col,\n#     device='cuda' if torch.cuda.is_available() else \"cpu\"\n# )\n\n# data_module = GraphDataModule(\n#     graphs_folder, batch_size=1, **dataset_kwargs)\n# data_module.setup()\n\n# ndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\n# edim = next(iter(data_module.train_dataloader())).edata['h'].shape[-1]\n\n# my_models = {\n#     # \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation,\n#     #               dropout, residual, num_classes),\n#     # f\"e_graphsage_{aggregation}\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                          residual, num_classes, num_neighbors=number_neighbors, aggregation=aggregation),\n#     f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                                                          residual, num_classes, num_neighbors=None, aggregation=aggregation),\n#     # \"e_gat_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #               residual, num_classes, num_neighbors=number_neighbors),\n#     # \"e_gat_no_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes, num_neighbors=None),\n# }\n\n# criterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n\n# for model_name, model in my_models.items():\n        \n#     config = {\n#         \"run_dtime\": run_dtime,\n#         \"type\": \"GNN\",\n#         \"model_name\": model_name,\n#         \"max_epochs\": max_epochs,\n#         \"learning_rate\": learning_rate,\n#         \"weight_decay\": weight_decay,\n#         \"ndim_out\": ndim_out,\n#         \"num_layers\": num_layers,\n#         \"number_neighbors\": number_neighbors,\n#         \"activation\": activation.__name__,\n#         \"dropout\": dropout,\n#         \"residual\": residual,\n#         \"multi_class\": multi_class,\n#         \"aggregation\": aggregation,\n#         \"early_stopping_patience\": early_stopping_patience,\n#         \"use_centralities_nfeats\": use_centralities_nfeats,}\n\n#     graph_model = GraphModel(model, criterion, learning_rate, config, model_name,\n#                              labels_mapping, weight_decay=weight_decay, using_wandb=using_wandb, norm=False, multi_class=True)\n\n    \n#     if using_wandb:\n#         wandb_logger = WandbLogger(\n#             project=f\"GNN-Analysis-{dataset.name}\", # Project Name Wandb\n#             name=f\"{model_name}---{g_type}\",\n#             config=config,\n#             save_dir=wandb_runs_path\n#         )\n#     else:\n#         wandb_logger = None\n        \n#     f1_checkpoint_callback = ModelCheckpoint(\n#         monitor=\"val_f1_score\",\n#         mode=\"max\",\n#         filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n#         save_top_k=save_top_k,\n#         verbose=False,\n#     )\n#     early_stopping_callback = EarlyStopping(\n#         monitor=\"val_loss\",\n#         mode=\"min\",\n#         patience=early_stopping_patience,\n#         verbose=False,\n#     )\n\n#     trainer = pl.Trainer(\n#         max_epochs=max_epochs,\n#         num_sanity_val_steps=0,\n#         log_every_n_steps=0,\n#         callbacks=[\n#             f1_checkpoint_callback,\n#             early_stopping_callback\n#         ],\n#         default_root_dir=logs_folder,\n#         logger=wandb_logger if using_wandb else None,\n#     )\n\n#     trainer.fit(graph_model, datamodule=data_module)\n    \n#     test_results = []\n    \n#     for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n#         graph_model.test_prefix = f\"best_f1_{i}\"\n#         results = trainer.test(\n#             graph_model, datamodule=data_module, ckpt_path=k)\n#         test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n\n#     logs = {\n#         \"median_f1_of_best_f1\": np.median(test_results),\n#         \"max_f1_of_best_f1\": np.max(test_results),\n#         \"avg_f1_of_best_f1\": np.mean(test_results)\n#     }\n\n#     if using_wandb:\n#         wandb.log(logs)\n#         wandb.finish()\n#     else:\n#         trainer.logger.log_metrics(logs, step=trainer.global_step)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T14:12:07.284Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>EGraphSAGE sum</h2>","metadata":{}},{"cell_type":"code","source":"# print(\"----------------------- TRAINING EGraphSAGE sum --------------------------\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T14:12:07.284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n\n# dataset = datasets[dataset_name]\n\n# # Hyperparameters\n# aggregation=\"sum\"\n\n# activation = F.relu\n# graphs_folder = graph_folder\n# if dataset_name == \"cic_ids_2017\":\n#     graphs_folder = \"/kaggle/working/cic_ids_2017/flow__multi_class__n_feats__unsorted\"\n# logs_folder = os.path.join(\"logs\", dataset.name)\n# os.makedirs(logs_folder, exist_ok=True)\n# wandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\n# os.makedirs(wandb_runs_path, exist_ok=True)\n\n# labels_mapping = {0: \"Normal\", 1: \"Attack\"}\n# num_classes = 2\n# if multi_class:\n#     with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n#     # /kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\n#     # with open(\"/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\", \"rb\") as f:\n#         labels_names = pickle.load(f)\n#     labels_mapping = labels_names[0]\n# num_classes = len(labels_mapping)\n\n# dataset_kwargs = dict(\n#     use_node_features=use_centralities_nfeats,\n#     multi_class=True,\n#     using_masking=False,\n#     masked_class=2,\n#     num_workers=0,\n#     label_col=dataset.label_col,\n#     class_num_col=dataset.class_num_col,\n#     device='cuda' if torch.cuda.is_available() else \"cpu\"\n# )\n\n# data_module = GraphDataModule(\n#     graphs_folder, batch_size=1, **dataset_kwargs)\n# data_module.setup()\n\n# ndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\n# edim = next(iter(data_module.train_dataloader())).edata['h'].shape[-1]\n\n# my_models = {\n#     # \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation,\n#     #               dropout, residual, num_classes),\n#     # f\"e_graphsage_{aggregation}\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                          residual, num_classes, num_neighbors=number_neighbors, aggregation=aggregation),\n#     f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                                                          residual, num_classes, num_neighbors=None, aggregation=aggregation),\n#     # \"e_gat_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #               residual, num_classes, num_neighbors=number_neighbors),\n#     # \"e_gat_no_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes, num_neighbors=None),\n# }\n\n# criterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n\n# for model_name, model in my_models.items():\n        \n#     config = {\n#         \"run_dtime\": run_dtime,\n#         \"type\": \"GNN\",\n#         \"model_name\": model_name,\n#         \"max_epochs\": max_epochs,\n#         \"learning_rate\": learning_rate,\n#         \"weight_decay\": weight_decay,\n#         \"ndim_out\": ndim_out,\n#         \"num_layers\": num_layers,\n#         \"number_neighbors\": number_neighbors,\n#         \"activation\": activation.__name__,\n#         \"dropout\": dropout,\n#         \"residual\": residual,\n#         \"multi_class\": multi_class,\n#         \"aggregation\": aggregation,\n#         \"early_stopping_patience\": early_stopping_patience,\n#         \"use_centralities_nfeats\": use_centralities_nfeats,}\n\n#     graph_model = GraphModel(model, criterion, learning_rate, config, model_name,\n#                              labels_mapping, weight_decay=weight_decay, using_wandb=using_wandb, norm=False, multi_class=True)\n\n    \n#     if using_wandb:\n#         wandb_logger = WandbLogger(\n#             project=f\"GNN-Analysis-{dataset.name}\", # Project Name Wandb\n#             name=f\"{model_name}---{g_type}\",\n#             config=config,\n#             save_dir=wandb_runs_path\n#         )\n#     else:\n#         wandb_logger = None\n        \n#     f1_checkpoint_callback = ModelCheckpoint(\n#         monitor=\"val_f1_score\",\n#         mode=\"max\",\n#         filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n#         save_top_k=save_top_k,\n#         verbose=False,\n#     )\n#     early_stopping_callback = EarlyStopping(\n#         monitor=\"val_loss\",\n#         mode=\"min\",\n#         patience=early_stopping_patience,\n#         verbose=False,\n#     )\n\n#     trainer = pl.Trainer(\n#         max_epochs=max_epochs,\n#         num_sanity_val_steps=0,\n#         log_every_n_steps=0,\n#         callbacks=[\n#             f1_checkpoint_callback,\n#             early_stopping_callback\n#         ],\n#         default_root_dir=logs_folder,\n#         logger=wandb_logger if using_wandb else None,\n#     )\n\n#     trainer.fit(graph_model, datamodule=data_module)\n    \n#     test_results = []\n    \n#     for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n#         graph_model.test_prefix = f\"best_f1_{i}\"\n#         results = trainer.test(\n#             graph_model, datamodule=data_module, ckpt_path=k)\n#         test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n\n#     logs = {\n#         \"median_f1_of_best_f1\": np.median(test_results),\n#         \"max_f1_of_best_f1\": np.max(test_results),\n#         \"avg_f1_of_best_f1\": np.mean(test_results)\n#     }\n\n#     if using_wandb:\n#         wandb.log(logs)\n#         wandb.finish()\n#     else:\n#         trainer.logger.log_metrics(logs, step=trainer.global_step)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T14:12:07.284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" # wandb.finish()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T14:12:07.284Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>EGraphSAGE Sampling mean</h2>","metadata":{}},{"cell_type":"code","source":"# print(\"----------------------- TRAINING EGraphSAGE Sampling mean --------------------------\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T14:12:07.284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n\n# dataset = datasets[dataset_name]\n\n# # Hyperparameters\n# aggregation=\"mean\"\n\n# activation = F.relu\n# graphs_folder = graph_folder\n# if dataset_name == \"cic_ids_2017\":\n#     graphs_folder = \"/kaggle/working/cic_ids_2017/flow__multi_class__n_feats__unsorted\"\n# logs_folder = os.path.join(\"logs\", dataset.name)\n# os.makedirs(logs_folder, exist_ok=True)\n# wandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\n# os.makedirs(wandb_runs_path, exist_ok=True)\n\n# labels_mapping = {0: \"Normal\", 1: \"Attack\"}\n# num_classes = 2\n# if multi_class:\n#     with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n#     # /kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\n#     # with open(\"/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\", \"rb\") as f:\n#         labels_names = pickle.load(f)\n#     labels_mapping = labels_names[0]\n# num_classes = len(labels_mapping)\n\n# dataset_kwargs = dict(\n#     use_node_features=use_centralities_nfeats,\n#     multi_class=True,\n#     using_masking=False,\n#     masked_class=2,\n#     num_workers=0,\n#     label_col=dataset.label_col,\n#     class_num_col=dataset.class_num_col,\n#     device='cuda' if torch.cuda.is_available() else \"cpu\"\n# )\n\n# data_module = GraphDataModule(\n#     graphs_folder, batch_size=1, **dataset_kwargs)\n# data_module.setup()\n\n# ndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\n# edim = next(iter(data_module.train_dataloader())).edata['h'].shape[-1]\n\n# my_models = {\n#     # \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation,\n#     #               dropout, residual, num_classes),\n#     f\"e_graphsage_{aggregation}\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                                              residual, num_classes, num_neighbors=number_neighbors, aggregation=aggregation),\n#     # f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                                      residual, num_classes, num_neighbors=None, aggregation=aggregation),\n#     # \"e_gat_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                   # residual, num_classes, num_neighbors=number_neighbors),\n#     # \"e_gat_no_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes, num_neighbors=None),\n# }\n\n# criterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n\n# for model_name, model in my_models.items():\n        \n#     config = {\n#         \"run_dtime\": run_dtime,\n#         \"type\": \"GNN\",\n#         \"model_name\": model_name,\n#         \"max_epochs\": max_epochs,\n#         \"learning_rate\": learning_rate,\n#         \"weight_decay\": weight_decay,\n#         \"ndim_out\": ndim_out,\n#         \"num_layers\": num_layers,\n#         \"number_neighbors\": number_neighbors,\n#         \"activation\": activation.__name__,\n#         \"dropout\": dropout,\n#         \"residual\": residual,\n#         \"multi_class\": multi_class,\n#         \"aggregation\": aggregation,\n#         \"early_stopping_patience\": early_stopping_patience,\n#         \"use_centralities_nfeats\": use_centralities_nfeats,}\n\n#     graph_model = GraphModel(model, criterion, learning_rate, config, model_name,\n#                              labels_mapping, weight_decay=weight_decay, using_wandb=using_wandb, norm=False, multi_class=True)\n\n    \n#     if using_wandb:\n#         wandb_logger = WandbLogger(\n#             project=f\"GNN-Analysis-{dataset.name}\", # Project Name Wandb\n#             name=f\"{model_name}---{g_type}\",\n#             config=config,\n#             save_dir=wandb_runs_path\n#         )\n#     else:\n#         wandb_logger = None\n        \n#     f1_checkpoint_callback = ModelCheckpoint(\n#         monitor=\"val_f1_score\",\n#         mode=\"max\",\n#         filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n#         save_top_k=save_top_k,\n#         verbose=False,\n#     )\n#     early_stopping_callback = EarlyStopping(\n#         monitor=\"val_loss\",\n#         mode=\"min\",\n#         patience=early_stopping_patience,\n#         verbose=False,\n#     )\n\n#     trainer = pl.Trainer(\n#         max_epochs=max_epochs,\n#         num_sanity_val_steps=0,\n#         log_every_n_steps=0,\n#         callbacks=[\n#             f1_checkpoint_callback,\n#             early_stopping_callback\n#         ],\n#         default_root_dir=logs_folder,\n#         logger=wandb_logger if using_wandb else None,\n#     )\n\n#     trainer.fit(graph_model, datamodule=data_module)\n    \n#     test_results = []\n    \n#     for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n#         graph_model.test_prefix = f\"best_f1_{i}\"\n#         results = trainer.test(\n#             graph_model, datamodule=data_module, ckpt_path=k)\n#         test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n\n#     logs = {\n#         \"median_f1_of_best_f1\": np.median(test_results),\n#         \"max_f1_of_best_f1\": np.max(test_results),\n#         \"avg_f1_of_best_f1\": np.mean(test_results)\n#     }\n\n#     if using_wandb:\n#         wandb.log(logs)\n#         wandb.finish()\n#     else:\n#         trainer.logger.log_metrics(logs, step=trainer.global_step)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T14:12:07.284Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>EGraphSAGE Sampling sum</h2>","metadata":{}},{"cell_type":"code","source":"# print(\"----------------------- TRAINING EGraphSAGE Sampling sum --------------------------\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T14:12:07.284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n\n# dataset = datasets[dataset_name]\n\n# # Hyperparameters\n# aggregation=\"sum\"\n\n# activation = F.relu\n# graphs_folder = graph_folder\n# if dataset_name == \"cic_ids_2017\":\n#     graphs_folder = \"/kaggle/working/cic_ids_2017/flow__multi_class__n_feats__unsorted\"\n# logs_folder = os.path.join(\"logs\", dataset.name)\n# os.makedirs(logs_folder, exist_ok=True)\n# wandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\n# os.makedirs(wandb_runs_path, exist_ok=True)\n\n# labels_mapping = {0: \"Normal\", 1: \"Attack\"}\n# num_classes = 2\n# if multi_class:\n#     with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n#     # /kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\n#     # with open(\"/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\", \"rb\") as f:\n#         labels_names = pickle.load(f)\n#     labels_mapping = labels_names[0]\n# num_classes = len(labels_mapping)\n\n# dataset_kwargs = dict(\n#     use_node_features=use_centralities_nfeats,\n#     multi_class=True,\n#     using_masking=False,\n#     masked_class=2,\n#     num_workers=0,\n#     label_col=dataset.label_col,\n#     class_num_col=dataset.class_num_col,\n#     device='cuda' if torch.cuda.is_available() else \"cpu\"\n# )\n\n# data_module = GraphDataModule(\n#     graphs_folder, batch_size=1, **dataset_kwargs)\n# data_module.setup()\n\n# ndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\n# edim = next(iter(data_module.train_dataloader())).edata['h'].shape[-1]\n\n# my_models = {\n#     # \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation,\n#     #               dropout, residual, num_classes),\n#     f\"e_graphsage_{aggregation}\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                                              residual, num_classes, num_neighbors=number_neighbors, aggregation=aggregation),\n#     # f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                                      residual, num_classes, num_neighbors=None, aggregation=aggregation),\n#     # \"e_gat_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                   # residual, num_classes, num_neighbors=number_neighbors),\n#     # \"e_gat_no_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes, num_neighbors=None),\n# }\n\n# criterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n\n# for model_name, model in my_models.items():\n        \n#     config = {\n#         \"run_dtime\": run_dtime,\n#         \"type\": \"GNN\",\n#         \"model_name\": model_name,\n#         \"max_epochs\": max_epochs,\n#         \"learning_rate\": learning_rate,\n#         \"weight_decay\": weight_decay,\n#         \"ndim_out\": ndim_out,\n#         \"num_layers\": num_layers,\n#         \"number_neighbors\": number_neighbors,\n#         \"activation\": activation.__name__,\n#         \"dropout\": dropout,\n#         \"residual\": residual,\n#         \"multi_class\": multi_class,\n#         \"aggregation\": aggregation,\n#         \"early_stopping_patience\": early_stopping_patience,\n#         \"use_centralities_nfeats\": use_centralities_nfeats,}\n\n#     graph_model = GraphModel(model, criterion, learning_rate, config, model_name,\n#                              labels_mapping, weight_decay=weight_decay, using_wandb=using_wandb, norm=False, multi_class=True)\n\n    \n#     if using_wandb:\n#         wandb_logger = WandbLogger(\n#             project=f\"GNN-Analysis-{dataset.name}\", # Project Name Wandb\n#             name=f\"{model_name}---{g_type}\",\n#             config=config,\n#             save_dir=wandb_runs_path\n#         )\n#     else:\n#         wandb_logger = None\n        \n#     f1_checkpoint_callback = ModelCheckpoint(\n#         monitor=\"val_f1_score\",\n#         mode=\"max\",\n#         filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n#         save_top_k=save_top_k,\n#         verbose=False,\n#     )\n#     early_stopping_callback = EarlyStopping(\n#         monitor=\"val_loss\",\n#         mode=\"min\",\n#         patience=early_stopping_patience,\n#         verbose=False,\n#     )\n\n#     trainer = pl.Trainer(\n#         max_epochs=max_epochs,\n#         num_sanity_val_steps=0,\n#         log_every_n_steps=0,\n#         callbacks=[\n#             f1_checkpoint_callback,\n#             early_stopping_callback\n#         ],\n#         default_root_dir=logs_folder,\n#         logger=wandb_logger if using_wandb else None,\n#     )\n\n#     trainer.fit(graph_model, datamodule=data_module)\n    \n#     test_results = []\n    \n#     for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n#         graph_model.test_prefix = f\"best_f1_{i}\"\n#         results = trainer.test(\n#             graph_model, datamodule=data_module, ckpt_path=k)\n#         test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n\n#     logs = {\n#         \"median_f1_of_best_f1\": np.median(test_results),\n#         \"max_f1_of_best_f1\": np.max(test_results),\n#         \"avg_f1_of_best_f1\": np.mean(test_results)\n#     }\n\n#     if using_wandb:\n#         wandb.log(logs)\n#         wandb.finish()\n#     else:\n#         trainer.logger.log_metrics(logs, step=trainer.global_step)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T14:12:07.285Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>E_GCN</h2>","metadata":{}},{"cell_type":"code","source":"# print(\"----------------------- TRAINING E_GCN --------------------------\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T14:12:07.285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n\n# dataset = datasets[dataset_name]\n\n# # Hyperparameters\n# aggregation=\"mean\"\n\n# activation = F.relu\n# graphs_folder = graph_folder\n# if dataset_name == \"cic_ids_2017\":\n#     graphs_folder = \"/kaggle/working/cic_ids_2017/flow__multi_class__n_feats__unsorted\"\n# logs_folder = os.path.join(\"logs\", dataset.name)\n# os.makedirs(logs_folder, exist_ok=True)\n# wandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\n# os.makedirs(wandb_runs_path, exist_ok=True)\n\n# labels_mapping = {0: \"Normal\", 1: \"Attack\"}\n# num_classes = 2\n# if multi_class:\n#     with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n#     # /kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\n#     # with open(\"/kaggle/input/cic-ids-2017-gnn-2/labels_names.pkl\", \"rb\") as f:\n#         labels_names = pickle.load(f)\n#     labels_mapping = labels_names[0]\n# num_classes = len(labels_mapping)\n\n# dataset_kwargs = dict(\n#     use_node_features=use_centralities_nfeats,\n#     multi_class=True,\n#     using_masking=False,\n#     masked_class=2,\n#     num_workers=0,\n#     label_col=dataset.label_col,\n#     class_num_col=dataset.class_num_col,\n#     device='cuda' if torch.cuda.is_available() else \"cpu\"\n# )\n\n# data_module = GraphDataModule(\n#     graphs_folder, batch_size=1, **dataset_kwargs)\n# data_module.setup()\n\n# ndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\n# edim = next(iter(data_module.train_dataloader())).edata['h'].shape[-1]\n\n# my_models = {\n#     \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation,\n#                   dropout, residual, num_classes),\n#     # f\"e_graphsage_{aggregation}\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                          residual, num_classes, num_neighbors=number_neighbors, aggregation=aggregation),\n#     # f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n#     #                                                      residual, num_classes, num_neighbors=None, aggregation=aggregation),\n#     # \"e_gat_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n#                   # residual, num_classes, num_neighbors=number_neighbors),\n#     # \"e_gat_no_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout, residual, num_classes, num_neighbors=None),\n# }\n\n# criterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n\n# for model_name, model in my_models.items():\n        \n#     config = {\n#         \"run_dtime\": run_dtime,\n#         \"type\": \"GNN\",\n#         \"model_name\": model_name,\n#         \"max_epochs\": max_epochs,\n#         \"learning_rate\": learning_rate,\n#         \"weight_decay\": weight_decay,\n#         \"ndim_out\": ndim_out,\n#         \"num_layers\": num_layers,\n#         \"number_neighbors\": number_neighbors,\n#         \"activation\": activation.__name__,\n#         \"dropout\": dropout,\n#         \"residual\": residual,\n#         \"multi_class\": multi_class,\n#         \"aggregation\": aggregation,\n#         \"early_stopping_patience\": early_stopping_patience,\n#         \"use_centralities_nfeats\": use_centralities_nfeats,}\n\n#     graph_model = GraphModel(model, criterion, learning_rate, config, model_name,\n#                              labels_mapping, weight_decay=weight_decay, using_wandb=using_wandb, norm=False, multi_class=True)\n\n    \n#     if using_wandb:\n#         wandb_logger = WandbLogger(\n#             project=f\"GNN-Analysis-{dataset.name}\", # Project Name Wandb\n#             name=f\"{model_name}---{g_type}\",\n#             config=config,\n#             save_dir=wandb_runs_path\n#         )\n#     else:\n#         wandb_logger = None\n        \n#     f1_checkpoint_callback = ModelCheckpoint(\n#         monitor=\"val_f1_score\",\n#         mode=\"max\",\n#         filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n#         save_top_k=save_top_k,\n#         verbose=False,\n#     )\n#     early_stopping_callback = EarlyStopping(\n#         monitor=\"val_loss\",\n#         mode=\"min\",\n#         patience=early_stopping_patience,\n#         verbose=False,\n#     )\n\n#     trainer = pl.Trainer(\n#         max_epochs=max_epochs,\n#         num_sanity_val_steps=0,\n#         log_every_n_steps=0,\n#         callbacks=[\n#             f1_checkpoint_callback,\n#             early_stopping_callback\n#         ],\n#         default_root_dir=logs_folder,\n#         logger=wandb_logger if using_wandb else None,\n#     )\n\n#     trainer.fit(graph_model, datamodule=data_module)\n    \n#     test_results = []\n    \n#     for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n#         graph_model.test_prefix = f\"best_f1_{i}\"\n#         results = trainer.test(\n#             graph_model, datamodule=data_module, ckpt_path=k)\n#         test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n\n#     logs = {\n#         \"median_f1_of_best_f1\": np.median(test_results),\n#         \"max_f1_of_best_f1\": np.max(test_results),\n#         \"avg_f1_of_best_f1\": np.mean(test_results)\n#     }\n\n#     if using_wandb:\n#         wandb.log(logs)\n#         wandb.finish()\n#     else:\n#         trainer.logger.log_metrics(logs, step=trainer.global_step)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-17T14:12:07.285Z"}},"outputs":[],"execution_count":null}]}