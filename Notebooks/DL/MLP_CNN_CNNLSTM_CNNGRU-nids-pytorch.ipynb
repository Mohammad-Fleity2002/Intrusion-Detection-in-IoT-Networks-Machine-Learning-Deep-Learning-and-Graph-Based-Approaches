{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8089266,"sourceType":"datasetVersion","datasetId":4775518},{"sourceId":8089281,"sourceType":"datasetVersion","datasetId":4775527},{"sourceId":470644,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":379672,"modelId":399557},{"sourceId":593572,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":444208,"modelId":460697}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import itertools\nimport json\nimport logging\nimport os\nimport pickle\nimport random\nimport shutil\nimport time\nfrom collections import OrderedDict, defaultdict\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\nfrom omegaconf import OmegaConf\nfrom pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\nfrom scipy.sparse import csr_matrix, diags\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.utils import class_weight as sklearn_class_weight\nfrom torch.optim import SGD, Adam\nfrom torch.utils.data import DataLoader, Dataset, WeightedRandomSampler","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:20:57.091676Z","iopub.execute_input":"2025-10-01T18:20:57.091923Z","iopub.status.idle":"2025-10-01T18:21:19.461183Z","shell.execute_reply.started":"2025-10-01T18:20:57.091906Z","shell.execute_reply":"2025-10-01T18:21:19.460634Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # df = pd.read_parquet(\"/kaggle/input/cic-ids-2017-parquet/cic_ids_2017.parquet\")\n# df = pd.read_parquet(\"/kaggle/input/cic-ton-iot-parquet/cic_ton_iot.parquet\")\n# print(df.head())\n# print(\"Shape:\", df.shape)\n# print(\"Columns:\", df.columns.tolist())\n# print(\"\\nInfo:\")\n# print(df.info())\n# print(\"\\nDescription:\")\n# print(df.describe(include='all'))\n# print(\"\\nMissing Values:\")\n# print(df.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T13:36:34.309359Z","iopub.execute_input":"2025-10-01T13:36:34.309811Z","iopub.status.idle":"2025-10-01T13:36:34.313167Z","shell.execute_reply.started":"2025-10-01T13:36:34.309791Z","shell.execute_reply":"2025-10-01T13:36:34.312462Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# if os.path.isdir(\"processed_data\"):\n#     shutil.rmtree(\"processed_data\")\n\n# if os.path.isdir(\"logs\"):\n#     shutil.rmtree(\"logs\")\n\n# if os.path.isdir(\"temp\"):\n#     shutil.rmtree(\"temp\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:21:29.970321Z","iopub.execute_input":"2025-10-01T18:21:29.971079Z","iopub.status.idle":"2025-10-01T18:21:29.974239Z","shell.execute_reply.started":"2025-10-01T18:21:29.971054Z","shell.execute_reply":"2025-10-01T18:21:29.973640Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install powerlaw","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:21:30.726020Z","iopub.execute_input":"2025-10-01T18:21:30.726289Z","iopub.status.idle":"2025-10-01T18:21:35.906968Z","shell.execute_reply.started":"2025-10-01T18:21:30.726268Z","shell.execute_reply":"2025-10-01T18:21:35.905995Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"models = [\n    \"mlp\",\n    # \"cnn\",\n    # \"cnn_lstm\",\n    # \"cnn_gru\",\n    # \"gru\"\n]\n\ndataset_name = \"cic_ids_2017\"\ndataset_name_d = \"cic-ids-2017\"\ndataset_file = \"cic_ids_2017.parquet\"\noriginal_datasets_files_path = \"/kaggle/input/cic-ids-2017-parquet\"\nname = \"cic_ids_2017\"\n\n# name = \"cic_ton_iot\"\n# dataset_name = \"cic_ton_iot\"\n# dataset_name_d = \"cic-ton-iot\"\n# dataset_file = \"cic_ton_iot.parquet\"\n# original_datasets_files_path = \"/kaggle/input/cic-ton-iot-parquet\"\n\ndataset_file_raw_type = \"parquet\"\nrun_dtime = time.strftime(\"%Y%m%d-%H%M%S\")\n\nmain_processed_dir = \"processed_data\"\nos.makedirs(main_processed_dir, exist_ok=True)\n# CHANGE 1.02\nnew_path = os.path.join(\"/kaggle/working/\",f\"{dataset_name}.parquet\")\ngraph_path = os.path.join(\"/kaggle/working/\",f\"{dataset_name}.gpickle\")\n\n# cn_measures = [\"degree\", \"betweenness\", \"closeness\", \"eigenvector\"]\ncn_measures = [\"betweenness\", \"degree\", \"pagerank\", \"closeness\", \"k_truss\"]\nnetwork_features = ['src_betweenness', 'dst_betweenness', 'src_degree', 'dst_degree', 'src_pagerank', 'dst_pagerank', 'src_closeness', 'dst_closeness', 'src_k_truss', 'dst_k_truss']\nif dataset_name==\"cic_ids_2017\":\n    dataset_DF_name = \"CICIDS2017\"\nelse:\n    dataset_DF_name = \"CICTONIOT\"\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:21:35.908767Z","iopub.execute_input":"2025-10-01T18:21:35.909044Z","iopub.status.idle":"2025-10-01T18:21:35.915630Z","shell.execute_reply.started":"2025-10-01T18:21:35.909021Z","shell.execute_reply":"2025-10-01T18:21:35.914907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import wandb\n# print(wandb.__version__)\n# from wandb.keras import WandbCallback\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:21:35.916425Z","iopub.execute_input":"2025-10-01T18:21:35.916689Z","iopub.status.idle":"2025-10-01T18:21:35.938648Z","shell.execute_reply.started":"2025-10-01T18:21:35.916673Z","shell.execute_reply":"2025-10-01T18:21:35.937950Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Configurations","metadata":{}},{"cell_type":"code","source":"logger = logging.getLogger(__name__)\n# secret_label = \"wandb-secret\"\nsecret_label=\"mohammad_wandb_secret\"\nsecret_value = UserSecretsClient().get_secret(secret_label)\n\nif wandb.run:\n    wandb.finish()\n    \nwandb.login(key=secret_value)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:21:35.940121Z","iopub.execute_input":"2025-10-01T18:21:35.940343Z","iopub.status.idle":"2025-10-01T18:21:42.950995Z","shell.execute_reply.started":"2025-10-01T18:21:35.940328Z","shell.execute_reply":"2025-10-01T18:21:42.950351Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_cfg = OmegaConf.create({\n        \"random_seed\": 42,\n        \"training\": {\n            \"multi_class\": True,\n            \"batch_size\": 256,\n            # \"batch_size\": 512,\n            \"max_epochs\": 15,\n            # \"max_epochs\": 10,\n            # \"max_epochs\": 1,\n            \"optimizer\": \"adam\",\n            # \"learning_rate\": 0.0008,\n            # for first dataset\n            \"learning_rate\": 0.001,\n            \"lr_decay\": True,\n            \"lr_decay_rate\": 0.5,\n            \"weight_decay\": 0.01,\n            \"LAMBD_2\": 0.01,\n            \"dropout\": True,\n            \"dropout_rate\": 0.5,\n            \"batch_norm\": True,\n            \"use_weighted_loss\": False,\n            \"weighted_loss_version\": \"v5\",\n            \"loss_type\": \"cross_entropy\",\n            \"sequence_length\": 3,\n            \"stride\": 1,\n            \"using_masking\": False,\n            \"masked_class\": 2,\n            \"oversample\": True,\n        },\n        \"logging\": {\n            # \"selected_type\": \"tensorboard\",\n            \"selected_type\": \"wandb\",\n            \"wandb\": {\n                \"project\": f\"DL-NIDS-2--{dataset_name_d}\",\n                \"entity\": \"mtermos-cesi\",\n                \"tags\": [ \"dl_nids\", \"dataset_name\" ],\n                \"save_dir\": \"logs/wandb_runs\",\n            },\n            \"tensorboard\": {\n                \"project\": f\"DL-NIDS-2--{dataset_name_d}\",\n                \"save_dir\": \"logs/tensorboard_runs\"\n            }\n        },\n        \"dataset\": {\n            \"name\": dataset_name,\n            \"raw\": dataset_file,\n            \"raw_type\": dataset_file_raw_type,\n            \"test_size\": 0.1   \n        },\n        \"dataset_properties\": {\n            \"src_ip_col\": \"Src IP\",\n            \"dst_ip_col\": \"Dst IP\",\n            \"timestamp_col\": \"Timestamp\",\n            \"flow_id_col\": \"Flow ID\",\n            \"timestamp_format\": \"%d/%m/%Y %I:%M:%S %p\",\n            \"label_col\": \"Label\",\n            \"class_col\": \"Attack\",\n            \"class_num_col\": \"Class\",\n            \"val_size\": 0.15,\n            # \"val_size\": 0.1,\n            \"drop_columns\": [ \"Flow ID\", \"Src IP\", \"Dst IP\", \"Timestamp\", \"Src Port\", \"Dst Port\", \"Attack\" ],\n            \"weak_columns\": [ \"Flow Duration\", \"Tot Bwd Pkts\", \"TotLen Bwd Pkts\", \"Fwd Pkt Len Max\", \"Fwd Pkt Len Mean\", \"Bwd Pkt Len Max\", \"Bwd Pkt Len Mean\", \"Bwd Pkt Len Std\", \"Flow Pkts/s\", \"Flow IAT Mean\", \"Flow IAT Max\", \"Fwd IAT Mean\", \"Bwd IAT Mean\", \"Pkt Len Max\", \"Pkt Len Mean\", \"Pkt Size Avg\", \"Fwd Byts/b Avg\", \"Fwd Pkts/b Avg\", \"Fwd Blk Rate Avg\", \"Active Mean, Idle Mean\" ]\n        },\n    })\n\nall_models_cfgs = {\n    # # TRIAL ONE\n    # \"cnn\": OmegaConf.create({\n    #     \"model\": {\"name\": \"cnn\", \"type\": \"cnn\"},\n    #     \"layers\": [\"cnn\", \"dense\"],\n    #     \"input_layer_norm\": True,\n    #     \"cnn\": {\n    #         \"filters\": [32, 64, 128],\n    #         \"kernel_sizes\": [7, 5, 3],\n    #         \"activation\": \"relu\",\n    #         \"dropout\": False,\n    #         \"dropout_rate\": 0.0,\n    #         \"batch_norm\": False,\n    #         \"layer_norm\": False,\n    #     },\n    #     \"dense\": {\n    #         \"units\": [128, 64],\n    #         \"activation\": \"relu\",\n    #         \"dropout\": True,\n    #         \"dropout_rate\": 0.3,\n    #         \"batch_norm\": True,\n    #         \"layer_norm\": False,\n    #     },\n    # }),\n#     # TRIAL TWO\n#     \"cnn\": OmegaConf.create({\n#     \"model\": {\"name\": \"cnn\", \"type\": \"cnn\"},\n#     \"layers\": [\"cnn\", \"dense\"],\n#     \"input_layer_norm\": False,\n#     \"cnn\": {\n#         \"filters\": [80, 160],            \n#         # \"filters\": [80, 160, 240],            \n#         \"kernel_sizes\": [7, 5],\n#         # \"kernel_sizes\": [7, 5, 3],\n#         \"activation\": \"relu\",\n#         \"dropout\": True,\n#         \"dropout_rate\": 0.2,\n#         \"batch_norm\": True,\n#         \"layer_norm\": False,\n#     },\n#     \"dense\": {\n#         # \"units\": [160, 100, 80],\n#         \"units\": [160, 80],\n#         \"activation\": \"relu\",\n#         \"dropout\": True,\n#         \"dropout_rate\": 0.2,\n#         \"batch_norm\": True,\n#         \"layer_norm\": True,\n#     },\n# }),\n# ORIGINAL\n    \"cnn\": OmegaConf.create({\n        \"model\": {\"name\": \"cnn\", \"type\": \"cnn\"},\n        \"layers\": [\"cnn\", \"dense\"],\n        \"input_layer_norm\": False,\n        \"cnn\": {\n            # \"filters\": [64, 128, 240],\n            \"filters\": [80, 80],\n            \"kernel_sizes\": [7, 7],\n            # \"kernel_sizes\": [7, 5,3],\n            # \"activation\": \"leaky_relu\",\n            \"activation\": \"relu\",\n            # \"activation\": \"gelu\",\n            \"dropout\": True,\n            \"dropout_rate\": 0.2,\n            \"batch_norm\": True,\n            \"layer_norm\": False,\n        },\n        \"dense\": {\n            \"units\": [200, 100, 80],\n            # \"units\": [240, 160, 80],\n            # \"units\": [160, 80],\n            # \"units\": [100],\n            # \"activation\": \"leaky_relu\",\n            \"activation\": \"relu\",\n            # \"activation\": \"gelu\",\n            \"dropout\": True,\n            \"dropout_rate\": 0.2,\n            \"batch_norm\": True,\n            \"layer_norm\": False,\n        },\n    }),\n    \"mlp\": OmegaConf.create({\n        \"model\": {\"name\": \"mlp\", \"type\": \"mlp\"},\n        \"layers\": [\"dense\"],\n        \"input_layer_norm\": False,\n        \"dense\": {\n            # \"units\": [200,240, 160, 80],\n            \"units\": [200, 100, 80],\n            \"activation\": \"relu\",\n            # \"activation\": \"leaky_relu\",\n            \"dropout\": True,\n            \"dropout_rate\": 0.25,\n            \"batch_norm\": True,\n            \"layer_norm\": False,\n        },\n    }),\n    \"gru\": OmegaConf.create({\n        # model\": {\"name\": \"cnn\", \"type\": \"cnn\"},\n        # \"layers\": [\"cnn\", \"dense\"],\n        # \"input_layer_norm\": False,\n        # \"cnn\":\n        \"model\": {\"name\": \"gru\", \"type\": \"gru\"},\n        \"layers\": [\"gru\", \"dense\"],\n        \"input_layer_norm\": False,\n        \"gru\": {\n            \"type\": \"gru\",\n            \"hidden_units\": [100, 100, 50],  # Three GRU layers\n            \"dropout\": [0.2, 0.1, 0.1],      # Dropout after each GRU layer\n            \"bidirectional\": False,\n            \"return_sequences\": [True, True, False],  # Only last layer returns final output\n        },\n        \"dense\": {\n            \"units\": [240,160,80],\n            \"activation\": \"relu\",\n        }\n    }),\n    \"cnn_lstm\": OmegaConf.create({\n        \"model\": {\"name\": \"cnn_lstm\", \"type\": \"cnn_lstm\"},\n        \"layers\": [\"cnn\", \"lstm\", \"dense\"],\n        \"input_layer_norm\": False,\n        \"cnn\": {\n            \"filters\": [80, 80],\n            \"kernel_sizes\": [3, 3],\n            # \"activation\": \"leaky_relu\",\n            \"activation\": \"relu\",\n            \"dropout\": True,\n            \"dropout_rate\": 0.3,\n            \"batch_norm\": True,\n            \"layer_norm\": False,\n        },\n        \"lstm\": {\n            \"hidden_size\": [80],\n            \"activation\": \"relu\",\n            # \"activation\": \"leaky_relu\",\n            \"dropout\": True,\n            \"dropout_rate\": 0.3,\n            \"batch_norm\": True,\n            \"layer_norm\": False,\n        },\n        \"dense\": {\n            \"units\": [200, 200, 80],\n            \"activation\": \"relu\",\n            # \"activation\": \"leaky_relu\",\n            \"dropout\": True,\n            \"dropout_rate\": 0.3,\n            \"batch_norm\": True,\n            \"layer_norm\": False,\n        },\n    }),\n     \"cnn_gru\": OmegaConf.create({\n        \"model\": {\"name\": \"cnn_gru\", \"type\": \"cnn_gru\"},\n        \"layers\": [\"cnn\", \"gru\", \"dense\"],\n        \"input_layer_norm\": False,\n        \"cnn\": {\n            \"filters\": [80, 80],\n            \"kernel_sizes\": [3, 3],\n            # \"activation\": \"leaky_relu\",\n            \"activation\": \"relu\",\n            \"dropout\": True,\n            \"dropout_rate\": 0.3,\n            \"batch_norm\": True,\n            \"layer_norm\": False,\n        },\n        \"gru\": {\n            \"hidden_size\": [80],\n            \"activation\": \"relu\",\n            # \"activation\": \"leaky_relu\",\n            \"dropout\": True,\n            \"dropout_rate\": 0.3,\n            \"batch_norm\": True,\n            \"layer_norm\": False,\n        },\n        \"dense\": {\n            \"units\": [200, 200, 80],\n            \"activation\": \"relu\",\n            # \"activation\": \"leaky_relu\",\n            \"dropout\": True,\n            \"dropout_rate\": 0.3,\n            \"batch_norm\": True,\n            \"layer_norm\": False,\n        },\n    })\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:21:45.172755Z","iopub.execute_input":"2025-10-01T18:21:45.173227Z","iopub.status.idle":"2025-10-01T18:21:45.204035Z","shell.execute_reply.started":"2025-10-01T18:21:45.173203Z","shell.execute_reply":"2025-10-01T18:21:45.203069Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"import networkx as nx\nimport igraph as ig\n# change 1.02\n# import /kaggle/input/centrality_network/pytorch/default/1 as network\nimport sys\nsys.path.append(\"/kaggle/input/centrality_network/pytorch/default/1\")\n\nfrom network_features import separate_graph, cal_betweenness_centrality, cal_k_core, cal_k_truss\nfrom CommCentralityCode import comm_centreality\nfrom modularity_vitality import modularity_vitality\n\nsys.path.append(\"/kaggle/input/githubrepofiles/pytorch/default/1\")\nfrom src.dataset.dataset_info import datasets\n\n# change 1.04\ndef add_centralities(df, new_path, graph_path, dataset, cn_measures, network_features):\n# def add_centralities(df, new_path, graph_path, dataset, cn_measures):\n        # change 1.05\n    # G = nx.from_pandas_edgelist(df, source=\"Src IP\",target=\"Dst IP\", create_using=nx.DiGraph())\n    G = nx.from_pandas_edgelist( df, source=dataset.src_ip_col, target=dataset.dst_ip_col, create_using=nx.DiGraph())\n    G.remove_nodes_from(list(nx.isolates(G)))\n    for node in G.nodes():\n        G.nodes[node]['label'] = node\n\n    G1 = ig.Graph.from_networkx(G)\n    labels = [G.nodes[node]['label'] for node in G.nodes()]\n    G1.vs['label'] = labels\n\n    part = G1.community_infomap()\n    communities = []\n    for com in part:\n        communities.append([G1.vs[node_index]['label'] for node_index in com])\n\n    community_labels = {}\n    for i, community in enumerate(communities):\n        for node in community:\n            community_labels[node] = i\n\n    nx.set_node_attributes(G, community_labels, \"new_community\")\n\n    intra_graph, inter_graph = separate_graph(G, communities)\n# [\"betweenness\", \"degree\", \"pagerank\", \"closeness\", \"k_truss\"]\n    if \"betweenness\" in cn_measures:\n        nx.set_node_attributes(G, cal_betweenness_centrality(G), \"betweenness\")\n        print(\"calculated betweenness\")\n    if \"local_betweenness\" in cn_measures:\n        nx.set_node_attributes(G, cal_betweenness_centrality(\n            intra_graph), \"local_betweenness\")\n        print(\"calculated local_betweenness\")\n    if \"global_betweenness\" in cn_measures:\n        nx.set_node_attributes(G, cal_betweenness_centrality(\n            inter_graph), \"global_betweenness\")\n        print(\"calculated global_betweenness\")\n    if \"degree\" in cn_measures:\n        nx.set_node_attributes(G, nx.degree_centrality(G), \"degree\")\n        print(\"calculated degree\")\n    if \"local_degree\" in cn_measures:\n        nx.set_node_attributes(\n            G, nx.degree_centrality(intra_graph), \"local_degree\")\n        print(\"calculated local_degree\")\n    if \"global_degree\" in cn_measures:\n        nx.set_node_attributes(G, nx.degree_centrality(\n            inter_graph), \"global_degree\")\n        print(\"calculated global_degree\")\n    if \"eigenvector\" in cn_measures:\n        nx.set_node_attributes(G, nx.eigenvector_centrality(\n            G, max_iter=600), \"eigenvector\")\n        print(\"calculated eigenvector\")\n    if \"local_eigenvector\" in cn_measures:\n        nx.set_node_attributes(G, nx.eigenvector_centrality(\n            intra_graph), \"local_eigenvector\")\n        print(\"calculated local_eigenvector\")\n    if \"global_eigenvector\" in cn_measures:\n        nx.set_node_attributes(G, nx.eigenvector_centrality(\n            inter_graph), \"global_eigenvector\")\n        print(\"calculated global_eigenvector\")\n    if \"closeness\" in cn_measures:\n        nx.set_node_attributes(G, nx.closeness_centrality(G), \"closeness\")\n        print(\"calculated closeness\")\n    if \"local_closeness\" in cn_measures:\n        nx.set_node_attributes(G, nx.closeness_centrality(\n            intra_graph), \"local_closeness\")\n        print(\"calculated local_closeness\")\n    if \"global_closeness\" in cn_measures:\n        nx.set_node_attributes(G, nx.closeness_centrality(\n            inter_graph), \"global_closeness\")\n        print(\"calculated global_closeness\")\n    if \"pagerank\" in cn_measures:\n        nx.set_node_attributes(G, nx.pagerank(G, alpha=0.85), \"pagerank\")\n        print(\"calculated pagerank\")\n    if \"local_pagerank\" in cn_measures:\n        nx.set_node_attributes(G, nx.pagerank(\n            intra_graph, alpha=0.85), \"local_pagerank\")\n        print(\"calculated local_pagerank\")\n    if \"global_pagerank\" in cn_measures:\n        nx.set_node_attributes(G, nx.pagerank(\n            inter_graph, alpha=0.85), \"global_pagerank\")\n        print(\"calculated global_pagerank\")\n    if \"k_core\" in cn_measures:\n        nx.set_node_attributes(G, cal_k_core(G), \"k_core\")\n        print(\"calculated k_core\")\n    if \"k_truss\" in cn_measures:\n        nx.set_node_attributes(G, cal_k_truss(G), \"k_truss\")\n        print(\"calculated k_truss\")\n    if \"Comm\" in cn_measures:\n        nx.set_node_attributes(\n            G, comm_centreality(G, community_labels), \"Comm\")\n        print(\"calculated Comm\")\n    if \"mv\" in cn_measures:\n        nx.set_node_attributes(G, modularity_vitality(G1, part), \"mv\")\n        print(\"calculated mv\")\n\n    nx.write_gexf(G, graph_path)\n\n    features_dicts = {}\n    for measure in cn_measures:\n        features_dicts[measure] = nx.get_node_attributes(G, measure)\n        print(f\"==>> features_dicts: {measure , len(features_dicts[measure])}\")\n\n    for feature in network_features:\n        if feature[:3] == \"src\":\n            df[feature] = df.apply(lambda row: features_dicts[feature[4:]].get(\n                row[dataset.src_ip_col], -1), axis=1)\n            # df[feature] = df.apply(lambda row: features_dicts[feature[4:]].get(\n                # row['Src Ip'], -1), axis=1)\n        if feature[:3] == \"dst\":\n            df[feature] = df.apply(lambda row: features_dicts[feature[4:]].get(\n                row[dataset.dst_ip_col], -1), axis=1)\n            # df[feature] = df.apply(lambda row: features_dicts[feature[4:]].get(row['Dst IP'], -1), axis=1)\n    df.to_parquet(new_path)\n    print(f\"DataFrame written to {new_path}\")\n    # print(df.columns)\n    # return network_features\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:21:51.068854Z","iopub.execute_input":"2025-10-01T18:21:51.069161Z","iopub.status.idle":"2025-10-01T18:21:51.569358Z","shell.execute_reply.started":"2025-10-01T18:21:51.069140Z","shell.execute_reply":"2025-10-01T18:21:51.568810Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATASET = datasets[name]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:21:56.297872Z","iopub.execute_input":"2025-10-01T18:21:56.298700Z","iopub.status.idle":"2025-10-01T18:21:56.302067Z","shell.execute_reply.started":"2025-10-01T18:21:56.298675Z","shell.execute_reply":"2025-10-01T18:21:56.301513Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(DATASET)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:21:56.578194Z","iopub.execute_input":"2025-10-01T18:21:56.578865Z","iopub.status.idle":"2025-10-01T18:21:56.582454Z","shell.execute_reply.started":"2025-10-01T18:21:56.578843Z","shell.execute_reply":"2025-10-01T18:21:56.581922Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(DATASET.src_ip_col)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:21:57.200497Z","iopub.execute_input":"2025-10-01T18:21:57.200757Z","iopub.status.idle":"2025-10-01T18:21:57.204506Z","shell.execute_reply.started":"2025-10-01T18:21:57.200741Z","shell.execute_reply":"2025-10-01T18:21:57.203796Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# utils\n\nACTIVATIONS = {\n    \"relu\":      nn.ReLU,\n    \"leaky_relu\": nn.LeakyReLU,\n    \"gelu\":      nn.GELU,\n    \"tanh\":      nn.Tanh,\n    \"sigmoid\": nn.Sigmoid,\n    \"none\":      None,\n}\n\nclass NumpyEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        elif isinstance(obj, np.floating):\n            return float(obj)\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        return super(NumpyEncoder, self).default(obj)\n\n\ndef plot_confusion_matrix(cm,\n                          target_names,\n                          title='Confusion matrix',\n                          cmap=None,\n                          normalized=False,\n                          file_path=None,\n                          show_figure=True):\n\n    accuracy = np.trace(cm) / float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    fig = plt.figure(figsize=(12, 12))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    thresh = cm.max() / 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalized:\n            plt.text(j, i, \"{:0.3f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(\n        accuracy, misclass))\n    if file_path:\n        plt.savefig(file_path)\n    if show_figure:\n        plt.show()\n    return fig\n\n\ndef calculate_fpr_fnr_with_global(cm):\n    \"\"\"\n    Calculate FPR and FNR for each class and globally for a multi-class confusion matrix.\n\n    Parameters:\n        cm (numpy.ndarray): Confusion matrix of shape (num_classes, num_classes).\n\n    Returns:\n        dict: A dictionary containing per-class and global FPR and FNR.\n    \"\"\"\n    num_classes = cm.shape[0]\n    results = {\"per_class\": {}, \"global\": {}}\n\n    # Initialize variables for global calculation\n    total_TP = 0\n    total_FP = 0\n    total_FN = 0\n    total_TN = 0\n\n    # Per-class calculation\n    for class_idx in range(num_classes):\n        TP = cm[class_idx, class_idx]\n        FN = np.sum(cm[class_idx, :]) - TP\n        FP = np.sum(cm[:, class_idx]) - TP\n        TN = np.sum(cm) - (TP + FP + FN)\n\n        # Calculate FPR and FNR for this class\n        FPR = FP / (FP + TN) if (FP + TN) != 0 else None\n        FNR = FN / (TP + FN) if (TP + FN) != 0 else None\n\n        # Store per-class results\n        results[\"per_class\"][class_idx] = {\"FPR\": FPR, \"FNR\": FNR}\n\n        # Update global counts\n        total_TP += TP\n        total_FP += FP\n        total_FN += FN\n        total_TN += TN\n\n    # Global calculation\n    global_FPR = total_FP / \\\n        (total_FP + total_TN) if (total_FP + total_TN) != 0 else None\n    global_FNR = total_FN / \\\n        (total_FN + total_TP) if (total_FN + total_TP) != 0 else None\n\n    results[\"global\"][\"FPR\"] = global_FPR\n    results[\"global\"][\"FNR\"] = global_FNR\n\n    return results\n\n\ndef compute_class_weights(targets: pd.Series,\n                          classes: np.ndarray,\n                          version: str = 'v4',\n                          device: torch.device = torch.device('cpu')) -> torch.Tensor:\n    \"\"\"\n    Compute class weights for CrossEntropyLoss, selectable by `version`.\n\n    Parameters\n    ----------\n    targets : pd.Series\n        Series of integer class labels.\n    classes : ndarray of shape (num_classes,)\n        Array of all possible class labels (e.g. np.arange(num_classes)).\n    version : str, one of {'v1', 'v2', 'v3', 'v4'}\n        Which weighting strategy to use:\n          - v1: total/(num_classes*count)  (naïve “balanced”)\n          - v2: normalize(1/(count + eps))\n          - v3: raw inverse (1/count)\n          - v4: sklearn compute_class_weight('balanced')\n    device : torch.device\n        Where to put the resulting tensor.\n\n    Returns\n    -------\n    torch.Tensor of shape (num_classes,)\n        Float tensor of weights.\n    \"\"\"\n    # get counts per class label\n    counts = targets.value_counts().to_dict()\n    num_classes = len(classes)\n    counts_arr = np.zeros(num_classes, dtype=float)\n    for lbl, cnt in counts.items():\n        counts_arr[int(lbl)] = cnt\n\n    if version == 'v1':\n        # v1: total samples divided equally across classes\n        total = counts_arr.sum()\n        weights_arr = np.zeros_like(counts_arr)\n        mask = counts_arr > 0\n        weights_arr[mask] = total / (num_classes * counts_arr[mask])\n        weight_tensor = torch.tensor(\n            weights_arr, dtype=torch.float, device=device)\n\n    elif version == 'v2':\n        # v2: normalized inverse-frequency with epsilon\n        class_counts = torch.tensor(\n            counts_arr, dtype=torch.float, device=device)\n        weight_tensor = 1.0 / (class_counts + 1e-6)\n        weight_tensor = weight_tensor / weight_tensor.sum()\n\n    elif version == 'v3':\n        # v3: raw inverse-frequency\n        counts_tensor = torch.tensor(\n            counts_arr, dtype=torch.float, device=device)\n        # avoid division by zero\n        inv = torch.zeros_like(counts_tensor)\n        mask = counts_tensor > 0\n        inv[mask] = 1.0 / counts_tensor[mask]\n        weight_tensor = inv\n\n    elif version == 'v4':\n        # v4: sklearn compute_class_weight\n        present = np.array(list(counts.keys()), dtype=int)\n        w_present = sklearn_class_weight.compute_class_weight(\n            class_weight='balanced', classes=present, y=targets.values\n        )\n        weights_arr = np.zeros_like(counts_arr)\n        for cls, w in zip(present, w_present):\n            weights_arr[int(cls)] = w\n        weight_tensor = torch.tensor(\n            weights_arr, dtype=torch.float, device=device)\n        \n    elif version == 'v5':\n        class_counts = torch.tensor(\n            counts_arr, dtype=torch.float, device=device)\n        weight_tensor = class_counts.sum() / (num_classes * class_counts + 1e-6)\n        \n    else:\n        raise ValueError(\n            f\"Unknown version '{version}', choose one of {{'v1','v2','v3','v4'}}\")\n\n    return weight_tensor\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:21:59.380447Z","iopub.execute_input":"2025-10-01T18:21:59.380968Z","iopub.status.idle":"2025-10-01T18:21:59.397917Z","shell.execute_reply.started":"2025-10-01T18:21:59.380945Z","shell.execute_reply":"2025-10-01T18:21:59.397274Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# normalization_layers\n\nclass ChannelLayerNorm(nn.Module):\n    \"\"\"LayerNorm over the CHANNEL dimension of a 1D conv output.\"\"\"\n\n    def __init__(self, num_channels, eps: float = 1e-5, elementwise_affine: bool = True):\n        super().__init__()\n        # This LN will normalize the last dim (channels after we transpose)\n        self.ln = nn.LayerNorm(num_channels, eps=eps,\n                               elementwise_affine=elementwise_affine)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # x: (B, C, L) → (B, L, C)\n        x = x.transpose(1, 2)\n        x = self.ln(x)            # norm across C\n        return x.transpose(1, 2)  # back to (B, C, L)\n\n\nclass SequenceNorm1d(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        use_batch_norm: bool = False,\n        use_layer_norm: bool = False,\n        **bn_kwargs,\n    ):\n        super().__init__()\n        if use_batch_norm:\n            # BN over the feature dimension: expect (B, D, L)\n            self.norm = nn.BatchNorm1d(dim, **bn_kwargs)\n            self.is_bn = True\n        elif use_layer_norm:\n            # LN over the feature dimension: expect (B, L, D)\n            self.norm = nn.LayerNorm(dim)\n            self.is_bn = False\n        else:\n            self.norm = None\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # x: (B, L, D)\n        if self.norm is None:\n            return x\n        if self.is_bn:\n            # shuffle so BN sees (B, D, L)\n            x = x.transpose(1, 2)\n            x = self.norm(x)\n            return x.transpose(1, 2)\n        else:\n            # LN works in‐place on last dim\n            return self.norm(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:22:02.484451Z","iopub.execute_input":"2025-10-01T18:22:02.484744Z","iopub.status.idle":"2025-10-01T18:22:02.492004Z","shell.execute_reply.started":"2025-10-01T18:22:02.484725Z","shell.execute_reply":"2025-10-01T18:22:02.491411Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # CNN_LSTM model\n# class CNNLSTM(nn.Module):\n#     def __init__(self, model_cfg, num_features, num_classes):\n#         super().__init__()\n\n#         self.model_cfg = model_cfg\n\n#         self.cnn_activation = ACTIVATIONS[model_cfg.cnn.activation]\n#         self.lstm_activation = ACTIVATIONS[model_cfg.lstm.activation]\n#         self.dense_activation = ACTIVATIONS[model_cfg.dense.activation]\n\n#         conv_layers = []\n\n#         in_channels = 1\n#         for i, filter in enumerate(model_cfg.cnn.filters):\n#             cnn = nn.Conv1d(\n#                 in_channels=in_channels,\n#                 out_channels=filter,\n#                 kernel_size=model_cfg.cnn.kernel_sizes[i],\n#                 padding=1\n#             )\n#             conv_layers.append(cnn)\n#             conv_layers.append(self.cnn_activation())\n#             if model_cfg.cnn.batch_norm:\n#                 conv_layers.append(nn.BatchNorm1d(filter))\n#             if model_cfg.cnn.layer_norm:\n#                 conv_layers.append(ChannelLayerNorm(filter))\n#             if model_cfg.cnn.dropout:\n#                 conv_layers.append(nn.Dropout(model_cfg.cnn.dropout_rate))\n#             in_channels = filter\n\n#         self.features = nn.Sequential(*conv_layers)\n\n#         self.lstm_layers = nn.ModuleList()\n#         self.lstm_activations = nn.ModuleList()\n#         self.lstm_normalization = nn.ModuleList()\n#         lstm_input_size = filter\n\n#         for hidden_dim in model_cfg.lstm.hidden_size:\n#             self.lstm_layers.append(\n#                 nn.LSTM(\n#                     input_size=lstm_input_size,\n#                     hidden_size=hidden_dim,\n#                     num_layers=1,\n#                     batch_first=True,\n#                     dropout=model_cfg.lstm.dropout_rate if model_cfg.lstm.dropout else 0,\n#                     bidirectional=False\n#                 )\n#             )\n#             if self.lstm_activation:\n#                 self.lstm_activations.append(self.lstm_activation())\n\n#             self.lstm_normalization.append(\n#                 SequenceNorm1d(\n#                     dim=hidden_dim,\n#                     use_batch_norm=model_cfg.lstm.batch_norm,\n#                     use_layer_norm=model_cfg.lstm.layer_norm,\n#                     # e.g. momentum=0.1, eps=1e-5 if you want custom BN args\n#                 )\n#             )\n\n#             lstm_input_size = hidden_dim\n\n#         # after LSTM, we'll take the last hidden‐state, so our\n#         # `input_dim` for the dense layers is just the last hidden_dim\n#         input_dim = model_cfg.lstm.hidden_size[-1]\n\n#         fc_layers = []\n#         for hidden_dim in model_cfg.dense.units:\n#             fc_layers .append(nn.Linear(input_dim, hidden_dim))\n#             fc_layers .append(self.dense_activation())\n#             if model_cfg.dense.batch_norm:\n#                 fc_layers .append(nn.BatchNorm1d(hidden_dim))\n#             if model_cfg.dense.layer_norm:\n#                 fc_layers.append(nn.LayerNorm(hidden_dim))\n#             if model_cfg.dense.dropout:\n#                 fc_layers .append(nn.Dropout(model_cfg.dense.dropout_rate))\n#             input_dim = hidden_dim\n\n#         fc_layers .append(nn.Linear(input_dim, num_classes))\n\n#         self.classifier = nn.Sequential(*fc_layers)\n\n#     def forward(self, x):\n#         # x = x.view(x.size(0), 1, x.size(-1))\n#         # → [batch, L_final, C_last]\n#         x = self.features(x)\n#         x = x.permute(0, 2, 1)\n\n#         # pass through each LSTM layer\n#         for i, lstm in enumerate(self.lstm_layers):\n#             x, _ = lstm(x)                    # x: [batch, L, hidden_dim_i]\n\n#             if i < len(self.lstm_activations):\n#                 x = self.lstm_activations[i](x)\n\n#             if len(self.lstm_normalization) > i:\n#                 x = self.lstm_normalization[i](x)\n\n#         # grab last time step\n#         x = x[:, -1, :]                       # → [batch, hidden_dim_last]\n#         return self.classifier(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:22:03.732299Z","iopub.execute_input":"2025-10-01T18:22:03.732647Z","iopub.status.idle":"2025-10-01T18:22:03.737659Z","shell.execute_reply.started":"2025-10-01T18:22:03.732628Z","shell.execute_reply":"2025-10-01T18:22:03.736885Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CNNGRU(nn.Module):\n    def __init__(self, model_cfg, num_features, num_classes):\n        super().__init__()\n\n        self.model_cfg = model_cfg\n\n        self.cnn_activation = ACTIVATIONS[model_cfg.cnn.activation]\n        self.gru_activation = ACTIVATIONS[model_cfg.gru.activation]\n        self.dense_activation = ACTIVATIONS[model_cfg.dense.activation]\n\n        # Build CNN feature extractor\n        conv_layers = []\n        in_channels = 1\n        for i, filter in enumerate(model_cfg.cnn.filters):\n            conv_layers.append(nn.Conv1d(\n                in_channels=in_channels,\n                out_channels=filter,\n                kernel_size=model_cfg.cnn.kernel_sizes[i],\n                padding=1\n            ))\n            conv_layers.append(self.cnn_activation())\n            if model_cfg.cnn.batch_norm:\n                conv_layers.append(nn.BatchNorm1d(filter))\n            if model_cfg.cnn.layer_norm:\n                conv_layers.append(ChannelLayerNorm(filter))\n            if model_cfg.cnn.dropout:\n                conv_layers.append(nn.Dropout(model_cfg.cnn.dropout_rate))\n            in_channels = filter\n\n        self.features = nn.Sequential(*conv_layers)\n\n        # Build GRU layers\n        self.gru_layers = nn.ModuleList()\n        self.gru_activations = nn.ModuleList()\n        self.gru_normalization = nn.ModuleList()\n        gru_input_size = filter\n\n        for hidden_dim in model_cfg.gru.hidden_size:\n            self.gru_layers.append(\n                nn.GRU(\n                    input_size=gru_input_size,\n                    hidden_size=hidden_dim,\n                    num_layers=1,\n                    batch_first=True,\n                    dropout=model_cfg.gru.dropout_rate if model_cfg.gru.dropout else 0,\n                    bidirectional=False\n                )\n            )\n            if self.gru_activation:\n                self.gru_activations.append(self.gru_activation())\n\n            self.gru_normalization.append(\n                SequenceNorm1d(\n                    dim=hidden_dim,\n                    use_batch_norm=model_cfg.gru.batch_norm,\n                    use_layer_norm=model_cfg.gru.layer_norm,\n                )\n            )\n            gru_input_size = hidden_dim\n\n        # Build classifier\n        input_dim = model_cfg.gru.hidden_size[-1]\n        fc_layers = []\n        for hidden_dim in model_cfg.dense.units:\n            fc_layers.append(nn.Linear(input_dim, hidden_dim))\n            fc_layers.append(self.dense_activation())\n            if model_cfg.dense.batch_norm:\n                fc_layers.append(nn.BatchNorm1d(hidden_dim))\n            if model_cfg.dense.layer_norm:\n                fc_layers.append(nn.LayerNorm(hidden_dim))\n            if model_cfg.dense.dropout:\n                fc_layers.append(nn.Dropout(model_cfg.dense.dropout_rate))\n            input_dim = hidden_dim\n\n        fc_layers.append(nn.Linear(input_dim, num_classes))\n        self.classifier = nn.Sequential(*fc_layers)\n\n    def forward(self, x):\n        # CNN expects input as [batch_size, channels, time_steps]\n        x = self.features(x)\n        x = x.permute(0, 2, 1)  # [batch, sequence_len, channels] for GRU\n\n        for i, gru in enumerate(self.gru_layers):\n            x, _ = gru(x)\n\n            if i < len(self.gru_activations):\n                x = self.gru_activations[i](x)\n\n            if len(self.gru_normalization) > i:\n                x = self.gru_normalization[i](x)\n\n        x = x[:, -1, :]  # last time step\n        return self.classifier(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:22:05.867604Z","iopub.execute_input":"2025-10-01T18:22:05.867864Z","iopub.status.idle":"2025-10-01T18:22:05.878323Z","shell.execute_reply.started":"2025-10-01T18:22:05.867847Z","shell.execute_reply":"2025-10-01T18:22:05.877566Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CNN_LSTM model\nclass CNNLSTM(nn.Module):\n    def __init__(self, model_cfg, num_features, num_classes):\n        super().__init__()\n\n        self.model_cfg = model_cfg\n\n        self.cnn_activation = ACTIVATIONS[model_cfg.cnn.activation]\n        self.lstm_activation = ACTIVATIONS[model_cfg.lstm.activation]\n        self.dense_activation = ACTIVATIONS[model_cfg.dense.activation]\n\n        conv_layers = []\n\n        in_channels = 1\n        # first Block\n        for i, filter in enumerate(model_cfg.cnn.filters):\n            cnn = nn.Conv1d(\n                in_channels=in_channels,\n                out_channels=filter,\n                kernel_size=model_cfg.cnn.kernel_sizes[i],\n                padding=1\n            )\n            conv_layers.append(cnn)\n            conv_layers.append(self.cnn_activation())\n            if model_cfg.cnn.batch_norm:\n                conv_layers.append(nn.BatchNorm1d(filter))\n            if model_cfg.cnn.layer_norm:\n                conv_layers.append(ChannelLayerNorm(filter))\n            if model_cfg.cnn.dropout:\n                conv_layers.append(nn.Dropout(model_cfg.cnn.dropout_rate))\n            in_channels = filter\n        conv_layers.append(nn.MaxPool1d(kernel_size=2))\n        # Second Block\n        for i, filter in enumerate(model_cfg.cnn.filters):\n            cnn = nn.Conv1d(\n                in_channels=in_channels,\n                out_channels=filter,\n                kernel_size=model_cfg.cnn.kernel_sizes[i],\n                padding=1\n            )\n            conv_layers.append(cnn)\n            conv_layers.append(self.cnn_activation())\n            if model_cfg.cnn.batch_norm:\n                conv_layers.append(nn.BatchNorm1d(filter))\n            if model_cfg.cnn.layer_norm:\n                conv_layers.append(ChannelLayerNorm(filter))\n            if model_cfg.cnn.dropout:\n                conv_layers.append(nn.Dropout(model_cfg.cnn.dropout_rate))\n            in_channels = filter\n        conv_layers.append(nn.AvgPool1d(kernel_size=2))\n        \n        self.features = nn.Sequential(*conv_layers)\n\n        self.lstm_layers = nn.ModuleList()\n        self.lstm_activations = nn.ModuleList()\n        self.lstm_normalization = nn.ModuleList()\n        lstm_input_size = filter\n\n        for hidden_dim in model_cfg.lstm.hidden_size:\n            self.lstm_layers.append(\n                nn.LSTM(\n                    input_size=lstm_input_size,\n                    hidden_size=hidden_dim,\n                    num_layers=1,\n                    batch_first=True,\n                    dropout=model_cfg.lstm.dropout_rate if model_cfg.lstm.dropout else 0,\n                    bidirectional=False\n                )\n            )\n            if self.lstm_activation:\n                self.lstm_activations.append(self.lstm_activation())\n\n            self.lstm_normalization.append(\n                SequenceNorm1d(\n                    dim=hidden_dim,\n                    use_batch_norm=model_cfg.lstm.batch_norm,\n                    use_layer_norm=model_cfg.lstm.layer_norm,\n                    # e.g. momentum=0.1, eps=1e-5 if you want custom BN args\n                )\n            )\n\n            lstm_input_size = hidden_dim\n\n        # after LSTM, we'll take the last hidden‐state, so our\n        # `input_dim` for the dense layers is just the last hidden_dim\n        input_dim = model_cfg.lstm.hidden_size[-1]\n\n        fc_layers = []\n        for hidden_dim in model_cfg.dense.units:\n            fc_layers .append(nn.Linear(input_dim, hidden_dim))\n            fc_layers .append(self.dense_activation())\n            if model_cfg.dense.batch_norm:\n                fc_layers .append(nn.BatchNorm1d(hidden_dim))\n            if model_cfg.dense.layer_norm:\n                fc_layers.append(nn.LayerNorm(hidden_dim))\n            if model_cfg.dense.dropout:\n                fc_layers .append(nn.Dropout(model_cfg.dense.dropout_rate))\n            input_dim = hidden_dim\n\n        fc_layers .append(nn.Linear(input_dim, num_classes))\n\n        self.classifier = nn.Sequential(*fc_layers)\n\n    def forward(self, x):\n        # x = x.view(x.size(0), 1, x.size(-1))\n        # → [batch, L_final, C_last]\n        x = self.features(x)\n        x = x.permute(0, 2, 1)\n\n        # pass through each LSTM layer\n        for i, lstm in enumerate(self.lstm_layers):\n            x, _ = lstm(x)                    # x: [batch, L, hidden_dim_i]\n\n            if i < len(self.lstm_activations):\n                x = self.lstm_activations[i](x)\n\n            if len(self.lstm_normalization) > i:\n                x = self.lstm_normalization[i](x)\n\n        # grab last time step\n        x = x[:, -1, :]                       # → [batch, hidden_dim_last]\n        return self.classifier(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:22:06.160706Z","iopub.execute_input":"2025-10-01T18:22:06.160954Z","iopub.status.idle":"2025-10-01T18:22:06.173106Z","shell.execute_reply.started":"2025-10-01T18:22:06.160937Z","shell.execute_reply":"2025-10-01T18:22:06.172539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # CNN model\n# class CNN(nn.Module):\n#     def __init__(self, model_cfg, num_features, num_classes):\n#         super().__init__()\n\n#         if model_cfg.input_layer_norm:\n#             self.input_norm = nn.LayerNorm(num_features)\n#         else:\n#             self.input_norm = nn.Identity()\n\n#         self.cnn_activation = ACTIVATIONS[model_cfg.cnn.activation]\n#         self.dense_activation = ACTIVATIONS[model_cfg.dense.activation]\n\n#         conv_layers = []\n\n#         in_channels = 1\n#         for i, filter in enumerate(model_cfg.cnn.filters):\n#             k = model_cfg.cnn.kernel_sizes[i]\n#             cnn = nn.Conv1d(\n#                 in_channels=in_channels,\n#                 out_channels=filter,\n#                 kernel_size=k,\n#                 padding=(k - 1) // 2\n#             )\n#             conv_layers.append(cnn)\n#             conv_layers.append(self.cnn_activation())\n#             if model_cfg.cnn.batch_norm:\n#                 conv_layers.append(nn.BatchNorm1d(filter))\n#             if model_cfg.cnn.layer_norm:\n#                 conv_layers.append(ChannelLayerNorm(filter))\n#             if model_cfg.cnn.dropout:\n#                 conv_layers.append(nn.Dropout(model_cfg.cnn.dropout_rate))\n#             in_channels = filter\n\n#         self.features = nn.Sequential(*conv_layers)\n#         with torch.no_grad():\n#             # make a dummy of shape (1, channels, length)\n#             dummy = torch.zeros(1, 1, num_features)\n#             feat = self.features(dummy)\n#             # feat.shape -> [1, C_last, L_final]\n#             input_dim = feat.size(1) * feat.size(2)\n\n#         # print(f\"==>> input_dim: {input_dim}\")\n\n#         # fc_layers = []\n#         # for hidden_dim in model_cfg.dense.units:\n#         #     fc_layers.append(nn.Linear(input_dim, hidden_dim))\n#         #     fc_layers.append(self.dense_activation())\n#         #     if model_cfg.dense.batch_norm:\n#         #         fc_layers.append(nn.BatchNorm1d(hidden_dim))\n#         #     if model_cfg.dense.layer_norm:\n#         #         fc_layers.append(nn.LayerNorm(hidden_dim))\n#         #     if model_cfg.dense.dropout:\n#         #         fc_layers.append(nn.Dropout(model_cfg.dense.dropout_rate))\n#         #     input_dim = hidden_dim\n\n#         fc_layers = OrderedDict()\n#         idx = 0\n#         for i, hidden_dim in enumerate(model_cfg.dense.units, start=1):\n#             fc_layers[f\"linear{i}\"] = nn.Linear(input_dim, hidden_dim)\n#             idx += 1\n\n#             fc_layers[f\"act{i}\"] = self.dense_activation()\n\n#             if model_cfg.dense.batch_norm:\n#                 fc_layers[f\"bn{i}\"] = nn.BatchNorm1d(hidden_dim)\n#                 idx += 1\n\n#             if model_cfg.dense.layer_norm:\n#                 fc_layers[f\"ln{i}\"] = nn.LayerNorm(hidden_dim)\n#                 idx += 1\n\n#             if model_cfg.dense.dropout:\n#                 fc_layers[f\"drop{i}\"] = nn.Dropout(\n#                     model_cfg.dense.dropout_rate)\n#                 idx += 1\n\n#             input_dim = hidden_dim\n#         # fc_layers .append(nn.Linear(input_dim, num_classes))\n#         fc_layers[\"out\"] = nn.Linear(input_dim, num_classes)\n\n#         self.classifier = nn.Sequential(fc_layers)\n#         # self.classifier = nn.Sequential(*fc_layers)\n\n#     def forward(self, x):\n#         x = self.input_norm(x)\n#         # print(f\"==>> x.shape: {x.shape}\")\n#         # x = x.view(x.size(0), 1, x.size(-1))\n#         # print(f\"==>> x.shape: {x.shape}\")\n#         x = self.features(x)\n#         x = x.flatten(1)\n#         return self.classifier(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:22:07.275538Z","iopub.execute_input":"2025-10-01T18:22:07.275838Z","iopub.status.idle":"2025-10-01T18:22:07.280762Z","shell.execute_reply.started":"2025-10-01T18:22:07.275818Z","shell.execute_reply":"2025-10-01T18:22:07.280135Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass GRUModel(nn.Module):\n    def __init__(self, config, input_size, output_size):\n        super(GRUModel, self).__init__()\n        self.layers = nn.ModuleList()\n        self.dropouts = []\n\n        hidden_units = config.gru.hidden_units\n        dropout_rates = config.gru.dropout\n\n        prev_size = input_size\n        for i in range(len(hidden_units)):\n            gru_layer = nn.GRU(\n                input_size=prev_size,\n                hidden_size=hidden_units[i],\n                batch_first=True\n            )\n            self.layers.append(gru_layer)\n            self.dropouts.append(nn.Dropout(dropout_rates[i]))\n            prev_size = hidden_units[i]\n\n        self.final_dense = nn.Sequential(\n            nn.Linear(prev_size, output_size),\n            nn.Sigmoid()\n        )\n\n    def __getitem__(self, idx):\n        start_idx = idx\n        end_idx = start_idx + self.seq_length\n    \n        if end_idx > len(self.data):\n            raise IndexError(f\"Sequence from {start_idx} to {end_idx} is out of bounds.\")\n    \n        x_seq = self.data[start_idx:end_idx]\n    \n        if (end_idx - 1) >= len(self.targets):\n            raise IndexError(f\"Label index {end_idx - 1} is out of bounds for targets of length {len(self.targets)}.\")\n    \n        y_label = self.targets[end_idx - 1]\n    \n        if self.transform:\n            x_seq = self.transform(x_seq)\n\n        return x_seq, y_label\n\n    def forward(self, x):\n        for i, gru in enumerate(self.layers):\n            x, _ = gru(x)\n            if i < len(self.layers) - 1:\n                # Return full sequence for stacking\n                x = self.dropouts[i](x)\n            else:\n                # Last GRU returns only the last hidden state\n                x = x[:, -1, :]\n                x = self.dropouts[i](x)\n\n        out = self.final_dense(x)\n        return out\n\n\n# class GRUModel(nn.Module):\n#     def __init__(self, config, input_size, num_classes):\n#         super(GRUModel, self).__init__()\n#         self.layers = nn.ModuleList()\n#         self.dropouts = []\n\n#         hidden_units = config.gru.hidden_units\n#         dropout_rates = config.gru.dropout\n#         return_sequences = config.gru.return_sequences  # can be used later if needed\n\n#         prev_size = input_size\n#         for i in range(len(hidden_units)):\n#             gru_layer = nn.GRU(\n#                 input_size=prev_size,\n#                 hidden_size=hidden_units[i],\n#                 batch_first=True\n#             )\n#             self.layers.append(gru_layer)\n#             self.dropouts.append(nn.Dropout(dropout_rates[i]))\n#             prev_size = hidden_units[i]\n\n#         self.final_dense = nn.Sequential(\n#             nn.Linear(prev_size, num_classes),\n#             nn.Sigmoid()\n#         )\n\n#     def forward(self, x):\n#         for i, gru in enumerate(self.layers):\n#             x, _ = gru(x)\n#             if i < len(self.layers) - 1:\n#                 # Apply dropout after intermediate GRU layers\n#                 x = self.dropouts[i](x)\n#             else:\n#                 # Take only the last timestep output for the last GRU layer\n#                 x = x[:, -1, :]\n#                 x = self.dropouts[i](x)\n\n#         out = self.final_dense(x)\n#         return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:22:10.248542Z","iopub.execute_input":"2025-10-01T18:22:10.248850Z","iopub.status.idle":"2025-10-01T18:22:10.256806Z","shell.execute_reply.started":"2025-10-01T18:22:10.248832Z","shell.execute_reply":"2025-10-01T18:22:10.256126Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self, model_cfg, num_features, num_classes):\n        super().__init__()\n\n        if model_cfg.input_layer_norm:\n            self.input_norm = nn.LayerNorm(num_features)\n        else:\n            self.input_norm = nn.Identity()\n\n        self.cnn_activation = ACTIVATIONS[model_cfg.cnn.activation]\n        self.dense_activation = ACTIVATIONS[model_cfg.dense.activation]\n\n        in_channels = 1\n        filters = model_cfg.cnn.filters\n        kernel_sizes = model_cfg.cnn.kernel_sizes\n\n        conv_blocks = []\n\n        # Block 1\n        conv_blocks.append(nn.Conv1d(in_channels, filters[0], kernel_sizes[0], padding=(kernel_sizes[0]-1)//2))\n        conv_blocks.append(self.cnn_activation())\n        if model_cfg.cnn.batch_norm:\n            conv_blocks.append(nn.BatchNorm1d(filters[0]))\n        if model_cfg.cnn.layer_norm:\n            conv_blocks.append(ChannelLayerNorm(filters[0]))\n        if model_cfg.cnn.dropout:\n            conv_blocks.append(nn.Dropout(model_cfg.cnn.dropout_rate))\n        conv_blocks.append(nn.MaxPool1d(kernel_size=2))\n\n        # Block 2\n        conv_blocks.append(nn.Conv1d(filters[0], filters[1], kernel_sizes[1], padding=(kernel_sizes[1]-1)//2))\n        conv_blocks.append(self.cnn_activation())\n        if model_cfg.cnn.batch_norm:\n            conv_blocks.append(nn.BatchNorm1d(filters[1]))\n        if model_cfg.cnn.layer_norm:\n            conv_blocks.append(ChannelLayerNorm(filters[1]))\n        if model_cfg.cnn.dropout:\n            conv_blocks.append(nn.Dropout(model_cfg.cnn.dropout_rate))\n        conv_blocks.append(nn.AvgPool1d(kernel_size=2))\n        # conv_blocks.append(nn.AdaptiveAvgPool1d(1))\n        self.features = nn.Sequential(*conv_blocks)\n\n        with torch.no_grad():\n            dummy = torch.zeros(1, 1, num_features)\n            feat = self.features(dummy)\n            input_dim = feat.size(1) * feat.size(2)\n\n        # Dense Layers\n        fc_layers = OrderedDict()\n        for i, hidden_dim in enumerate(model_cfg.dense.units, start=1):\n            fc_layers[f\"linear{i}\"] = nn.Linear(input_dim, hidden_dim)\n            fc_layers[f\"act{i}\"] = self.dense_activation()\n            if model_cfg.dense.batch_norm:\n                fc_layers[f\"bn{i}\"] = nn.BatchNorm1d(hidden_dim)\n            if model_cfg.dense.layer_norm:\n                fc_layers[f\"ln{i}\"] = nn.LayerNorm(hidden_dim)\n            if model_cfg.dense.dropout:\n                fc_layers[f\"drop{i}\"] = nn.Dropout(model_cfg.dense.dropout_rate)\n            input_dim = hidden_dim\n\n        fc_layers[\"out\"] = nn.Linear(input_dim, num_classes)\n        self.classifier = nn.Sequential(fc_layers)\n\n    def forward(self, x):\n        x = self.input_norm(x)\n        x = self.features(x)\n        x = x.flatten(1)\n        return self.classifier(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:22:10.539802Z","iopub.execute_input":"2025-10-01T18:22:10.540041Z","iopub.status.idle":"2025-10-01T18:22:10.550047Z","shell.execute_reply.started":"2025-10-01T18:22:10.540026Z","shell.execute_reply":"2025-10-01T18:22:10.549327Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# MLP model\nclass MLP(nn.Module):\n    def __init__(self, model_cfg, num_features, num_classes):\n        super().__init__()\n\n        if model_cfg.input_layer_norm:\n            self.input_norm = nn.LayerNorm(num_features)\n        else:\n            self.input_norm = nn.Identity()\n\n        self.dense_activation = ACTIVATIONS[model_cfg.dense.activation]\n\n        input_dim = num_features\n        layers = OrderedDict()\n        idx = 0\n        for i, hidden_dim in enumerate(model_cfg.dense.units, start=1):\n            layers[f\"linear{i}\"] = nn.Linear(input_dim, hidden_dim)\n            idx += 1\n\n            layers[f\"act{i}\"] = self.dense_activation()\n\n            if model_cfg.dense.batch_norm:\n                layers[f\"bn{i}\"] = nn.BatchNorm1d(hidden_dim)\n                idx += 1\n\n            if model_cfg.dense.layer_norm:\n                layers[f\"ln{i}\"] = nn.LayerNorm(hidden_dim)\n                idx += 1\n\n            if model_cfg.dense.dropout:\n                layers[f\"drop{i}\"] = nn.Dropout(\n                    model_cfg.dense.dropout_rate)\n                idx += 1\n\n            input_dim = hidden_dim\n        # fc_layers .append(nn.Linear(input_dim, num_classes))\n        layers[\"out\"] = nn.Linear(input_dim, num_classes)\n\n        self.network = nn.Sequential(layers)\n\n    def forward(self, x):\n        x = self.input_norm(x)\n        return self.network(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:22:10.848842Z","iopub.execute_input":"2025-10-01T18:22:10.849103Z","iopub.status.idle":"2025-10-01T18:22:10.855385Z","shell.execute_reply.started":"2025-10-01T18:22:10.849084Z","shell.execute_reply":"2025-10-01T18:22:10.854829Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# loss_functions\nclass FocalLoss(nn.Module):\n    \"\"\"\n    Focal Loss for multi‐class classification.\n\n    Args:\n        alpha (float): balance factor, multiplies the focal term.\n        gamma (float): focusing parameter; higher ⇒ more focus on hard examples.\n        reduction (str): 'none' | 'mean' | 'sum'\n    \"\"\"\n\n    def __init__(self, alpha: float = 1.0, gamma: float = 2.0, reduction: str = 'mean'):\n        super().__init__()\n        if reduction not in ('none', 'mean', 'sum'):\n            raise ValueError(f\"Invalid reduction mode: {reduction}\")\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        logits: shape (batch_size, num_classes)\n        targets: shape (batch_size,) with class indices\n        \"\"\"\n        # per‐sample cross‐entropy without reduction\n        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n        # pt = probability assigned to the true class\n        pt = torch.exp(-ce_loss)\n        # focal scaling\n        loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n\n        if self.reduction == 'mean':\n            return loss.mean()\n        elif self.reduction == 'sum':\n            return loss.sum()\n        else:  # 'none'\n            return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:22:11.151393Z","iopub.execute_input":"2025-10-01T18:22:11.151684Z","iopub.status.idle":"2025-10-01T18:22:11.157608Z","shell.execute_reply.started":"2025-10-01T18:22:11.151665Z","shell.execute_reply":"2025-10-01T18:22:11.156905Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# LitClassifier\nfrom sklearn.preprocessing import label_binarize\n\nclass LitClassifier(pl.LightningModule):\n    def __init__(self, model, model_name, training_cfg, labels_mapping, weight_tensor=None, using_wandb=False):\n\n        super().__init__()\n        # self.save_hyperparameters()\n\n        self.model = model\n        self.model_name = model_name\n        self.learning_rate = training_cfg.learning_rate\n        self.weight_decay = training_cfg.weight_decay\n        self.labels = list(labels_mapping.values())\n        self.labels_mapping = labels_mapping\n        self.using_wandb = using_wandb\n        self.multi_class = training_cfg.multi_class\n        self.batch_size = training_cfg.batch_size\n\n        if training_cfg.optimizer == \"adam\":\n            self.optimizer = Adam\n        elif training_cfg.optimizer == \"sgd\":\n            self.optimizer = SGD\n\n        if training_cfg.loss_type == \"focal\":\n            self.criterion = FocalLoss(alpha=training_cfg.focal_loss_alpha,\n                                       gamma=training_cfg.focal_loss_gamma, reduction=training_cfg.focal_loss_reduction)\n        elif training_cfg.loss_type == \"cross_entropy\":\n            self.criterion = nn.CrossEntropyLoss(weight=weight_tensor)\n\n        self.train_epoch_metrics = {}\n        self.val_epoch_metrics = {}\n        self.train_outputs = {\"preds\": [], \"targets\": []}\n        self.val_outputs = {\"preds\": [], \"targets\": []}\n        self.test_outputs = {\"preds\": [], \"targets\": [],'logits':[]}\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        pred = self(x)\n        loss = self.criterion(pred, y)\n        pred = pred.argmax(dim=1)\n        acc = (pred == y).float().mean() * 100.0\n        self.log('train_loss', loss, on_epoch=True,\n                 prog_bar=True, batch_size=self.batch_size)\n        self.log('train_acc', acc, on_epoch=True,\n                 prog_bar=True, batch_size=self.batch_size)\n\n        self.train_outputs[\"preds\"].append(pred)\n        self.train_outputs[\"targets\"].append(y)\n\n        return loss\n\n    def on_train_epoch_end(self):\n        all_preds = torch.cat(self.train_outputs[\"preds\"]).detach().cpu().numpy()\n        all_targets = torch.cat(\n            self.train_outputs[\"targets\"]).detach().cpu().numpy()\n        weighted_f1 = f1_score(all_targets, all_preds,\n                               average=\"weighted\") * 100.0\n        self.log(\"train_f1_score\", weighted_f1, on_epoch=True,\n                 prog_bar=True, batch_size=self.batch_size)\n\n        print(f\"[INFO] Finished Epoch {self.current_epoch + 1}:\")\n        self.train_outputs = {\"preds\": [], \"targets\": []}\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        pred = self(x)\n        loss = self.criterion(pred, y)\n        pred = pred.argmax(dim=1)\n        acc = (pred == y).float().mean() * 100.0\n        self.log('val_loss', loss, on_epoch=True,\n                 prog_bar=True, batch_size=self.batch_size)\n        self.log('val_acc', acc, on_epoch=True,\n                 prog_bar=True, batch_size=self.batch_size)\n\n        self.val_outputs[\"preds\"].append(pred)\n        self.val_outputs[\"targets\"].append(y)\n\n        return {\"val_loss\": loss, \"val_acc\": acc}\n\n    def on_validation_epoch_end(self):\n        all_preds = torch.cat(self.val_outputs[\"preds\"]).detach().cpu().numpy()\n        all_targets = torch.cat(\n            self.val_outputs[\"targets\"]).detach().cpu().numpy()\n        weighted_f1 = f1_score(all_targets, all_preds,\n                               average=\"weighted\") * 100.0\n        self.log(\"val_f1_score\", weighted_f1, on_epoch=True,\n                 prog_bar=True, batch_size=self.batch_size)\n\n        # report = classification_report(\n        #     all_targets, all_preds, digits=4, output_dict=False, zero_division=0)\n\n        # print(\"Validation Classification Report:\\n\", report)\n\n        # if self.using_wandb:\n        #     class_report = classification_report(all_targets, all_preds,\n        #                                          digits=4,\n        #                                          output_dict=True,\n        #                                          zero_division=0)\n        #     report_df = pd.DataFrame(class_report).T.reset_index()\n        #     report_df = report_df.rename(columns={\"index\": \"class\"})\n\n        #     table = wandb.Table(dataframe=report_df)\n        #     wandb.log({f\"classification_report_{self.server_round}\": table})\n            \n        self.val_outputs = {\"preds\": [], \"targets\": []}\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        pred = self(x)\n        logits=self(x)\n        loss = self.criterion(pred, y)\n        pred = pred.argmax(dim=1)\n        acc = (pred == y).float().mean() * 100.0\n\n        self.log(\"test_loss\", loss, on_epoch=True,\n                 prog_bar=True, batch_size=self.batch_size)\n        self.log(\"test_acc\", acc, on_epoch=True,\n                 prog_bar=True, batch_size=self.batch_size)\n        self.test_outputs['logits'].append(logits)\n        self.test_outputs[\"preds\"].append(pred)\n        self.test_outputs[\"targets\"].append(y)\n        return {\"test_loss\": loss, \"test_acc\": acc, \"preds\": pred, \"targets\": y}\n\n    def on_test_epoch_end(self):\n        all_preds = torch.cat(self.test_outputs[\"preds\"]).detach().cpu().numpy()\n        all_logits = torch.cat(self.test_outputs[\"logits\"]).detach().cpu().numpy()\n\n        all_targets = torch.cat(\n            self.test_outputs[\"targets\"]).detach().cpu().numpy()\n        self.test_outputs = {\"preds\": [], \"targets\": [],\"logits\":[]}\n        weighted_f1 = f1_score(all_targets, all_preds,\n                               average=\"weighted\") * 100.0\n        self.log(\"test_f1\", weighted_f1, on_epoch=True,\n                 prog_bar=True, batch_size=self.batch_size)\n        \n        # ===== AUC calculation =====\n        try:\n            # Binarize the labels for multiclass AUC\n            y_true_bin = label_binarize(all_targets, classes=list(range(len(self.labels))))\n            all_probs = torch.softmax(torch.tensor(all_logits), dim=1).numpy()\n            auc_score = roc_auc_score(y_true_bin, all_probs, multi_class=\"ovr\", average=\"weighted\")\n            self.log(\"test_auc\", auc_score * 100.0, on_epoch=True, prog_bar=True, batch_size=self.batch_size)\n        except ValueError as e:\n            print(f\"[WARN] Could not compute AUC: {e}\")\n            auc_score = None\n        # ===========================\n        \n        all_targets = np.vectorize(self.labels_mapping.get)(all_targets)\n        all_preds = np.vectorize(self.labels_mapping.get)(all_preds)\n\n        cm = confusion_matrix(all_targets, all_preds, labels=self.labels)\n\n        cr = classification_report(\n            all_targets, all_preds, digits=4, output_dict=True, zero_division=0)\n        report = classification_report(\n            all_targets, all_preds, digits=4, output_dict=False, zero_division=0)\n        weighted_f1 = f1_score(all_targets, all_preds,\n                               average=\"weighted\") * 100\n\n        results_fpr_fnr = calculate_fpr_fnr_with_global(cm)\n        fpr = results_fpr_fnr[\"global\"][\"FPR\"]\n        fnr = results_fpr_fnr[\"global\"][\"FNR\"]\n\n        results = {\n            \"test_weighted_f1\": weighted_f1,\n            \"test_auc\": auc_score * 100 if auc_score is not None else None,\n            \"test_fpr\": fpr,\n            \"test_fnr\": fnr,\n            \"classification_report\": cr,\n            \"results_fpr_fnr\": results_fpr_fnr\n        }\n        \n        os.makedirs(\"temp\", exist_ok=True)\n        json_path = os.path.join(\"temp\", f\"{self.model_name}_results.json\")\n        with open(json_path, \"w\") as f:\n            json.dump(results, f, indent=4, cls=NumpyEncoder)\n\n        if self.using_wandb:\n            wandb.save(json_path)\n\n        print(\"=== Test Evaluation Metrics ===\")\n        print(\"Classification Report:\\n\", report)\n\n        cm_normalized = confusion_matrix(\n            all_targets, all_preds, labels=self.labels, normalize=\"true\")\n        fig = plot_confusion_matrix(cm=cm,\n                                    normalized=False,\n                                    target_names=self.labels,\n                                    title=f\"Confusion Matrix of {self.model_name}\",\n                                    file_path=None,\n                                    show_figure=False)\n\n        if self.using_wandb:\n            wandb.log({f\"confusion_matrix_{self.model_name}\": wandb.Image(\n                fig), \"epoch\": self.current_epoch})\n        fig = plot_confusion_matrix(cm=cm_normalized,\n                                    normalized=True,\n                                    target_names=self.labels,\n                                    title=f\"Confusion Matrix of {self.model_name}\",\n                                    file_path=None,\n                                    show_figure=False)\n\n        if self.using_wandb:\n            wandb.log({f\"confusion_matrix_{self.model_name}_normalized\": wandb.Image(\n                fig), \"epoch\": self.current_epoch})\n\n    def configure_optimizers(self):\n        optimizer = self.optimizer(\n            self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n        return optimizer\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:22:14.620096Z","iopub.execute_input":"2025-10-01T18:22:14.620562Z","iopub.status.idle":"2025-10-01T18:22:14.642207Z","shell.execute_reply.started":"2025-10-01T18:22:14.620539Z","shell.execute_reply":"2025-10-01T18:22:14.641512Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# init_model\ndef init_model(training_cfg, model_cfg, num_features, labels_mapping, weight_tensor, using_wandb):\n\n    num_classes = 2\n    if training_cfg.multi_class:\n        num_classes = len(labels_mapping)\n\n    if model_cfg.model.type == \"mlp\":\n        model = MLP(model_cfg, num_features, num_classes)\n    elif model_cfg.model.type == \"cnn\":\n        model = CNN(model_cfg, num_features, num_classes)\n    elif model_cfg.model.type == \"cnn_lstm\":\n        model = CNNLSTM(model_cfg, num_features, num_classes)\n    elif model_cfg.model.type == \"cnn_gru\":\n        model = CNNGRU(model_cfg, num_features, num_classes)\n    elif model_cfg.model.type == \"gru\":\n        model = GRUModel(model_cfg, num_features, num_classes)\n\n    # print(f\"===> last layer: {model.network[-1]}\")\n    # print(f\"===> last layer result: {torch.argmax(model(torch.randn(2, num_features)), dim=1)}\")\n\n    return LitClassifier(model, model_cfg.model.type, training_cfg, labels_mapping, weight_tensor, using_wandb)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:22:17.624141Z","iopub.execute_input":"2025-10-01T18:22:17.624404Z","iopub.status.idle":"2025-10-01T18:22:17.629687Z","shell.execute_reply.started":"2025-10-01T18:22:17.624384Z","shell.execute_reply":"2025-10-01T18:22:17.629157Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare dataframes and dataloaders","metadata":{}},{"cell_type":"code","source":"# read and prepare dataframes\n\ndef load_df(file_path, raw_type):\n    if raw_type == \"parquet\":\n        return pd.read_parquet(file_path)\n    elif raw_type == \"csv\":\n        return pd.read_csv(file_path)\n\ndef create_processed_df(processed_dir, base_cfg):\n    os.makedirs(processed_dir, exist_ok=True)\n    \n    dataset = base_cfg.dataset\n    dp = base_cfg.dataset_properties\n    \n    df = load_df(os.path.join(original_datasets_files_path, dataset.raw), dataset.raw_type)\n    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n    df.dropna(axis=0, how='any', inplace=True)\n    df.drop_duplicates(subset=list(set(\n        df.columns) - set([dp.timestamp_col, dp.flow_id_col])), keep=\"first\", inplace=True)\n    \n    classes = df[dp.class_col].unique()\n    \n    label_encoder = LabelEncoder()\n    label_encoder.fit(list(classes))\n    labels_names = dict(zip(label_encoder.transform(\n        label_encoder.classes_), label_encoder.classes_))\n    labels_names = {int(k): str(v) for k, v in labels_names.items()}\n    \n    \n    with open(os.path.join(processed_dir, \"labels_names.pkl\"), 'wb') as f:\n            pickle.dump([labels_names, classes], f)\n        \n    df[dp.class_num_col] = label_encoder.transform(df[dp.class_col])\n    df = add_centralities(\n        df= df,\n        new_path=new_path,\n        graph_path=graph_path,\n        dataset=DATASET,\n        cn_measures=cn_measures,\n        network_features=network_features\n    )\n    \n    train_df, test_df = train_test_split(\n                df, test_size=dataset.test_size, random_state=base_cfg.random_seed, stratify=df[dp.class_num_col])\n    \n    train_df.to_parquet(os.path.join(processed_dir, \"train.parquet\"))\n    test_df.to_parquet(os.path.join(processed_dir, \"test.parquet\"))\n    return train_df, test_df, labels_names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:22:18.731692Z","iopub.execute_input":"2025-10-01T18:22:18.732124Z","iopub.status.idle":"2025-10-01T18:22:18.739801Z","shell.execute_reply.started":"2025-10-01T18:22:18.732104Z","shell.execute_reply":"2025-10-01T18:22:18.739096Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load_processed_df \ndef load_processed_df(base_cfg):\n    processed_dir = os.path.join(main_processed_dir, base_cfg.dataset.name)\n    if os.path.exists(processed_dir) and os.path.exists(os.path.join(processed_dir, \"test.parquet\")):\n        print(\"loading existing files\")\n        train_df = pd.read_parquet(os.path.join(processed_dir, \"train.parquet\"))\n        test_df = pd.read_parquet(os.path.join(processed_dir, \"test.parquet\"))\n\n        with open(os.path.join(processed_dir, \"labels_names.pkl\"), \"rb\") as f:\n            labels_names = pickle.load(f)\n        labels_mapping = labels_names[0]\n\n        return train_df, test_df, labels_mapping\n    else:\n        print(\"creating files\")\n        return create_processed_df(processed_dir, base_cfg)\n\n\ndef load_dfs(base_cfg):\n\n    dp = base_cfg.dataset_properties\n    train_df, test_df, labels_mapping = load_processed_df(base_cfg)\n\n    if base_cfg.training.multi_class:\n        label_col = dp.class_num_col\n    else:\n        label_col = dp.label_col\n\n    train_labels = train_df[label_col]\n    train_df.drop(columns=dp.drop_columns + [dp.class_col, dp.class_num_col, dp.label_col] + dp.weak_columns,\n                   inplace=True, errors='ignore')\n    \n    test_labels = test_df[label_col]\n    test_df.drop(columns=dp.drop_columns + [dp.class_col, dp.class_num_col, dp.label_col] + dp.weak_columns,\n                   inplace=True, errors='ignore')\n\n    input_dim = train_df.shape[1]\n\n    return (\n        train_df,\n        train_labels,\n        test_df,\n        test_labels,\n        input_dim,\n        labels_mapping\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:22:19.957118Z","iopub.execute_input":"2025-10-01T18:22:19.957669Z","iopub.status.idle":"2025-10-01T18:22:19.964559Z","shell.execute_reply.started":"2025-10-01T18:22:19.957647Z","shell.execute_reply":"2025-10-01T18:22:19.963887Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# LightningDataModule\n\nclass FnnDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n\nclass CnnDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        x_sample = self.X[idx].unsqueeze(0)  # add channel dimension\n        return x_sample, self.y[idx]\n\n\nclass SlidingWindowDataset(Dataset):\n    def __init__(self, data, targets, seq_length=10, stride=1, transform=None):\n        super().__init__()\n        self.data = data\n        self.targets = targets\n        self.seq_length = seq_length\n        self.stride = stride\n        self.transform = transform\n        self.num_sequences = (len(self.data) - seq_length) // stride + 1\n        assert self.num_sequences > 0, \"Not enough data to form even one sequence!\"\n\n    def __len__(self):\n        return self.num_sequences\n\n    def __getitem__(self, idx):\n        start_idx = idx * self.stride\n        end_idx = start_idx + self.seq_length\n        # shape: (seq_length, num_features)\n        x_seq = self.data[start_idx:end_idx]\n        y_label = self.targets[end_idx - 1]\n        if self.transform:\n            x_seq = self.transform(x_seq)\n        # Convert to tensors\n        x_seq = torch.as_tensor(x_seq, dtype=torch.float32)\n        y_label = torch.as_tensor(y_label, dtype=torch.long)\n        return x_seq, y_label\n\n\nclass LitDataModule(pl.LightningDataModule):\n    def __init__(\n        self,\n        X_train,\n        y_train,\n        X_val,\n        y_val,\n        X_test,\n        y_test,\n        dataset,\n        model_type,\n        batch_size=128,\n        oversample=False,\n        **dataset_kwargs\n    ):\n\n        super().__init__()\n        \n        self.X_train = X_train\n        self.y_train = y_train\n        self.X_val = X_val\n        self.y_val = y_val\n        self.X_test = X_test\n        self.y_test = y_test\n        \n        self.batch_size = batch_size\n        self.oversample = oversample\n        self.dataset = dataset\n        self.dataset_kwargs = dataset_kwargs\n        self.model_type = model_type\n        self.num_features = 0\n        \n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.criterion(y_hat, y)\n    \n        self.log(\"test_loss\", loss, prog_bar=True, on_epoch=True, logger=True)\n        return {\"test_loss\": loss}\n\n    def setup(self, stage=None):\n\n        scaler = StandardScaler()\n\n        self.X_train = scaler.fit_transform(self.X_train)\n        self.X_val = scaler.transform(self.X_val)\n        self.X_test = scaler.transform(self.X_test)\n\n        classes = np.unique(self.y_train)\n        weights = sklearn_class_weight.compute_class_weight(\n            'balanced', classes=classes, y=self.y_train)\n\n\n        \n        # self.class_weights = torch.FloatTensor(weights)\n\n    def _get_dataloader(self, split, sampler=None):\n        if split == \"train\":\n            X, y = self.X_train, self.y_train\n            shuffle = True\n        elif split == \"val\":\n            X, y = self.X_val, self.y_val\n            shuffle = False\n        elif split == \"test\":\n            X, y = self.X_test, self.y_test\n            shuffle = False\n        else:\n            raise ValueError(f\"Unknown split: {split}\")\n\n        if self.model_type == \"mlp\":\n            dataset = FnnDataset(X, y)\n        elif self.model_type in [\"cnn\", \"cnn_lstm\",\"cnn_gru\"]:\n            dataset = CnnDataset(X, y)\n        elif self.model_type in [\"lstm\", \"gru\"]:\n            dataset = SlidingWindowDataset(\n                X, y, seq_length=self.dataset_kwargs[\"sequence_length\"], stride=self.dataset_kwargs[\"stride\"])\n\n        if sampler:\n            return DataLoader(dataset, batch_size=self.batch_size, sampler=sampler)\n        else:\n            return DataLoader(dataset, batch_size=self.batch_size, shuffle=shuffle, num_workers=0, drop_last=False)\n\n    def train_dataloader(self):\n        if self.oversample:\n            # compute sample weights based on class frequency\n            class_counts = np.bincount(self.y_train)\n            inv_freq = 1.0 / class_counts\n            alpha    = 0.5\n            class_weights = inv_freq ** alpha\n            \n            sample_weights = class_weights[self.y_train]\n            sampler = WeightedRandomSampler(\n                weights=sample_weights,\n                num_samples=len(sample_weights),\n                replacement=True,\n            )\n\n        else:\n            sampler = None\n            # return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=0)\n\n\n        # for x,y in self._get_dataloader(\"train\"):\n        #     print(f\"==>> torch.unique(y, return_counts=True): {torch.unique(y, return_counts=True)}\")\n        #     break\n        return self._get_dataloader(\"train\", sampler)\n\n    def val_dataloader(self):\n        return self._get_dataloader(\"val\")\n\n    def test_dataloader(self):\n        return self._get_dataloader(\"test\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:22:25.646677Z","iopub.execute_input":"2025-10-01T18:22:25.646928Z","iopub.status.idle":"2025-10-01T18:22:25.661953Z","shell.execute_reply.started":"2025-10-01T18:22:25.646912Z","shell.execute_reply":"2025-10-01T18:22:25.661308Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Main Code","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport time\nimport psutil\nfrom sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix, log_loss\n\ndef main(base_cfg, models):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # models_cfg_mapping = {model: all_models_cfgs[model] for model in models}\n\n    models_cfg_mapping = {model: all_models_cfgs[model] for model in models}\n    logging_cfg = base_cfg.logging[base_cfg.logging.selected_type]\n    \n    os.makedirs(\n        base_cfg.logging[base_cfg.logging.selected_type].save_dir, exist_ok=True)\n\n    # loading clients data\n    \n    X_tr, y_tr, X_test, y_test, input_dim, labels_mapping = load_dfs(base_cfg)\n    # print(f\"==>> X_tr.columns: {X_tr.columns}\")\n    # print(f\"==>> X_test.columns: {X_test.columns}\")\n\n    X_train, X_val, y_train, y_val = train_test_split(X_tr, y_tr, test_size=base_cfg.dataset_properties.val_size)\n    \n    run_dtime = time.strftime(\"%Y%m%d-%H%M%S\")\n\n\n    for model_name in models:\n\n        config = {\n            \"model_name\": model_name,\n            \"input_dim\": input_dim,\n            \"run_dtime\": run_dtime,\n            \"input_layer_norm\": models_cfg_mapping[model_name].input_layer_norm\n        }\n\n        for attribute_name, attribute_value in base_cfg.training.items():\n            config[attribute_name] = attribute_value\n\n        for layer in models_cfg_mapping[model_name].layers:\n            model_config = models_cfg_mapping[model_name][layer]\n            for attribute_name, attribute_value in model_config.items():\n                config[f\"{model_name}_{layer}_{attribute_name}\"] = attribute_value\n                \n        dataset_kwargs = dict(\n            sequence_length=base_cfg.training.sequence_length,\n            stride=base_cfg.training.stride,\n            using_masking=base_cfg.training.using_masking,\n            masked_class=base_cfg.training.masked_class,\n            num_workers=3,\n            device='cuda' if torch.cuda.is_available() else \"cpu\"\n        )\n    \n        data_module = LitDataModule(\n            X_train=np.array(X_train),\n            y_train=np.array(y_train),\n            X_val=np.array(X_val),\n            y_val=np.array(y_val),\n            X_test=np.array(X_test),\n            y_test=np.array(y_test),\n            # X_train, y_train, X_val, y_val, X_test, y_test,\n            dataset=base_cfg.dataset,\n            batch_size=base_cfg.training.batch_size,\n            multi_class=base_cfg.training.multi_class,\n            model_type = model_name,\n            oversample = base_cfg.training.oversample,\n            **dataset_kwargs)\n        data_module.setup()\n\n        \n        if base_cfg.training.use_weighted_loss:\n            weight_tensor = compute_class_weights(np.array(y_train), np.array(list(labels_mapping.keys())), version=base_cfg.training.weighted_loss_version, device=device)\n        else:\n            weight_tensor = None\n            \n        pl_model = init_model(base_cfg.training, models_cfg_mapping[model_name], input_dim, labels_mapping,\n                           weight_tensor, base_cfg.logging.selected_type == None)        \n\n        \n        if base_cfg.logging.selected_type == \"wandb\":\n            logger = WandbLogger(\n                project=logging_cfg.project,\n                name=model_name,\n                config=config,\n                save_dir=f\"{logging_cfg.save_dir}/{model_name}\",\n                # settings=wandb.Settings(quiet=True)\n            )\n\n        else:\n            logger = TensorBoardLogger(\n                f\"{logging_cfg.save_dir}/{time.strftime('%Y%m%d-%H%M%S')}/{model_name}\")\n\n        trainer = pl.Trainer(\n            max_epochs=base_cfg.training.max_epochs,\n            num_sanity_val_steps=0,\n            log_every_n_steps=0,\n            # enable_progress_bar=False,\n            enable_checkpointing=False,\n            logger=logger,\n        )\n        # 🕒 Start timing & memory tracking\n        start_time = time.time()\n        process = psutil.Process(os.getpid())\n        mem_before = process.memory_info().rss / 1024 / 1024  # MB\n        \n        trainer.fit(pl_model, datamodule=data_module)\n        print(f\"[INFO] Finished training model: {model_name}\")\n        results=trainer.test(pl_model, datamodule=data_module)\n        # results = trainer.test(pl_model, datamodule=data_module)\n        test_loss = results[0].get(\"test_loss\", None)  # extract test_loss\n        print(f\"[INFO] Test Loss for {model_name}: {test_loss}\")\n\n        # 🕒 End timing & memory tracking\n        elapsed_time = time.time() - start_time\n        mem_after = process.memory_info().rss / 1024 / 1024\n        mem_used = mem_after - mem_before\n\n        metrics = {\n            \"Model\": model_name,\n            \"Time_Seconds\": elapsed_time,\n            \"Memory_MB\": mem_used,\n            \"test_loss\":test_loss\n        }\n\n        print(metrics)\n        \n        # Save metrics to JSON\n        json_path = f\"/kaggle/working/temp/{model_name}_metrics.json\"\n        with open(json_path, \"w\") as f:\n            json.dump(metrics, f, indent=4) \n\n        if base_cfg.logging.selected_type == \"wandb\":\n            wandb.finish()\n        # if base_cfg.logging.selected_type == \"wandb\":\n        #     wandb.finish()\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:22:26.780108Z","iopub.execute_input":"2025-10-01T18:22:26.780383Z","iopub.status.idle":"2025-10-01T18:22:26.792994Z","shell.execute_reply.started":"2025-10-01T18:22:26.780358Z","shell.execute_reply":"2025-10-01T18:22:26.792161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"main(base_cfg, models)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:22:28.594450Z","iopub.execute_input":"2025-10-01T18:22:28.594746Z","iopub.status.idle":"2025-10-01T19:10:22.074849Z","shell.execute_reply.started":"2025-10-01T18:22:28.594724Z","shell.execute_reply":"2025-10-01T19:10:22.074132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# file_path = \"/kaggle/working/temp/cnn_results.json\"\n# if os.path.exists(file_path):\n#     os.remove(file_path)\n#     print(f\"Deleted: {file_path}\")\n# else:\n#     print(\"File does not exist:\", file_path)\n# model_name=\"cnn\"\n# elapsed_time=\"100\"\n# mem_used=\"100\"\n# test_loss=\"100\"\n# metrics = {\n#     \"Model\": model_name,\n#     \"Time_Seconds\": elapsed_time,\n#     \"Memory_MB\": mem_used,\n#     \"test_loss\":test_loss\n# }\n\n# print(metrics)\n\n# # Save metrics to JSON\n# json_path = f\"/kaggle/working/temp/{model_name}_metrics.json\"\n# with open(json_path, \"w\") as f:\n#     json.dump(metrics, f, indent=4) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T13:59:23.842587Z","iopub.execute_input":"2025-10-01T13:59:23.842887Z","iopub.status.idle":"2025-10-01T13:59:23.846671Z","shell.execute_reply.started":"2025-10-01T13:59:23.842865Z","shell.execute_reply":"2025-10-01T13:59:23.846039Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nlabels_map={}\n# Your label mapping (from the print output you shared)\nif dataset_name==\"cic_ton_iot\":\n    labels_map = {\n        \"0\": \"Benign\",\n        \"1\": \"backdoor\",\n        \"2\": \"ddos\",\n        \"3\": \"dos\",\n        \"4\": \"injection\",\n        \"5\": \"mitm\",\n        \"6\": \"password\",\n        \"7\": \"ransomware\",\n        \"8\": \"scanning\",\n        \"9\": \"xss\"\n    }\nelse:\n    labels_map = {\n        \"0\": \"BENIGN\",\n        \"1\": \"Bot\",\n        \"2\": \"DDoS\",\n        \"3\": \"DoS GoldenEye\",\n        \"4\": \"DoS Hulk\",\n        \"5\": \"DoS Slowhttptest\",\n        \"6\": \"DoS slowloris\",\n        \"7\": \"FTP-Patator\",\n        \"8\": \"Heartbleed\",\n        \"9\": \"Infiltration\",\n        \"10\": \"PortScan\",\n        \"11\": \"SSH-Patator\",\n        \"12\": \"Web Attack \\ufffd Brute Force\",\n        \"13\": \"Web Attack \\ufffd Sql Injection\",\n        \"14\": \"Web Attack \\ufffd XSS\"\n    }\ndef json_to_csv_per_file(json_folder, output_folder, dataset_name=dataset_name):\n    os.makedirs(output_folder, exist_ok=True)\n    rows_all=[]\n    rows_m_avg=[]\n    training_time=0\n    training_mem_used=0\n    for filename in os.listdir(json_folder):\n        # print(filename)\n        if filename.endswith(\"results.json\"):\n            # continue\n            filepath = os.path.join(json_folder, filename)\n            with open(filepath, \"r\") as f:\n                data = json.load(f)\n            no_ext = filename.replace(\"_results.json\", \"\")\n            # Remove the trailing \"_metrics\"\n            # print(no_ext)\n            model_name=no_ext\n            metric_path = os.path.join(json_folder,f\"{no_ext}_metrics.json\")\n            # print(metric_path)\n            filepath = os.path.join(json_folder, filename)\n            rows = []\n            # --- Per-class metrics ---\n            classification_report=data[\"classification_report\"]\n            global_value=classification_report[\"weighted avg\"]\n            macro_value=classification_report[\"macro avg\"]\n            t_fnr=0\n            t_fpr=0\n            t=0\n            m_avg_fnr=0\n            m_avg_fpr=0\n            # print(classification_report)\n            # print(\"macro_value:\",macro_value)\n            if \"results_fpr_fnr\" in data and \"per_class\" in data[\"results_fpr_fnr\"]:\n                # print(data[\"results_fpr_fnr\"])\n                for cls, metrics in data[\"results_fpr_fnr\"][\"per_class\"].items():\n                    attack=labels_map.get(cls,cls)\n                    # print(f\"{attack}\")\n                    # print(metrics)\n                    metrics_d=classification_report[attack]\n                    row = {\n                        \"ATTACK TYPE\": attack,\n                        \"precision\": metrics_d.get(\"precision\"),\n                        \"recall\": metrics_d.get(\"recall\"),\n                        \"f1_score\": metrics_d.get(\"f1-score\"),\n                        \"support\": metrics_d.get(\"support\"),\n                        \"FPR\": metrics.get(\"FPR\"),\n                        \"FNR\": metrics.get(\"FNR\"),\n                    }\n                    t_fnr+=row[\"FNR\"]\n                    t_fpr+=row[\"FPR\"]\n                    t+=1\n                    # print()\n                    # print(metrics_d.get(\"precision\"))\n                    # print()\n                    rows.append(row)\n                m_avg_fnr+=t_fnr/t\n                m_avg_fpr+=t_fpr/t\n                # print(m_avg_fpr)\n            df_classes = pd.DataFrame(rows)\n            with open(metric_path, \"r\") as f:\n                device_data = json.load(f)\n            # print(device_data)\n            MEMORY_USED=device_data[\"Memory_MB\"]\n            TIME_CONSUMED=device_data[\"Time_Seconds\"]\n            test_loss=device_data[\"test_loss\"]\n            Model=device_data[\"Model\"]\n            # print(\"Model:\", Model)\n            # print(\"Memory:\", MEMORY_USED)\n            # print(\"Time:\", TIME_CONSUMED)\n            # print(\"test Loss:\",test_loss)\n            # --- Global / test metrics ---\n            global_metrics = {\n                \"test_accuracy\":classification_report[\"accuracy\"],\n                \"test_weighted_f1\": global_value[\"f1-score\"],\n                \"test_loss\":test_loss,\n                \"recall\":global_value[\"recall\"],\n                \"precision\":global_value[\"precision\"],\n                \"test_fpr\": data.get(\"test_fpr\"),\n                \"test_fnr\": data.get(\"test_fnr\"),\n                \"test_auc\": data.get(\"test_auc\"),\n                \"global_FPR\": data[\"results_fpr_fnr\"][\"global\"].get(\"FPR\") if \"results_fpr_fnr\" in data and \"global\" in data[\"results_fpr_fnr\"] else None,\n                \"global_FNR\": data[\"results_fpr_fnr\"][\"global\"].get(\"FNR\") if \"results_fpr_fnr\" in data and \"global\" in data[\"results_fpr_fnr\"] else None,\n                \"time_seconds\":TIME_CONSUMED,\n                \"memory_mb\":MEMORY_USED\n            }\n            rows_all.append({\n                \"model_name\":Model,\n                \"test_accuracy\":classification_report[\"accuracy\"],\n                \"test_weighted_f1\": global_value[\"f1-score\"],\n                \"test_loss\":test_loss,\n                \"recall\":global_value[\"recall\"],\n                \"precision\":global_value[\"precision\"],\n                \"test_fpr\": data.get(\"test_fpr\"),\n                \"test_fnr\": data.get(\"test_fnr\"),\n                \"test_auc\": data.get(\"test_auc\"),\n                \"global_FPR\": data[\"results_fpr_fnr\"][\"global\"].get(\"FPR\") if \"results_fpr_fnr\" in data and \"global\" in data[\"results_fpr_fnr\"] else None,\n                \"global_FNR\": data[\"results_fpr_fnr\"][\"global\"].get(\"FNR\") if \"results_fpr_fnr\" in data and \"global\" in data[\"results_fpr_fnr\"] else None,\n                \"time_seconds\":TIME_CONSUMED,\n                \"memory_mb\":MEMORY_USED\n            }\n            )\n            # Macro_Avg\n            rows_m_avg.append({\n                \"model\": Model,\n                \"F1\": macro_value[\"f1-score\"],\n                \"Precision\": macro_value[\"precision\"],\n                \"Recall\": macro_value[\"recall\"],\n                \"Support\": macro_value[\"support\"],\n                \"FNR\": m_avg_fnr,\n                \"FPR\": m_avg_fpr,\n            })\n            # print(\"rows_m_avg:\",rows_m_avg)\n            df_global = pd.DataFrame([global_metrics])\n\n            # --- Save CSV ---\n            csv_output = os.path.join(output_folder, filename.replace(\".json\", \".csv\"))\n            with open(csv_output, \"w\") as f:\n                f.write(\"### Per-class Metrics\\n\")\n            df_classes.to_csv(csv_output, mode=\"a\", index=False)\n            with open(csv_output, \"a\") as f:\n                f.write(\"\\n### Global Metrics\\n\")\n            df_global.to_csv(csv_output, mode=\"a\", index=False)\n\n            print(f\"✅ CSV saved as {csv_output}\")\n        # print(rows_all)\n    df = pd.DataFrame(rows_all)\n    temp_dir = \"/kaggle/working/temp/csv_results/\"\n    \n    # Create folder if it doesn't exist\n    if not os.path.exists(temp_dir):\n        os.makedirs(temp_dir)\n    \n    # Save CSV inside temp/\n    if dataset_name==\"cic_ton_iot\":\n        csv_path = os.path.join(temp_dir, \"ML_BENCHMARKING_CIC_TON_IOT.csv\")\n    else:\n        csv_path = os.path.join(temp_dir, \"ML_BENCHMARKING_CIC_IDS_2017.csv\")\n    df.to_csv(csv_path, index=False)\n    print(f\"✅ CSV saved as {csv_path}\")\n    \n    df = pd.DataFrame(rows_m_avg)\n    temp_dir = \"/kaggle/working/temp/csv_results/\"\n    \n    # Create folder if it doesn't exist\n    if not os.path.exists(temp_dir):\n        os.makedirs(temp_dir)\n    \n    # Save CSV inside temp/\n    if dataset_name==\"cic_ton_iot\":\n        csv_path = os.path.join(temp_dir, \"macro_results_CIC_TON_IOT.csv\")\n    else:\n        csv_path = os.path.join(temp_dir, \"macro_results_CIC_IDS_2017.csv\")\n    df.to_csv(csv_path, index=False)\n    print(f\"✅ CSV saved as {csv_path}\")\n\njson_folder = \"/kaggle/working/temp/\"\noutput_folder = \"/kaggle/working/temp/csv_results\"\njson_to_csv_per_file(json_folder, output_folder,dataset_name=dataset_name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T13:59:24.013242Z","iopub.execute_input":"2025-10-01T13:59:24.013917Z","iopub.status.idle":"2025-10-01T13:59:24.047021Z","shell.execute_reply.started":"2025-10-01T13:59:24.013891Z","shell.execute_reply":"2025-10-01T13:59:24.046311Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(dataset_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T13:59:33.565658Z","iopub.execute_input":"2025-10-01T13:59:33.565979Z","iopub.status.idle":"2025-10-01T13:59:33.570060Z","shell.execute_reply.started":"2025-10-01T13:59:33.565955Z","shell.execute_reply":"2025-10-01T13:59:33.569465Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List all installed packages with versions\n!pip list\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Save full environment snapshot\n# !pip list --format=freeze > /kaggle/working/environment_snapshot.txt\n# print(\"✅ Environment snapshot saved to /kaggle/working/environment_snapshot.txt\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}