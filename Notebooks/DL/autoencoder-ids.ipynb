{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8089266,"sourceType":"datasetVersion","datasetId":4775518},{"sourceId":8089281,"sourceType":"datasetVersion","datasetId":4775527},{"sourceId":12650227,"sourceType":"datasetVersion","datasetId":7994437},{"sourceId":470644,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":379672,"modelId":399557},{"sourceId":593572,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":444208,"modelId":460697}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1>AutoEncoderDecoder</h1>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader,Dataset ,TensorDataset, WeightedRandomSampler, RandomSampler\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nimport wandb\nfrom omegaconf import OmegaConf\nimport os\nimport torchmetrics\nimport warnings\nfrom kaggle_secrets import UserSecretsClient\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport time\nimport psutil\nimport json\nfrom sklearn.metrics import roc_auc_score, precision_score, recall_score\nimport networkx as nx\nimport igraph as ig\n# change 1.02\n# import /kaggle/input/centrality_network/pytorch/default/1 as network\nimport sys\nsys.path.append(\"/kaggle/input/centrality_network/pytorch/default/1\")\n\nfrom network_features import separate_graph, cal_betweenness_centrality, cal_k_core, cal_k_truss\nfrom CommCentralityCode import comm_centreality\nfrom modularity_vitality import modularity_vitality\n\nsys.path.append(\"/kaggle/input/githubrepofiles/pytorch/default/1\")\nfrom src.dataset.dataset_info import datasets\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:40:03.391784Z","iopub.execute_input":"2025-10-27T10:40:03.392055Z","iopub.status.idle":"2025-10-27T10:40:21.565551Z","shell.execute_reply.started":"2025-10-27T10:40:03.392028Z","shell.execute_reply":"2025-10-27T10:40:21.564938Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# name = \"cic_ton_iot_5_percent\"\n# name = \"cic_ton_iot\"\n# name = \"cic_ids_2017_5_percent\"\nname = \"cic_ids_2017\"\nNO_NODE_FEATURE=False # False: centrality added  True: centality not used\n\ndataset = datasets[name]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:40:23.470642Z","iopub.execute_input":"2025-10-27T10:40:23.471520Z","iopub.status.idle":"2025-10-27T10:40:23.475029Z","shell.execute_reply.started":"2025-10-27T10:40:23.471493Z","shell.execute_reply":"2025-10-27T10:40:23.474260Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\n# change 1.04\ndef add_centralities(df, new_path, graph_path, dataset, cn_measures, network_features):\n# def add_centralities(df, new_path, graph_path, dataset, cn_measures):\n        # change 1.05\n    # G = nx.from_pandas_edgelist(df, source=\"Src IP\",target=\"Dst IP\", create_using=nx.DiGraph())\n    if NO_NODE_FEATURE:\n        print(\"NO node features added\")\n        return df\n    G = nx.from_pandas_edgelist( df, source=dataset.src_ip_col, target=dataset.dst_ip_col, create_using=nx.DiGraph())\n    G.remove_nodes_from(list(nx.isolates(G)))\n    for node in G.nodes():\n        G.nodes[node]['label'] = node\n\n    G1 = ig.Graph.from_networkx(G)\n    labels = [G.nodes[node]['label'] for node in G.nodes()]\n    G1.vs['label'] = labels\n\n    part = G1.community_infomap()\n    communities = []\n    for com in part:\n        communities.append([G1.vs[node_index]['label'] for node_index in com])\n\n    community_labels = {}\n    for i, community in enumerate(communities):\n        for node in community:\n            community_labels[node] = i\n\n    nx.set_node_attributes(G, community_labels, \"new_community\")\n\n    intra_graph, inter_graph = separate_graph(G, communities)\n\n    if \"betweenness\" in cn_measures:\n        nx.set_node_attributes(G, cal_betweenness_centrality(G), \"betweenness\")\n        print(\"calculated betweenness\")\n    if \"local_betweenness\" in cn_measures:\n        nx.set_node_attributes(G, cal_betweenness_centrality(\n            intra_graph), \"local_betweenness\")\n        print(\"calculated local_betweenness\")\n    if \"global_betweenness\" in cn_measures:\n        nx.set_node_attributes(G, cal_betweenness_centrality(\n            inter_graph), \"global_betweenness\")\n        print(\"calculated global_betweenness\")\n    if \"degree\" in cn_measures:\n        nx.set_node_attributes(G, nx.degree_centrality(G), \"degree\")\n        print(\"calculated degree\")\n    if \"local_degree\" in cn_measures:\n        nx.set_node_attributes(\n            G, nx.degree_centrality(intra_graph), \"local_degree\")\n        print(\"calculated local_degree\")\n    if \"global_degree\" in cn_measures:\n        nx.set_node_attributes(G, nx.degree_centrality(\n            inter_graph), \"global_degree\")\n        print(\"calculated global_degree\")\n    if \"eigenvector\" in cn_measures:\n        nx.set_node_attributes(G, nx.eigenvector_centrality(\n            G, max_iter=600), \"eigenvector\")\n        print(\"calculated eigenvector\")\n    if \"local_eigenvector\" in cn_measures:\n        nx.set_node_attributes(G, nx.eigenvector_centrality(\n            intra_graph), \"local_eigenvector\")\n        print(\"calculated local_eigenvector\")\n    if \"global_eigenvector\" in cn_measures:\n        nx.set_node_attributes(G, nx.eigenvector_centrality(\n            inter_graph), \"global_eigenvector\")\n        print(\"calculated global_eigenvector\")\n    if \"closeness\" in cn_measures:\n        nx.set_node_attributes(G, nx.closeness_centrality(G), \"closeness\")\n        print(\"calculated closeness\")\n    if \"local_closeness\" in cn_measures:\n        nx.set_node_attributes(G, nx.closeness_centrality(\n            intra_graph), \"local_closeness\")\n        print(\"calculated local_closeness\")\n    if \"global_closeness\" in cn_measures:\n        nx.set_node_attributes(G, nx.closeness_centrality(\n            inter_graph), \"global_closeness\")\n        print(\"calculated global_closeness\")\n    if \"pagerank\" in cn_measures:\n        nx.set_node_attributes(G, nx.pagerank(G, alpha=0.85), \"pagerank\")\n        print(\"calculated pagerank\")\n    if \"local_pagerank\" in cn_measures:\n        nx.set_node_attributes(G, nx.pagerank(\n            intra_graph, alpha=0.85), \"local_pagerank\")\n        print(\"calculated local_pagerank\")\n    if \"global_pagerank\" in cn_measures:\n        nx.set_node_attributes(G, nx.pagerank(\n            inter_graph, alpha=0.85), \"global_pagerank\")\n        print(\"calculated global_pagerank\")\n    if \"k_core\" in cn_measures:\n        nx.set_node_attributes(G, cal_k_core(G), \"k_core\")\n        print(\"calculated k_core\")\n    if \"k_truss\" in cn_measures:\n        nx.set_node_attributes(G, cal_k_truss(G), \"k_truss\")\n        print(\"calculated k_truss\")\n    if \"Comm\" in cn_measures:\n        nx.set_node_attributes(\n            G, comm_centreality(G, community_labels), \"Comm\")\n        print(\"calculated Comm\")\n    if \"mv\" in cn_measures:\n        nx.set_node_attributes(G, modularity_vitality(G1, part), \"mv\")\n        print(\"calculated mv\")\n\n    # nx.write_gexf(G, graph_path)\n\n    features_dicts = {}\n    for measure in cn_measures:\n        features_dicts[measure] = nx.get_node_attributes(G, measure)\n        print(f\"==>> features_dicts: {measure , len(features_dicts[measure])}\")\n\n    for feature in network_features:\n        if feature[:3] == \"src\":\n            df[feature] = df.apply(lambda row: features_dicts[feature[4:]].get(\n                row[dataset.src_ip_col], -1), axis=1)\n            # df[feature] = df.apply(lambda row: features_dicts[feature[4:]].get(\n                # row['Src Ip'], -1), axis=1)\n        if feature[:3] == \"dst\":\n            df[feature] = df.apply(lambda row: features_dicts[feature[4:]].get(\n                row[dataset.dst_ip_col], -1), axis=1)\n            # df[feature] = df.apply(lambda row: features_dicts[feature[4:]].get(row['Dst IP'], -1), axis=1)\n    print(f\"--------------------------  writting the DataFrame to {new_path} ----------------------\")\n    df.to_parquet(new_path)\n    print(f\"--------------------------DataFrame written to {new_path} --------------------------\")\n    # print(df.columns)\n    # return network_features\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:40:25.736696Z","iopub.execute_input":"2025-10-27T10:40:25.736964Z","iopub.status.idle":"2025-10-27T10:40:25.751878Z","shell.execute_reply.started":"2025-10-27T10:40:25.736944Z","shell.execute_reply":"2025-10-27T10:40:25.751171Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"warnings.filterwarnings('ignore')\n\n# CHANGE 1.02\nnew_path = os.path.join(\"/kaggle/working/\",f\"{name}_with_centralities.parquet\")\ngraph_path = os.path.join(\"/kaggle/working/\",f\"{name}_graph.gpickle\")\n# cn_measures = [\"degree\", \"betweenness\", \"closeness\", \"eigenvector\"]\ncn_measures = [\"betweenness\", \"degree\", \"pagerank\", \"closeness\", \"k_truss\"]\nnetwork_features = ['src_betweenness', 'dst_betweenness', 'src_degree', 'dst_degree', 'src_pagerank', 'dst_pagerank', 'src_closeness', 'dst_closeness', 'src_k_truss', 'dst_k_truss']\nif name==\"cic_ids_2017\":\n    # Optimized Configuration\n    config = OmegaConf.create({\n        \"wandb\": {\n            \"project\": \"DL-NIDS-2--cic-ids-2017\",\n            \"entity\": \"mohammad-fleity-lebanese-university\",\n            \"tags\": [\"AutoEncoderDecoder\", \"CIC-IDS-2017\", \"PyTorch\"],\n            \"notes\": \"Optimized AutoEncoderDecoder for network intrusion detection\",\n        },\n        \"model\": {\n            \"hidden_size\": 128,          # Increased capacity\n            \"num_layers\": 2,             # Deeper network\n            \"dropout\": 0.4,              # Stronger regularization\n            \"dense_units\": [128, 64],    # Better feature extraction\n            \"learning_rate\": 0.0001,     # Slower learning\n            \"weight_decay\": 1e-4         # Stronger L2 regularization\n        },\n        \"training\": {\n            \"sequence_length\": 5,        # Longer temporal context\n            \"batch_size\": 256,           # Larger batches\n            \"max_epochs\": 15,            # More training time\n            # \"max_epochs\": 1,            # More training time\n            \"early_stopping_patience\": 7,# More patience\n            \"oversample\": True,          # Class balancing\n            \"gpus\": 1 if torch.cuda.is_available() else 0,\n            \"train_size\": 0.7,           # Proper train/val split\n            \"val_size\": 0.15             # 70/15/15 split\n        },\n        \"data\": {\n            \"raw\": \"cic_ids_2017.parquet\",\n            \"num_workers\": 4\n        }\n    })\n    dataset_name=\"CIC_IDS_2017\"\nelif name ==\"cic_ton_iot\":\n    config = OmegaConf.create({\n        \"wandb\": {\n            \"project\": \"DL-NIDS-2--cic-ton-iot\",\n            \"entity\": \"mohammad-fleity-lebanese-university\",\n            \"tags\": [\"AutoEncoderDecoder\", \"CIC-TON-IOT\", \"PyTorch\"],\n            \"notes\": \"Optimized AutoEncoderDecoder for network intrusion detection\"\n        },\n        \"model\": {\n            \"hidden_size\": 128,          # Increased capacity\n            \"num_layers\": 2,             # Deeper network\n            \"dropout\": 0.4,              # Stronger regularization\n            \"dense_units\": [128, 64],    # Better feature extraction\n            \"learning_rate\": 0.0001,     # Slower learning\n            \"weight_decay\": 1e-4         # Stronger L2 regularization\n        },\n        \"training\": {\n            \"sequence_length\": 5,        # Longer temporal context\n            \"batch_size\": 256,           # Larger batches\n            \"max_epochs\": 15,            # More training time\n            # \"max_epochs\": 1,            # More training time\n            \"early_stopping_patience\": 7,# More patience\n            \"oversample\": True,          # Class balancing\n            \"gpus\": 1 if torch.cuda.is_available() else 0,\n            \"train_size\": 0.7,           # Proper train/val split\n            \"val_size\": 0.15             # 70/15/15 split\n        },\n        \"data\": {\n            \"raw\": \"cic_ton_iot.parquet\",\n            \"num_workers\": 4\n        }\n    })\n    dataset_name=\"CIC_TON_IOT\"\n    \nelif name ==\"cic_ids_2017_5_percent\": # THIS IS JUST FOR TESTING THE FUNCTIONALITIES FASTER\n    config = OmegaConf.create({\n        \"wandb\": {\n            \"project\": \"DL-NIDS-2--cic-ton-iot\",\n            \"entity\": \"mohammad-fleity-lebanese-university\",\n            \"tags\": [\"AutoEncoderDecoder\", \"CIC-TON-IOT\", \"PyTorch\"],\n            \"notes\": \"Optimized AutoEncoderDecoder for network intrusion detection\"\n        },\n        \"model\": {\n            \"hidden_size\": 128,          # Increased capacity\n            \"num_layers\": 2,             # Deeper network\n            \"dropout\": 0.4,              # Stronger regularization\n            \"dense_units\": [128, 64],    # Better feature extraction\n            \"learning_rate\": 0.0001,     # Slower learning\n            \"weight_decay\": 1e-4         # Stronger L2 regularization\n        },\n        \"training\": {\n            \"sequence_length\": 5,        # Longer temporal context\n            \"batch_size\": 256,           # Larger batches\n            \"max_epochs\": 1,            # More training time\n            \"early_stopping_patience\": 7,# More patience\n            \"oversample\": True,          # Class balancing\n            \"gpus\": 1 if torch.cuda.is_available() else 0,\n            \"train_size\": 0.7,           # Proper train/val split\n            \"val_size\": 0.15             # 70/15/15 split\n        },\n        \"data\": {\n            \"raw\": \"cic_ids_2017_5_percent.parquet\",\n            \"num_workers\": 4\n        }\n    })\n    dataset_name=\"cic_ids_2017_5_percent\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:40:26.028013Z","iopub.execute_input":"2025-10-27T10:40:26.028618Z","iopub.status.idle":"2025-10-27T10:40:26.039459Z","shell.execute_reply.started":"2025-10-27T10:40:26.028599Z","shell.execute_reply":"2025-10-27T10:40:26.038887Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def calculate_fpr_fnr_with_global(cm):\n    \"\"\"\n    Calculate FPR and FNR for each class and globally for a multi-class confusion matrix.\n\n    Parameters:\n        cm (numpy.ndarray): Confusion matrix of shape (num_classes, num_classes).\n\n    Returns:\n        dict: A dictionary containing per-class and global FPR and FNR.\n    \"\"\"\n    num_classes = cm.shape[0]\n    results = {\"per_class\": {}, \"global\": {}}\n\n    # Initialize variables for global calculation\n    total_TP = 0\n    total_FP = 0\n    total_FN = 0\n    total_TN = 0\n\n    # Per-class calculation\n    for class_idx in range(num_classes):\n        TP = cm[class_idx, class_idx]\n        FN = np.sum(cm[class_idx, :]) - TP\n        FP = np.sum(cm[:, class_idx]) - TP\n        TN = np.sum(cm) - (TP + FP + FN)\n\n        # Calculate FPR and FNR for this class\n        FPR = FP / (FP + TN) if (FP + TN) != 0 else None\n        FNR = FN / (TP + FN) if (TP + FN) != 0 else None\n\n        # Store per-class results\n        results[\"per_class\"][class_idx] = {\"FPR\": FPR, \"FNR\": FNR}\n\n        # Update global counts\n        total_TP += TP\n        total_FP += FP\n        total_FN += FN\n        total_TN += TN\n\n    # Global calculation\n    global_FPR = total_FP / \\\n        (total_FP + total_TN) if (total_FP + total_TN) != 0 else None\n    global_FNR = total_FN / \\\n        (total_FN + total_TP) if (total_FN + total_TP) != 0 else None\n\n    results[\"global\"][\"FPR\"] = global_FPR\n    results[\"global\"][\"FNR\"] = global_FNR\n\n    return results\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:40:28.794364Z","iopub.execute_input":"2025-10-27T10:40:28.795040Z","iopub.status.idle":"2025-10-27T10:40:28.800903Z","shell.execute_reply.started":"2025-10-27T10:40:28.795017Z","shell.execute_reply":"2025-10-27T10:40:28.800133Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\nclass NIDSDataModule(pl.LightningDataModule):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.batch_size = config.training.batch_size\n        self.sequence_length = config.training.sequence_length\n        self.num_workers = config.data.num_workers\n        self.oversample = config.training.oversample\n        self.non_numeric_cols=[]\n        self.scaler=None\n        self.alpha = 0.5\n\n    def prepare_data(self):\n        print(f\"--------------------- {name}-parquet -----------------------\")\n        if not os.path.exists(new_path):\n            print(\"--------------------- Centralities Calculations  -----------------------\")\n            if dataset_name==\"CIC_IDS_2017\":\n                df = pd.read_parquet(os.path.join('/kaggle/input/cic-ids-2017-parquet', self.config.data.raw))\n                # print(\"--------------------- cic-ids-2017-parquet -----------------------\")\n            elif dataset_name==\"CIC_TON_IOT\":\n                df = pd.read_parquet(os.path.join('/kaggle/input/cic-ton-iot-parquet', self.config.data.raw))\n                # print(\"--------------------- cic-ton-iot-parquet -----------------------\")\n            elif dataset_name==\"cic_ids_2017_5_percent\":\n                df = pd.read_parquet(os.path.join('/kaggle/input/cic-ids-2017-5-percent', self.config.data.raw))\n                # print(\"--------------------- /kaggle/input/cic-ids-2017-5-percent -----------------------\")\n    \n            print(\"Clean data\")\n            df.replace([np.inf, -np.inf], np.nan, inplace=True)\n            df.dropna(inplace=True)\n            df.drop_duplicates(inplace=True)\n            \n            print(\"Reset index after cleaning\")\n            df = df.reset_index(drop=True)\n            \n            print(\"Identify non-numeric columns\")\n            self.non_numeric_cols = ['Label', 'Timestamp', 'Flow ID', 'Src IP', \n                                     'Src Port', 'Attack', 'Dst IP', 'Dst Port', 'Protocol']\n            self.non_numeric_cols = [col for col in self.non_numeric_cols if col in df.columns]\n    \n    \n            print(\"Encode labels\")\n            self.label_encoder = LabelEncoder()\n            df['Label_Num'] = self.label_encoder.fit_transform(df['Attack'])\n            self.classes = self.label_encoder.classes_\n            \n            # print(\"Initialize scaler\")\n            # self.scaler = StandardScaler()\n            # CHANGE 1.01\n            # print(\"Calculate Centralities start\")\n            print(\"--------------------- Centralities Calculations starting-----------------------\")\n            df = add_centralities(\n                df= df,\n                new_path=new_path,\n                graph_path=graph_path,\n                dataset=dataset,\n                cn_measures=cn_measures,\n                network_features=network_features\n            )\n        else:\n            # print(new_path,\" exist now!\")\n            print(\"--------------------- Centralities Extractions  -----------------------\")\n            df = pd.read_parquet(new_path)\n        # CHANGES TO REMOVE use only for test with 5_perecent \n        if dataset_name==\"cic_ids_2017_5_percent\":\n            multiplier = 5  # change as you wish\n    \n            target_substring = \"Sql Injection\"\n            target_rows = df[df['Attack'].str.contains(target_substring, na=False)]\n            \n            if target_rows.empty:\n                print(f\"Warning: no rows found matching '{target_substring}' in train set.\")\n            else:\n                duplicated_block = pd.concat([target_rows] * (multiplier - 1), ignore_index=True)\n                df = pd.concat([df, duplicated_block], ignore_index=True)\n                print(f\"Duplicated target class rows by factor {multiplier}. New count:\",\n                  df['Attack'].str.contains(target_substring, na=False).sum())\n            \n            \n            \n        print(\"\\n===== Class Distribution =====\")\n        counts = df[\"Attack\"].value_counts()\n        print(counts)\n        # print()\n        print(\"Initialize scaler\")\n        self.scaler = StandardScaler()\n        print(\"Train/Val/Test split\")\n        train_df, test_df = train_test_split(\n            df,\n            test_size=1 - self.config.training.train_size,\n            random_state=42,\n            stratify=df['Label_Num']\n        )\n        val_df, test_df = train_test_split(\n            test_df,\n            test_size=0.5,\n            random_state=42,\n            stratify=test_df['Label_Num']\n        )\n        # Reset indices after splitting\n        train_df = train_df.reset_index(drop=True)\n        val_df = val_df.reset_index(drop=True)\n        test_df = test_df.reset_index(drop=True)\n        \n        # Use `fit=True` only for training data\n        self.X_train, self.y_train = self._prepare_features(train_df, fit=True)\n        self.X_val, self.y_val = self._prepare_features(val_df, fit=False)\n        self.X_test, self.y_test = self._prepare_features(test_df, fit=False)\n        # print(\"Val:\\n\", val_df['Label_Num'].value_counts(normalize=True).sort_index())\n        # print(\"Test:\\n\", test_df['Label_Num'].value_counts(normalize=True).sort_index())\n\n    def _prepare_features(self, df, fit=False):\n        X = df.drop(['Label_Num'] + self.non_numeric_cols, axis=1)\n        y = df['Label_Num']\n        if fit:\n            X = self.scaler.fit_transform(X)\n        else:\n            X = self.scaler.transform(X)\n        # return self.create_sequences(X, y)\n        return X, y\n        \n    def create_sequences(self, X, y):\n        sequences = []\n        labels = []\n        y_values = y.values if hasattr(y, 'values') else y  # Convert to numpy array if pandas Series\n        for i in range(len(X) - self.sequence_length):\n            seq = X[i:i+self.sequence_length]\n            sequences.append(seq)\n            labels.append(y_values[i+self.sequence_length-1])\n        return np.array(sequences), np.array(labels)\n       \n    def setup(self, stage=None):\n        self.scaler = StandardScaler()\n        # self.scaler.fit(self.X_train.reshape(-1, self.X_train.shape[-1]))\n        \n        # self.X_train = self.scaler.transform(self.X_train.reshape(-1, self.X_train.shape[-1])).reshape(self.X_train.shape)\n        # self.X_val = self.scaler.transform(self.X_val.reshape(-1, self.X_val.shape[-1])).reshape(self.X_val.shape)\n        # self.X_test = self.scaler.transform(self.X_test.reshape(-1, self.X_test.shape[-1])).reshape(self.X_test.shape)\n        \n        # self.train_dataset = TensorDataset(torch.FloatTensor(self.X_train), torch.LongTensor(self.y_train))\n        # self.val_dataset = TensorDataset(torch.FloatTensor(self.X_val), torch.LongTensor(self.y_val))\n        # self.test_dataset = TensorDataset(torch.FloatTensor(self.X_test), torch.LongTensor(self.y_test))\n\n        self.train_dataset = TimeSeriesDataset(\n            data=torch.FloatTensor(self.X_train),\n            sequence_length=self.sequence_length,\n            target_idx=torch.LongTensor(self.y_train)\n        )\n        \n        self.val_dataset = TimeSeriesDataset(\n            data=torch.FloatTensor(self.X_val),\n            sequence_length=self.sequence_length,\n            target_idx=torch.LongTensor(self.y_val)\n        )\n        \n        self.test_dataset = TimeSeriesDataset(\n            data=torch.FloatTensor(self.X_test),\n            sequence_length=self.sequence_length,\n            target_idx=torch.LongTensor(self.y_test)\n        )\n            \n    def train_dataloader(self):\n        if self.oversample:\n            class_counts = np.bincount(self.y_train)\n            weights = 1. / class_counts[self.y_train]\n            sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n        else:\n            sampler = RandomSampler(self.train_dataset)\n            \n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            sampler=sampler,\n            num_workers=self.num_workers,\n            persistent_workers=True, \n            # persistent_workers=False, # better for tpu\n            pin_memory=True\n        )\n    # c=0\n    def val_dataloader(self):\n        # print(c)\n        # c+=1\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n            # pin_memory=False # better for tpu\n        )\n    \n    def test_dataloader(self):\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n            # pin_memory=False # TPU \n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:40:29.132428Z","iopub.execute_input":"2025-10-27T10:40:29.132648Z","iopub.status.idle":"2025-10-27T10:40:29.149678Z","shell.execute_reply.started":"2025-10-27T10:40:29.132631Z","shell.execute_reply":"2025-10-27T10:40:29.148978Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import threading, time, psutil\n\ntry:\n    import pynvml\n    pynvml.nvmlInit()\n    GPU_AVAILABLE = True\nexcept:\n    GPU_AVAILABLE = False\n\ndef monitor_resources(process, peaks, interval=0.2):\n    \"\"\"Monitor CPU + GPU memory usage in a background thread.\"\"\"\n    peaks[\"cpu\"] = 0.0\n    peaks[\"gpu\"] = 0.0\n    \n    handle = None\n    if GPU_AVAILABLE:\n        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n    \n    while getattr(threading.current_thread(), \"running\", True):\n        try:\n            cpu_mem = process.memory_info().rss / (1024 ** 2)\n            gpu_mem = 0.0\n            if GPU_AVAILABLE:\n                gpu_mem = pynvml.nvmlDeviceGetMemoryInfo(handle).used / (1024 ** 2)\n            peaks[\"cpu\"] = max(peaks[\"cpu\"], cpu_mem)\n            peaks[\"gpu\"] = max(peaks[\"gpu\"], gpu_mem)\n        except psutil.NoSuchProcess:\n            break\n        time.sleep(interval)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:40:30.904234Z","iopub.execute_input":"2025-10-27T10:40:30.904731Z","iopub.status.idle":"2025-10-27T10:40:30.910200Z","shell.execute_reply.started":"2025-10-27T10:40:30.904708Z","shell.execute_reply":"2025-10-27T10:40:30.909646Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"warnings.filterwarnings('ignore')\n\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, data, sequence_length, target_idx=None):\n        \"\"\"\n        Dataset for time series data that creates sequences on-the-fly.\n \n        Args:\n            data: Input data tensor of shape (n_samples, n_features)\n            sequence_length: Length of sequences to create\n            target_idx: Optional tensor of target indices. If None, uses the last position in sequence\n        \"\"\"\n        self.data = data\n        self.sequence_length = sequence_length\n        self.target_idx = target_idx\n \n    def __len__(self):\n        return len(self.data)\n \n    def __getitem__(self, idx):\n        # Calculate start index for the sequence\n        start_idx = max(0, idx - self.sequence_length + 1)\n        \n        # Get the sequence\n        sequence = self.data[start_idx:idx + 1]\n        \n        # Ensure sequence is 2D [seq_len, features]\n        if sequence.dim() == 1:\n            sequence = sequence.unsqueeze(0)  # Add sequence length dimension if missing\n        \n        # Pad the beginning if needed\n        if len(sequence) < self.sequence_length:\n            padding = torch.zeros(self.sequence_length - len(sequence), sequence.shape[1])\n            sequence = torch.cat([padding, sequence], dim=0)\n        \n        # Get target\n        target = self.target_idx[idx] if self.target_idx is not None else -1\n        \n        return sequence, target\n\nclass AutoEncoderModel(pl.LightningModule):\n    def __init__(self, input_size, num_classes, config):\n        super().__init__()\n        self.save_hyperparameters({'config': config})\n        self.config = config\n        \n        self.encoder = nn.Sequential(\n            nn.Linear(input_size, 128),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n            nn.Linear(64, 32)\n        )\n\n        self.decoder = nn.Sequential(\n            nn.Linear(32, 64),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n            nn.Linear(64, 128),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n            nn.Linear(128, input_size)  # match input size for reconstruction\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Linear(32, 64),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n            nn.Linear(64, num_classes)\n        )\n\n        self.recon_loss = nn.MSELoss()\n        self.class_loss = nn.CrossEntropyLoss()\n\n        # Metrics\n        self.train_acc = torchmetrics.classification.Accuracy(task='multiclass', num_classes=num_classes)\n        self.val_acc = torchmetrics.classification.Accuracy(task='multiclass', num_classes=num_classes)\n        self.test_acc = torchmetrics.classification.Accuracy(task='multiclass', num_classes=num_classes)\n\n    def forward(self, x):\n        batch_size, seq_len, features = x.shape\n        x_flat = x.view(batch_size, -1)\n        z = self.encoder(x_flat)\n\n        x_hat = self.decoder(z).view(batch_size, seq_len, features)\n        logits = self.classifier(z)\n        return x_hat, logits\n    def on_train_epoch_end(self):\n        print(f\"✅ Finished Epoch {self.current_epoch+1}\")\n        \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        x_hat, logits = self(x)\n        loss_recon = self.recon_loss(x_hat, x)\n        loss_cls = self.class_loss(logits, y)\n        loss = loss_recon + loss_cls\n\n        preds = torch.argmax(logits, dim=1)\n        self.train_acc.update(preds, y)\n\n        self.log(\"train_loss_epoch\", loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"train_acc_epoch\", self.train_acc.compute() * 100, on_step=False, on_epoch=True, prog_bar=True)\n        # self.log(\"train_acc_epoch\", self.train_acc.compute()*100, on_step=False, on_epoch=True, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        x_hat, logits = self(x)\n        loss_recon = self.recon_loss(x_hat, x)\n        loss_cls = self.class_loss(logits, y)\n        loss = loss_recon + loss_cls\n\n        preds = torch.argmax(logits, dim=1)\n        self.val_acc.update(preds, y)\n\n        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n        # self.log(\"val_acc\", self.val_acc*100, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"val_acc\", self.val_acc.compute() * 100, on_step=False, on_epoch=True, prog_bar=True)\n        return loss\n\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        x_hat, logits = self(x)\n        loss_recon = self.recon_loss(x_hat, x)\n        # loss_cls = self.class_loss(logits, y)\n        loss = loss_recon \n        # loss = loss_recon + loss_cls\n\n        preds = torch.argmax(logits, dim=1)\n        self.test_acc.update(preds, y)\n\n        self.log(\"test_loss\", loss, on_step=False, on_epoch=True)\n        # print(\"loss: \",loss ,\".\")\n        # self.log(\"test_acc\", self.test_acc, on_step=False, on_epoch=True)\n        self.log(\"test_acc\", self.test_acc.compute() * 100, on_step=False, on_epoch=True, prog_bar=True)\n        return {\"loss\": loss}\n\n    def configure_optimizers(self):\n        return optim.AdamW(\n            self.parameters(),\n            lr=self.hparams.config.model.learning_rate,\n            weight_decay=self.hparams.config.model.weight_decay\n        )\n\ndef init_wandb():\n    user_secrets = UserSecretsClient()\n    wandb_api_key = user_secrets.get_secret(\"mohammad_wandb_secret\")\n    wandb.login(key=wandb_api_key)\n    \n    run = wandb.init(\n        project=config.wandb.project,\n        entity=config.wandb.entity,\n        tags=config.wandb.tags,\n        notes=config.wandb.notes,\n        config={\n            \"input_size\": None,\n            \"num_classes\": None,\n            \"sequence_length\": config.training.sequence_length,\n            # \"train_samples\": config.training.max_train_samples,\n            # \"val_samples\": config.training.max_val_samples,\n            # \"test_samples\": config.training.max_test_samples,\n            \"model_config\": dict(config.model),\n            \"training_config\": dict(config.training)\n        }\n    )\n    \n    wandb_logger = WandbLogger(\n        experiment=run,\n        log_model='all'\n    )\n    \n    return wandb_logger, run\n\ndef main():\n    # wandb_logger, run = init_wandb()\n    \n    data_module = NIDSDataModule(config)\n    data_module.prepare_data()\n    data_module.setup()\n    \n    sample_x, _ = next(iter(data_module.train_dataloader()))\n    input_size_per_timestep = sample_x.shape[2]  # Features per timestep\n    total_input_size = input_size_per_timestep * config.training.sequence_length\n    num_classes = len(data_module.classes)\n    \n    # run.config.update({\n    #     \"input_size_per_timestep\": input_size_per_timestep,\n    #     \"total_input_size\": total_input_size,\n    #     \"num_classes\": num_classes\n    # })\n    \n    model = AutoEncoderModel(total_input_size, num_classes, config)\n    \n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=config.training.early_stopping_patience,\n        mode='min'\n    )\n    \n    checkpoint_callback = ModelCheckpoint(\n        monitor='val_loss',\n        mode='min',\n        save_top_k=1,\n        dirpath='checkpoints',\n        filename='best_model'\n    )\n \n    trainer = pl.Trainer(\n        precision=16,\n        # logger=wandb_logger,\n        max_epochs=config.training.max_epochs,\n        callbacks=[early_stopping, checkpoint_callback],\n        deterministic=True,\n        gradient_clip_val=1.0,\n        enable_progress_bar=True,\n        log_every_n_steps=1000\n    )\n    \n    # start_time = time.time()\n    # process = psutil.Process()\n    print(\"------------- start training ---------------------\")\n    \n    # Measure start time and memory\n    cpu_mem_used=gpu_mem_used=elapsed_time=0\n    process = psutil.Process(os.getpid())\n    cpu_mem_before = process.memory_info().rss / (1024 ** 2)\n    handle = None\n    if GPU_AVAILABLE:\n        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n    if GPU_AVAILABLE:\n        gpu_mem_before = pynvml.nvmlDeviceGetMemoryInfo(handle).used / (1024 ** 2)\n    \n    process = psutil.Process(os.getpid())\n    peaks = {}\n    \n    monitor_thread = threading.Thread(target=monitor_resources, args=(process, peaks))\n    monitor_thread.running = True\n    monitor_thread.start()\n    \n    start_time = time.time()\n    # Train model\n    trainer.fit(model, datamodule=data_module)\n    print(\"------------- end training ---------------------\")\n    print(\"------------- start testing ---------------------\")\n    # Test model\n    test_results = trainer.test(model, datamodule=data_module)\n    \n    # End time and memory\n    elapsed_time = time.time() - start_time\n    # Stop thread safely\n    monitor_thread.running = False\n    monitor_thread.join()\n    \n    peak_cpu = peaks.get(\"cpu\", 0.0)\n    peak_gpu = peaks.get(\"gpu\", 0.0)\n    cpu_mem_used=peak_cpu-cpu_mem_before\n    gpu_mem_used=peak_gpu-gpu_mem_before\n    \n    # end_time = time.time()\n    # time_consumption = end_time - start_time\n    # memory_consumption_mb = process.memory_info().rss / (1024 * 1024)\n    \n    print(\"------------- end testing ---------------------\")\n \n    # Collect all predictions and targets for final evaluation\n    test_loader = data_module.test_dataloader()\n    all_preds = []\n    all_targets = []\n    all_probs = []  # <-- store probabilities here\n    criterion = torch.nn.CrossEntropyLoss()\n    test_loss_sum=0\n    total_samples=0\n    \n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            x, y = batch\n            _, logits = model(x)  # unpack tuple\n            preds = torch.argmax(logits, dim=1)\n            probs = torch.softmax(logits, dim=1).cpu().numpy()\n            \n            loss = criterion(logits, y)  # use logits for classification loss\n            test_loss_sum += loss.item() * x.size(0)\n            total_samples += x.size(0)\n            \n            all_probs.extend(probs)\n            all_preds.extend(preds.cpu().numpy())\n            all_targets.extend(y.cpu().numpy())  \n    class_names = data_module.classes.tolist()\n    # metrics calculation\n    print(\"------------- metric calculation ---------------------\")\n    test_acc = accuracy_score(all_targets, all_preds)\n    test_f1 = f1_score(all_targets, all_preds, average='weighted')\n    test_recall = recall_score(all_targets, all_preds, average='weighted')\n    test_precision = precision_score(all_targets, all_preds, average='weighted')\n    test_loss_OLD = test_results[0]['test_loss'] if 'test_loss' in test_results[0] else None\n    test_loss = test_loss_sum / total_samples\n    print(test_loss_OLD)\n    print(test_loss)\n\n    all_probs = np.array(all_probs)\n    \n    try:\n        if all_probs.shape[1] == 2:\n            # Binary classification → probability of positive class\n            auc_score = roc_auc_score(all_targets, all_probs[:, 1])\n        else:\n            # Multi-class classification\n            auc_score = roc_auc_score(all_targets, all_probs, multi_class='ovr')\n    except Exception as e:\n        print(f\"AUC computation failed: {e}\")\n        auc_score = None\n\n    \n    # False Positive / False Negative Rate\n    cm = confusion_matrix(all_targets, all_preds)\n    cr = classification_report(\n        all_targets, all_preds, digits=4, output_dict=True, zero_division=0)\n    report = classification_report(\n        all_targets, all_preds, digits=4, output_dict=False, zero_division=0)\n\n        \n    FP = cm.sum(axis=0) - np.diag(cm)\n    FN = cm.sum(axis=1) - np.diag(cm)\n    TP = np.diag(cm)\n    TN = cm.sum() - (FP + FN + TP)\n    fpr = FP.sum() / (FP.sum() + TN.sum())\n    fnr = FN.sum() / (FN.sum() + TP.sum())\n    # --- Per-class FPR/FNR ---\n    per_class_fpr = np.where((FP + TN) > 0, FP / (FP + TN), 0.0)\n    per_class_fnr = np.where((FN + TP) > 0, FN / (FN + TP), 0.0)\n    \n    # Convert to dict with class names\n    fpr_dict = {class_names[i]: per_class_fpr[i] for i in range(len(class_names))}\n    fnr_dict = {class_names[i]: per_class_fnr[i] for i in range(len(class_names))}\n\n    metrics_dict = {\n        \"Test Accuracy\": test_acc,\n        \"F1 Score\": test_f1,\n        \"Test Loss\": test_loss,\n        \"Recall\": test_recall,\n        \"Precision\": test_precision,\n        \"AUC\": auc_score,\n        \"False Positive Rate\": fpr,\n        \"False Negative Rate\": fnr,\n        \"Time Consumption (s)\": elapsed_time,\n        # \"Memory Consumption (MB)\": memory_consumption_mb\n        \"CPU_Peak_MB\": peak_cpu,\n        \"GPU_Peak_MB\": peak_gpu,\n        \"cpu_mem_used\":cpu_mem_used,\n        \"gpu_mem_used\":gpu_mem_used,\n    }\n    # Print metrics\n    print(\"------------- final evaluation metric ---------------------\")\n    for k, v in metrics_dict.items():\n        print(f\"{k}: {v}\")\n\n    metrics_df = pd.DataFrame([metrics_dict])\n\n\n\n    class_names = [str(c) for c in data_module.label_encoder.classes_]\n    print(\"--------------- class names --------------------\")\n    print(class_names)\n    # Generate classification report dict once\n    report = classification_report(\n        all_targets,\n        all_preds,\n        target_names=class_names,\n        output_dict=True\n    )\n    for i, cls in enumerate(class_names):\n        report[cls][\"FPR\"] = per_class_fpr[i]\n        report[cls][\"FNR\"] = per_class_fnr[i]\n\n    # Optionally, also store global ones\n    report[\"macro avg\"][\"FPR\"] = fpr\n    report[\"macro avg\"][\"FNR\"] = fnr\n    report[\"weighted avg\"][\"FPR\"] = np.average(per_class_fpr, weights=cm.sum(axis=1))\n    report[\"weighted avg\"][\"FNR\"] = np.average(per_class_fnr, weights=cm.sum(axis=1))\n        \n    print(\"Classification Report:\")\n    print(classification_report(all_targets, all_preds, target_names=class_names))\n    \n    print(\"Confusion Matrix:\")\n    print(confusion_matrix(all_targets, all_preds))\n    \n    \n    print(\"------------- classification report dict -------------\")\n    print(report)\n    final_report = pd.DataFrame(report).transpose()\n    metrics_df = pd.DataFrame([metrics_dict])\n    merged = pd.concat([final_report, metrics_df], axis=0)\n    \n    merged.to_csv(os.path.join(\"/kaggle/working/\",f\"AE_{dataset_name}__NODEFEATURES-{not NO_NODE_FEATURE}__report.csv\"))\n    # Create a wandb Table for the classification report\n    # report_table = wandb.Table(columns=[\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"])\n    # for class_name in class_names:\n    #     report_table.add_data(\n    #         class_name,\n    #         report[class_name][\"precision\"],\n    #         report[class_name][\"recall\"],\n    #         report[class_name][\"f1-score\"],\n    #         report[class_name][\"support\"]\n    #     )\n    \n    # # Add weighted averages\n    # report_table.add_data(\n    #     \"Weighted Avg\",\n    #     report[\"weighted avg\"][\"precision\"],\n    #     report[\"weighted avg\"][\"recall\"],\n    #     report[\"weighted avg\"][\"f1-score\"],\n    #     report[\"weighted avg\"][\"support\"]\n    # )\n    # print(\"empty\")\n    # wandb.log({\"classification_report\": report_table})\n    \n    # Finish wandb run\n    # wandb.finish()\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:40:31.530714Z","iopub.execute_input":"2025-10-27T10:40:31.531246Z","iopub.status.idle":"2025-10-27T11:22:28.921873Z","shell.execute_reply.started":"2025-10-27T10:40:31.531223Z","shell.execute_reply":"2025-10-27T11:22:28.921161Z"}},"outputs":[{"name":"stdout","text":"--------------------- cic_ids_2017-parquet -----------------------\n--------------------- Centralities Calculations  -----------------------\nClean data\nReset index after cleaning\nIdentify non-numeric columns\nEncode labels\n--------------------- Centralities Calculations starting-----------------------\ncalculated betweenness\ncalculated degree\ncalculated closeness\ncalculated pagerank\ncalculated k_truss\n==>> features_dicts: ('betweenness', 19129)\n==>> features_dicts: ('degree', 19129)\n==>> features_dicts: ('pagerank', 19129)\n==>> features_dicts: ('closeness', 19129)\n==>> features_dicts: ('k_truss', 19129)\n--------------------------  writting the DataFrame to /kaggle/working/cic_ids_2017_with_centralities.parquet ----------------------\n--------------------------DataFrame written to /kaggle/working/cic_ids_2017_with_centralities.parquet --------------------------\n\n===== Class Distribution =====\nAttack\nBENIGN                        2271122\nDoS Hulk                       230123\nPortScan                       158804\nDDoS                           128025\nDoS GoldenEye                   10293\nFTP-Patator                      7935\nSSH-Patator                      5897\nDoS slowloris                    5796\nDoS Slowhttptest                 5499\nBot                              1956\nWeb Attack � Brute Force         1507\nWeb Attack � XSS                  652\nInfiltration                       36\nWeb Attack � Sql Injection         21\nHeartbleed                         11\nName: count, dtype: int64\nInitialize scaler\nTrain/Val/Test split\n------------- start training ---------------------\n--------------------- cic_ids_2017-parquet -----------------------\n--------------------- Centralities Extractions  -----------------------\n\n===== Class Distribution =====\nAttack\nBENIGN                        2271122\nDoS Hulk                       230123\nPortScan                       158804\nDDoS                           128025\nDoS GoldenEye                   10293\nFTP-Patator                      7935\nSSH-Patator                      5897\nDoS slowloris                    5796\nDoS Slowhttptest                 5499\nBot                              1956\nWeb Attack � Brute Force         1507\nWeb Attack � XSS                  652\nInfiltration                       36\nWeb Attack � Sql Injection         21\nHeartbleed                         11\nName: count, dtype: int64\nInitialize scaler\nTrain/Val/Test split\n","output_type":"stream"},{"name":"stderr","text":"2025-10-27 10:50:30.131673: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761562230.372645      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761562230.451268      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89774b938a294225a4f1473b202ba93d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✅ Finished Epoch 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✅ Finished Epoch 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✅ Finished Epoch 3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✅ Finished Epoch 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✅ Finished Epoch 5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✅ Finished Epoch 6\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✅ Finished Epoch 7\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✅ Finished Epoch 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✅ Finished Epoch 9\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✅ Finished Epoch 10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✅ Finished Epoch 11\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✅ Finished Epoch 12\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✅ Finished Epoch 13\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✅ Finished Epoch 14\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✅ Finished Epoch 15\n------------- end training ---------------------\n------------- start testing ---------------------\n--------------------- cic_ids_2017-parquet -----------------------\n--------------------- Centralities Extractions  -----------------------\n\n===== Class Distribution =====\nAttack\nBENIGN                        2271122\nDoS Hulk                       230123\nPortScan                       158804\nDDoS                           128025\nDoS GoldenEye                   10293\nFTP-Patator                      7935\nSSH-Patator                      5897\nDoS slowloris                    5796\nDoS Slowhttptest                 5499\nBot                              1956\nWeb Attack � Brute Force         1507\nWeb Attack � XSS                  652\nInfiltration                       36\nWeb Attack � Sql Injection         21\nHeartbleed                         11\nName: count, dtype: int64\nInitialize scaler\nTrain/Val/Test split\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d18208317aad41c1bafe9c06df92191e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    97.0942611694336     \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1016428470611572    \u001b[0m\u001b[35m \u001b[0m│\n└───────────────────────────┴───────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     97.0942611694336      </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1016428470611572     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"------------- end testing ---------------------\n------------- metric calculation ---------------------\n1.1016428470611572\n0.07708910028137783\n------------- final evaluation metric ---------------------\nTest Accuracy: 0.9714111922141119\nF1 Score: 0.9837784811359098\nTest Loss: 0.07708910028137783\nRecall: 0.9714111922141119\nPrecision: 0.9973584853024942\nAUC: 0.9829646048363955\nFalse Positive Rate: 0.0020420576989920057\nFalse Negative Rate: 0.028588807785888078\nTime Consumption (s): 1919.323121547699\nCPU_Peak_MB: 15334.05859375\nGPU_Peak_MB: 521.875\ncpu_mem_used: 8371.796875\ngpu_mem_used: 154.0\n--------------- class names --------------------\n['BENIGN', 'Bot', 'DDoS', 'DoS GoldenEye', 'DoS Hulk', 'DoS Slowhttptest', 'DoS slowloris', 'FTP-Patator', 'Heartbleed', 'Infiltration', 'PortScan', 'SSH-Patator', 'Web Attack � Brute Force', 'Web Attack � Sql Injection', 'Web Attack � XSS']\nClassification Report:\n                            precision    recall  f1-score   support\n\n                    BENIGN       1.00      0.97      0.98    340668\n                       Bot       0.03      0.99      0.05       293\n                      DDoS       1.00      1.00      1.00     19204\n             DoS GoldenEye       0.96      0.99      0.98      1544\n                  DoS Hulk       1.00      1.00      1.00     34518\n          DoS Slowhttptest       0.91      0.98      0.94       825\n             DoS slowloris       0.89      0.99      0.94       870\n               FTP-Patator       0.87      0.99      0.93      1190\n                Heartbleed       0.18      1.00      0.31         2\n              Infiltration       0.02      0.50      0.03         6\n                  PortScan       1.00      0.99      1.00     23820\n               SSH-Patator       0.91      0.99      0.95       885\n  Web Attack � Brute Force       0.45      0.77      0.57       226\nWeb Attack � Sql Injection       0.00      0.00      0.00         3\n          Web Attack � XSS       0.33      0.22      0.27        98\n\n                  accuracy                           0.97    424152\n                 macro avg       0.64      0.83      0.66    424152\n              weighted avg       1.00      0.97      0.98    424152\n\nConfusion Matrix:\n[[329034  10998      5     16     72     71     32    133      8    176\n       9     54     58      1      1]\n [     4    289      0      0      0      0      0      0      0      0\n       0      0      0      0      0]\n [     1      0  19163      8     27      1      1      1      0      0\n       1      1      0      0      0]\n [     0      0      0   1535      2      1      3      0      0      0\n       0      0      0      2      1]\n [     1      0     13     29  34435      1      8      4      1      0\n       0      2     11      0     13]\n [     0      0      0      2      1    808      9      1      0      0\n       0      0      4      0      0]\n [     0      0      0      1      0      9    858      1      0      0\n       0      0      1      0      0]\n [     0      0      0      0      1      0      9   1178      0      0\n       0      1      1      0      0]\n [     0      0      0      0      0      0      0      0      2      0\n       0      0      0      0      0]\n [     2      1      0      0      0      0      0      0      0      3\n       0      0      0      0      0]\n [     1      1     13      4     10      0     37     34      0      0\n   23648      7     64      0      1]\n [     0      0      0      2      0      0      2      4      0      0\n       0    876      1      0      0]\n [     0      0      0      0      0      0      0      0      0      0\n       0     23    175      0     28]\n [     0      0      0      2      0      0      0      0      0      0\n       0      1      0      0      0]\n [     0      0      0      0      3      0      0      0      0      0\n       0      1     72      0     22]]\n------------- classification report dict -------------\n{'BENIGN': {'precision': 0.9999726479517874, 'recall': 0.965849448730142, 'f1-score': 0.9826148891088843, 'support': 340668, 'FPR': 0.00010780508840017249, 'FNR': 0.03415055126985805}, 'Bot': {'precision': 0.025600141730888475, 'recall': 0.9863481228668942, 'f1-score': 0.04990502503885339, 'support': 293, 'FPR': 0.02595202649937833, 'FNR': 0.013651877133105802}, 'DDoS': {'precision': 0.9983849119516516, 'recall': 0.9978650281191418, 'f1-score': 0.9981249023386635, 'support': 19204, 'FPR': 7.655303890869939e-05, 'FNR': 0.0021349718808581545}, 'DoS GoldenEye': {'precision': 0.9599749843652282, 'recall': 0.9941709844559585, 'f1-score': 0.9767737830098631, 'support': 1544, 'FPR': 0.00015144057850300987, 'FNR': 0.005829015544041451}, 'DoS Hulk': {'precision': 0.9966426442071141, 'recall': 0.9975954574424938, 'f1-score': 0.9971188232057797, 'support': 34518, 'FPR': 0.00029771529178665104, 'FNR': 0.0024045425575062286}, 'DoS Slowhttptest': {'precision': 0.9068462401795735, 'recall': 0.9793939393939394, 'f1-score': 0.9417249417249417, 'support': 825, 'FPR': 0.00019606592539573426, 'FNR': 0.020606060606060607}, 'DoS slowloris': {'precision': 0.894681960375391, 'recall': 0.9862068965517241, 'f1-score': 0.9382176052487697, 'support': 870, 'FPR': 0.0002386116111717484, 'FNR': 0.013793103448275862}, 'FTP-Patator': {'precision': 0.8687315634218289, 'recall': 0.9899159663865547, 'f1-score': 0.9253731343283582, 'support': 1190, 'FPR': 0.00042084158860606865, 'FNR': 0.010084033613445379}, 'Heartbleed': {'precision': 0.18181818181818182, 'recall': 1.0, 'f1-score': 0.3076923076923077, 'support': 2, 'FPR': 2.1218908405045385e-05, 'FNR': 0.0}, 'Infiltration': {'precision': 0.01675977653631285, 'recall': 0.5, 'f1-score': 0.03243243243243243, 'support': 6, 'FPR': 0.00041495145539507624, 'FNR': 0.5}, 'PortScan': {'precision': 0.9995773100008454, 'recall': 0.9927791771620487, 'f1-score': 0.9961666456042799, 'support': 23820, 'FPR': 2.497926720821718e-05, 'FNR': 0.007220822837951301}, 'SSH-Patator': {'precision': 0.906832298136646, 'recall': 0.9898305084745763, 'f1-score': 0.9465153970826581, 'support': 885, 'FPR': 0.00021263174308415255, 'FNR': 0.010169491525423728}, 'Web Attack � Brute Force': {'precision': 0.45219638242894056, 'recall': 0.7743362831858407, 'f1-score': 0.5709624796084829, 'support': 226, 'FPR': 0.000500087279383666, 'FNR': 0.22566371681415928}, 'Web Attack � Sql Injection': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 3, 'FPR': 7.072986144020144e-06, 'FNR': 1.0}, 'Web Attack � XSS': {'precision': 0.3333333333333333, 'recall': 0.22448979591836735, 'f1-score': 0.2682926829268293, 'support': 98, 'FPR': 0.00010376037014153858, 'FNR': 0.7755102040816326}, 'accuracy': 0.9714111922141119, 'macro avg': {'precision': 0.6360901584291816, 'recall': 0.8252521072458454, 'f1-score': 0.6621276699567401, 'support': 424152, 'FPR': 0.0020420576989920057, 'FNR': 0.028588807785888078}, 'weighted avg': {'precision': 0.9973584853024942, 'recall': 0.9714111922141119, 'f1-score': 0.9837784811359098, 'support': 424152, 'FPR': 0.00013695384602405335, 'FNR': 0.02858880778588808}}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}