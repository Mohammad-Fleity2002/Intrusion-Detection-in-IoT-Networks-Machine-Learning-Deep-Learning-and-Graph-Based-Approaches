{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8089266,"sourceType":"datasetVersion","datasetId":4775518},{"sourceId":8089281,"sourceType":"datasetVersion","datasetId":4775527},{"sourceId":12650227,"sourceType":"datasetVersion","datasetId":7994437},{"sourceId":470644,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":379672,"modelId":399557},{"sourceId":593572,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":444208,"modelId":460697}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1>AutoEncoderDecoder</h1>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader,Dataset ,TensorDataset, WeightedRandomSampler, RandomSampler\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nimport wandb\nfrom omegaconf import OmegaConf\nimport os\nimport torchmetrics\nimport warnings\nfrom kaggle_secrets import UserSecretsClient\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport time\nimport psutil\nfrom sklearn.metrics import roc_auc_score, precision_score, recall_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T12:12:00.059981Z","iopub.execute_input":"2025-10-04T12:12:00.060741Z","iopub.status.idle":"2025-10-04T12:12:25.568363Z","shell.execute_reply.started":"2025-10-04T12:12:00.060718Z","shell.execute_reply":"2025-10-04T12:12:25.567773Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import networkx as nx\nimport igraph as ig\n# change 1.02\n# import /kaggle/input/centrality_network/pytorch/default/1 as network\nimport sys\nsys.path.append(\"/kaggle/input/centrality_network/pytorch/default/1\")\n\nfrom network_features import separate_graph, cal_betweenness_centrality, cal_k_core, cal_k_truss\nfrom CommCentralityCode import comm_centreality\nfrom modularity_vitality import modularity_vitality\n\nsys.path.append(\"/kaggle/input/githubrepofiles/pytorch/default/1\")\nfrom src.dataset.dataset_info import datasets\n\n# change 1.04\ndef add_centralities(df, new_path, graph_path, dataset, cn_measures, network_features):\n# def add_centralities(df, new_path, graph_path, dataset, cn_measures):\n        # change 1.05\n    # G = nx.from_pandas_edgelist(df, source=\"Src IP\",target=\"Dst IP\", create_using=nx.DiGraph())\n    G = nx.from_pandas_edgelist( df, source=dataset.src_ip_col, target=dataset.dst_ip_col, create_using=nx.DiGraph())\n    G.remove_nodes_from(list(nx.isolates(G)))\n    for node in G.nodes():\n        G.nodes[node]['label'] = node\n\n    G1 = ig.Graph.from_networkx(G)\n    labels = [G.nodes[node]['label'] for node in G.nodes()]\n    G1.vs['label'] = labels\n\n    part = G1.community_infomap()\n    communities = []\n    for com in part:\n        communities.append([G1.vs[node_index]['label'] for node_index in com])\n\n    community_labels = {}\n    for i, community in enumerate(communities):\n        for node in community:\n            community_labels[node] = i\n\n    nx.set_node_attributes(G, community_labels, \"new_community\")\n\n    intra_graph, inter_graph = separate_graph(G, communities)\n\n    if \"betweenness\" in cn_measures:\n        nx.set_node_attributes(G, cal_betweenness_centrality(G), \"betweenness\")\n        print(\"calculated betweenness\")\n    if \"local_betweenness\" in cn_measures:\n        nx.set_node_attributes(G, cal_betweenness_centrality(\n            intra_graph), \"local_betweenness\")\n        print(\"calculated local_betweenness\")\n    if \"global_betweenness\" in cn_measures:\n        nx.set_node_attributes(G, cal_betweenness_centrality(\n            inter_graph), \"global_betweenness\")\n        print(\"calculated global_betweenness\")\n    if \"degree\" in cn_measures:\n        nx.set_node_attributes(G, nx.degree_centrality(G), \"degree\")\n        print(\"calculated degree\")\n    if \"local_degree\" in cn_measures:\n        nx.set_node_attributes(\n            G, nx.degree_centrality(intra_graph), \"local_degree\")\n        print(\"calculated local_degree\")\n    if \"global_degree\" in cn_measures:\n        nx.set_node_attributes(G, nx.degree_centrality(\n            inter_graph), \"global_degree\")\n        print(\"calculated global_degree\")\n    if \"eigenvector\" in cn_measures:\n        nx.set_node_attributes(G, nx.eigenvector_centrality(\n            G, max_iter=600), \"eigenvector\")\n        print(\"calculated eigenvector\")\n    if \"local_eigenvector\" in cn_measures:\n        nx.set_node_attributes(G, nx.eigenvector_centrality(\n            intra_graph), \"local_eigenvector\")\n        print(\"calculated local_eigenvector\")\n    if \"global_eigenvector\" in cn_measures:\n        nx.set_node_attributes(G, nx.eigenvector_centrality(\n            inter_graph), \"global_eigenvector\")\n        print(\"calculated global_eigenvector\")\n    if \"closeness\" in cn_measures:\n        nx.set_node_attributes(G, nx.closeness_centrality(G), \"closeness\")\n        print(\"calculated closeness\")\n    if \"local_closeness\" in cn_measures:\n        nx.set_node_attributes(G, nx.closeness_centrality(\n            intra_graph), \"local_closeness\")\n        print(\"calculated local_closeness\")\n    if \"global_closeness\" in cn_measures:\n        nx.set_node_attributes(G, nx.closeness_centrality(\n            inter_graph), \"global_closeness\")\n        print(\"calculated global_closeness\")\n    if \"pagerank\" in cn_measures:\n        nx.set_node_attributes(G, nx.pagerank(G, alpha=0.85), \"pagerank\")\n        print(\"calculated pagerank\")\n    if \"local_pagerank\" in cn_measures:\n        nx.set_node_attributes(G, nx.pagerank(\n            intra_graph, alpha=0.85), \"local_pagerank\")\n        print(\"calculated local_pagerank\")\n    if \"global_pagerank\" in cn_measures:\n        nx.set_node_attributes(G, nx.pagerank(\n            inter_graph, alpha=0.85), \"global_pagerank\")\n        print(\"calculated global_pagerank\")\n    if \"k_core\" in cn_measures:\n        nx.set_node_attributes(G, cal_k_core(G), \"k_core\")\n        print(\"calculated k_core\")\n    if \"k_truss\" in cn_measures:\n        nx.set_node_attributes(G, cal_k_truss(G), \"k_truss\")\n        print(\"calculated k_truss\")\n    if \"Comm\" in cn_measures:\n        nx.set_node_attributes(\n            G, comm_centreality(G, community_labels), \"Comm\")\n        print(\"calculated Comm\")\n    if \"mv\" in cn_measures:\n        nx.set_node_attributes(G, modularity_vitality(G1, part), \"mv\")\n        print(\"calculated mv\")\n\n    # nx.write_gexf(G, graph_path)\n\n    features_dicts = {}\n    for measure in cn_measures:\n        features_dicts[measure] = nx.get_node_attributes(G, measure)\n        print(f\"==>> features_dicts: {measure , len(features_dicts[measure])}\")\n\n    for feature in network_features:\n        if feature[:3] == \"src\":\n            df[feature] = df.apply(lambda row: features_dicts[feature[4:]].get(\n                row[dataset.src_ip_col], -1), axis=1)\n            # df[feature] = df.apply(lambda row: features_dicts[feature[4:]].get(\n                # row['Src Ip'], -1), axis=1)\n        if feature[:3] == \"dst\":\n            df[feature] = df.apply(lambda row: features_dicts[feature[4:]].get(\n                row[dataset.dst_ip_col], -1), axis=1)\n            # df[feature] = df.apply(lambda row: features_dicts[feature[4:]].get(row['Dst IP'], -1), axis=1)\n    print(f\"--------------------------  writting the DataFrame to {new_path} ----------------------\")\n    df.to_parquet(new_path)\n    print(f\"--------------------------DataFrame written to {new_path} --------------------------\")\n    # print(df.columns)\n    # return network_features\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T12:12:25.569451Z","iopub.execute_input":"2025-10-04T12:12:25.569858Z","iopub.status.idle":"2025-10-04T12:12:26.339469Z","shell.execute_reply.started":"2025-10-04T12:12:25.569824Z","shell.execute_reply":"2025-10-04T12:12:26.338457Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# name = \"cic_ton_iot_5_percent\"\nname = \"cic_ton_iot\"\n# name = \"cic_ids_2017_5_percent\"\n# name = \"cic_ids_2017\"\n\ndataset = datasets[name]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T12:12:26.340185Z","iopub.execute_input":"2025-10-04T12:12:26.340700Z","iopub.status.idle":"2025-10-04T12:12:26.344056Z","shell.execute_reply.started":"2025-10-04T12:12:26.340679Z","shell.execute_reply":"2025-10-04T12:12:26.343551Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"warnings.filterwarnings('ignore')\n\n# CHANGE 1.02\nnew_path = os.path.join(\"/kaggle/working/\",f\"{name}_with_centralities.parquet\")\ngraph_path = os.path.join(\"/kaggle/working/\",f\"{name}_graph.gpickle\")\n# cn_measures = [\"degree\", \"betweenness\", \"closeness\", \"eigenvector\"]\ncn_measures = [\"betweenness\", \"degree\", \"pagerank\", \"closeness\", \"k_truss\"]\nnetwork_features = ['src_betweenness', 'dst_betweenness', 'src_degree', 'dst_degree', 'src_pagerank', 'dst_pagerank', 'src_closeness', 'dst_closeness', 'src_k_truss', 'dst_k_truss']\nif name==\"cic_ids_2017\":\n    # Optimized Configuration\n    config = OmegaConf.create({\n        \"wandb\": {\n            \"project\": \"DL-NIDS-2--cic-ids-2017\",\n            \"entity\": \"mohammad-fleity-lebanese-university\",\n            \"tags\": [\"AutoEncoderDecoder\", \"CIC-IDS-2017\", \"PyTorch\"],\n            \"notes\": \"Optimized AutoEncoderDecoder for network intrusion detection\",\n        },\n        \"model\": {\n            \"hidden_size\": 128,          # Increased capacity\n            \"num_layers\": 2,             # Deeper network\n            \"dropout\": 0.4,              # Stronger regularization\n            \"dense_units\": [128, 64],    # Better feature extraction\n            \"learning_rate\": 0.0001,     # Slower learning\n            \"weight_decay\": 1e-4         # Stronger L2 regularization\n        },\n        \"training\": {\n            \"sequence_length\": 5,        # Longer temporal context\n            \"batch_size\": 256,           # Larger batches\n            \"max_epochs\": 15,            # More training time\n            # \"max_epochs\": 1,            # More training time\n            \"early_stopping_patience\": 7,# More patience\n            \"oversample\": True,          # Class balancing\n            \"gpus\": 1 if torch.cuda.is_available() else 0,\n            \"train_size\": 0.7,           # Proper train/val split\n            \"val_size\": 0.15             # 70/15/15 split\n        },\n        \"data\": {\n            \"raw\": \"cic_ids_2017.parquet\",\n            \"num_workers\": 4\n        }\n    })\n    dataset_name=\"CIC_IDS_2017\"\nelif name ==\"cic_ton_iot\":\n    config = OmegaConf.create({\n        \"wandb\": {\n            \"project\": \"DL-NIDS-2--cic-ton-iot\",\n            \"entity\": \"mohammad-fleity-lebanese-university\",\n            \"tags\": [\"AutoEncoderDecoder\", \"CIC-TON-IOT\", \"PyTorch\"],\n            \"notes\": \"Optimized AutoEncoderDecoder for network intrusion detection\"\n        },\n        \"model\": {\n            \"hidden_size\": 128,          # Increased capacity\n            \"num_layers\": 2,             # Deeper network\n            \"dropout\": 0.4,              # Stronger regularization\n            \"dense_units\": [128, 64],    # Better feature extraction\n            \"learning_rate\": 0.0001,     # Slower learning\n            \"weight_decay\": 1e-4         # Stronger L2 regularization\n        },\n        \"training\": {\n            \"sequence_length\": 5,        # Longer temporal context\n            \"batch_size\": 256,           # Larger batches\n            \"max_epochs\": 15,            # More training time\n            # \"max_epochs\": 1,            # More training time\n            \"early_stopping_patience\": 7,# More patience\n            \"oversample\": True,          # Class balancing\n            \"gpus\": 1 if torch.cuda.is_available() else 0,\n            \"train_size\": 0.7,           # Proper train/val split\n            \"val_size\": 0.15             # 70/15/15 split\n        },\n        \"data\": {\n            \"raw\": \"cic_ton_iot.parquet\",\n            \"num_workers\": 4\n        }\n    })\n    dataset_name=\"CIC_TON_IOT\"\n    \nelif name ==\"cic_ids_2017_5_percent\": # THIS IS JUST FOR TESTING THE FUNCTIONALITIES FASTER\n    config = OmegaConf.create({\n        \"wandb\": {\n            \"project\": \"DL-NIDS-2--cic-ton-iot\",\n            \"entity\": \"mohammad-fleity-lebanese-university\",\n            \"tags\": [\"AutoEncoderDecoder\", \"CIC-TON-IOT\", \"PyTorch\"],\n            \"notes\": \"Optimized AutoEncoderDecoder for network intrusion detection\"\n        },\n        \"model\": {\n            \"hidden_size\": 128,          # Increased capacity\n            \"num_layers\": 2,             # Deeper network\n            \"dropout\": 0.4,              # Stronger regularization\n            \"dense_units\": [128, 64],    # Better feature extraction\n            \"learning_rate\": 0.0001,     # Slower learning\n            \"weight_decay\": 1e-4         # Stronger L2 regularization\n        },\n        \"training\": {\n            \"sequence_length\": 5,        # Longer temporal context\n            \"batch_size\": 256,           # Larger batches\n            \"max_epochs\": 1,            # More training time\n            \"early_stopping_patience\": 7,# More patience\n            \"oversample\": True,          # Class balancing\n            \"gpus\": 1 if torch.cuda.is_available() else 0,\n            \"train_size\": 0.7,           # Proper train/val split\n            \"val_size\": 0.15             # 70/15/15 split\n        },\n        \"data\": {\n            \"raw\": \"cic_ids_2017_5_percent.parquet\",\n            \"num_workers\": 4\n        }\n    })\n    dataset_name=\"cic_ids_2017_5_percent\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T12:12:26.345699Z","iopub.execute_input":"2025-10-04T12:12:26.345900Z","iopub.status.idle":"2025-10-04T12:12:26.366652Z","shell.execute_reply.started":"2025-10-04T12:12:26.345883Z","shell.execute_reply":"2025-10-04T12:12:26.366136Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def calculate_fpr_fnr_with_global(cm):\n    \"\"\"\n    Calculate FPR and FNR for each class and globally for a multi-class confusion matrix.\n\n    Parameters:\n        cm (numpy.ndarray): Confusion matrix of shape (num_classes, num_classes).\n\n    Returns:\n        dict: A dictionary containing per-class and global FPR and FNR.\n    \"\"\"\n    num_classes = cm.shape[0]\n    results = {\"per_class\": {}, \"global\": {}}\n\n    # Initialize variables for global calculation\n    total_TP = 0\n    total_FP = 0\n    total_FN = 0\n    total_TN = 0\n\n    # Per-class calculation\n    for class_idx in range(num_classes):\n        TP = cm[class_idx, class_idx]\n        FN = np.sum(cm[class_idx, :]) - TP\n        FP = np.sum(cm[:, class_idx]) - TP\n        TN = np.sum(cm) - (TP + FP + FN)\n\n        # Calculate FPR and FNR for this class\n        FPR = FP / (FP + TN) if (FP + TN) != 0 else None\n        FNR = FN / (TP + FN) if (TP + FN) != 0 else None\n\n        # Store per-class results\n        results[\"per_class\"][class_idx] = {\"FPR\": FPR, \"FNR\": FNR}\n\n        # Update global counts\n        total_TP += TP\n        total_FP += FP\n        total_FN += FN\n        total_TN += TN\n\n    # Global calculation\n    global_FPR = total_FP / \\\n        (total_FP + total_TN) if (total_FP + total_TN) != 0 else None\n    global_FNR = total_FN / \\\n        (total_FN + total_TP) if (total_FN + total_TP) != 0 else None\n\n    results[\"global\"][\"FPR\"] = global_FPR\n    results[\"global\"][\"FNR\"] = global_FNR\n\n    return results\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T12:12:26.367560Z","iopub.execute_input":"2025-10-04T12:12:26.367739Z","iopub.status.idle":"2025-10-04T12:12:26.390309Z","shell.execute_reply.started":"2025-10-04T12:12:26.367724Z","shell.execute_reply":"2025-10-04T12:12:26.389767Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# class NIDSDataModule(pl.LightningDataModule):\n#     def __init__(self, config):\n#         super().__init__()\n#         self.config = config\n#         self.batch_size = config.training.batch_size\n#         self.sequence_length = config.training.sequence_length\n#         self.num_workers = config.data.num_workers\n#         self.oversample = config.training.oversample\n#         self.max_train_samples = config.training.max_train_samples\n#         self.max_val_samples = config.training.max_val_samples\n#         self.max_test_samples = config.training.max_test_samples\n\n#     def prepare_data(self):\n#         df = pd.read_parquet(os.path.join('/kaggle/input/cic-ids-2017-parquet', self.config.data.raw))\n#         print(\"--------------------- cic-ids-2017-parquet -----------------------\")\n#         # df = pd.read_parquet(os.path.join('/kaggle/input/cic-ton-iot-parquet', self.config.data.raw))\n#         # print(\"--------------------- cic-ton-iot-parquet -----------------------\")\n\n#         # Clean data\n#         df.replace([np.inf, -np.inf], np.nan, inplace=True)\n#         df.dropna(inplace=True)\n#         df.drop_duplicates(inplace=True)\n    \n#         # Identify non-numeric columns\n#         self.non_numeric_cols = ['Label', 'Timestamp', 'Flow ID', 'Src IP', \n#                                'Src Port', 'Attack', 'Dst IP', 'Dst Port', 'Protocol']\n#         self.non_numeric_cols = [col for col in self.non_numeric_cols if col in df.columns]\n    \n#         # Encode labels\n#         self.label_encoder = LabelEncoder()\n#         df['Label_Num'] = self.label_encoder.fit_transform(df['Attack'])\n#         self.classes = self.label_encoder.classes_\n    \n#         # Initialize scaler\n#         self.scaler = StandardScaler()\n    \n#         # Stratified split with sample limits\n#         train_df, test_df = train_test_split(\n#             df,\n#             test_size=0.3,  # 70% train, 30% test+val\n#             random_state=42,\n#             stratify=df['Label_Num']\n#         )\n        \n#         # Further split test into val and test\n#         val_df, test_df = train_test_split(\n#             test_df,\n#             test_size=0.5,  # 15% val, 15% test\n#             random_state=42,\n#             stratify=test_df['Label_Num']\n#         )\n    \n#         # Process each split with sample limits\n#         self.X_train, self.y_train = self._prepare_features(train_df, fit=True)\n#         self.X_val, self.y_val = self._prepare_features(val_df, fit=False)\n#         self.X_test, self.y_test = self._prepare_features(test_df, fit=False)\n        \n#         # Apply sample limits\n#         self._limit_samples()\n\n#     def _limit_samples(self):\n#         \"\"\"Limit samples according to configuration\"\"\" # --------------- unused\n#         pass\n    \n#     def _prepare_features(self, df, fit=False):\n#         X = df.drop(['Label_Num'] + self.non_numeric_cols, axis=1)\n#         y = df['Label_Num']\n#         if fit:\n#             X = self.scaler.fit_transform(X)\n#         else:\n#             X = self.scaler.transform(X)\n#         # return self.create_sequences(X, y)\n#         return X, y\n\n#     def create_sequences(self, X, y):\n#         \"\"\"Create sequence\"\"\" # ---------------- unused\n#         sequences = []\n#         labels = []\n#         for i in range(len(X) - self.sequence_length):\n#             sequences.append(X[i:i+self.sequence_length])\n#             labels.append(y.iloc[i+self.sequence_length-1])\n#         return np.array(sequences), np.array(labels)\n    \n\n#     def setup(self, stage=None):\n#         self.scaler = StandardScaler()\n#         self.scaler.fit(self.X_train.reshape(-1, self.X_train.shape[-1]))\n        \n#         self.X_train = self.scaler.transform(self.X_train.reshape(-1, self.X_train.shape[-1])).reshape(self.X_train.shape)\n#         self.X_val = self.scaler.transform(self.X_val.reshape(-1, self.X_val.shape[-1])).reshape(self.X_val.shape)\n#         self.X_test = self.scaler.transform(self.X_test.reshape(-1, self.X_test.shape[-1])).reshape(self.X_test.shape)\n        \n#         # self.train_dataset = TensorDataset(torch.FloatTensor(self.X_train), torch.LongTensor(self.y_train))\n#         # self.val_dataset = TensorDataset(torch.FloatTensor(self.X_val), torch.LongTensor(self.y_val))\n#         # self.test_dataset = TensorDataset(torch.FloatTensor(self.X_test), torch.LongTensor(self.y_test))\n\n#         self.train_dataset = TimeSeriesDataset(\n#             data=torch.FloatTensor(self.X_train),\n#             sequence_length=self.sequence_length,\n#             target_idx=torch.LongTensor(self.y_train.values)\n#         )\n        \n#         self.val_dataset = TimeSeriesDataset(\n#             data=torch.FloatTensor(self.X_val),\n#             sequence_length=self.sequence_length,\n#             target_idx=torch.LongTensor(self.y_val.values)\n#         )\n        \n#         self.test_dataset = TimeSeriesDataset(\n#             data=torch.FloatTensor(self.X_test),\n#             sequence_length=self.sequence_length,\n#             target_idx=torch.LongTensor(self.y_test.values)\n#         )\n\n#         # print(\"the model will be trained on: \",len(self.train_dataset),\" samples.\")\n#         # print(\"the model will be validated on: \",len(self.val_dataset),\" samples.\")\n#         # print(\"the model will be tested on: \",len(self.test_dataset),\" samples.\")\n        \n#     def train_dataloader(self):\n#         if self.oversample:\n#             class_counts = np.bincount(self.y_train)\n#             weights = 1. / class_counts[self.y_train]\n#             sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n#         else:\n#             sampler = RandomSampler(self.train_dataset)\n            \n#         return DataLoader(\n#             self.train_dataset,\n#             batch_size=self.batch_size,\n#             sampler=sampler,\n#             num_workers=self.num_workers,\n#             persistent_workers=True,\n#             pin_memory=True\n#         )\n    \n#     def val_dataloader(self):\n#         return DataLoader(\n#             self.val_dataset,\n#             batch_size=self.batch_size,\n#             shuffle=False,\n#             num_workers=self.num_workers,\n#             pin_memory=True\n#         )\n    \n#     def test_dataloader(self):\n#         return DataLoader(\n#             self.test_dataset,\n#             batch_size=self.batch_size,\n#             shuffle=False,\n#             num_workers=self.num_workers,\n#             pin_memory=True\n#         )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T12:12:26.390943Z","iopub.execute_input":"2025-10-04T12:12:26.391229Z","iopub.status.idle":"2025-10-04T12:12:26.415964Z","shell.execute_reply.started":"2025-10-04T12:12:26.391203Z","shell.execute_reply":"2025-10-04T12:12:26.415496Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\nclass NIDSDataModule(pl.LightningDataModule):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.batch_size = config.training.batch_size\n        self.sequence_length = config.training.sequence_length\n        self.num_workers = config.data.num_workers\n        self.oversample = config.training.oversample\n        self.non_numeric_cols=[]\n        self.scaler=None\n        self.alpha = 0.5\n\n    def prepare_data(self):\n        print(f\"--------------------- {name}-parquet -----------------------\")\n        if not os.path.exists(new_path):\n            print(\"--------------------- Centralities Calculations  -----------------------\")\n            if dataset_name==\"CIC_IDS_2017\":\n                df = pd.read_parquet(os.path.join('/kaggle/input/cic-ids-2017-parquet', self.config.data.raw))\n                # print(\"--------------------- cic-ids-2017-parquet -----------------------\")\n            elif dataset_name==\"CIC_TON_IOT\":\n                df = pd.read_parquet(os.path.join('/kaggle/input/cic-ton-iot-parquet', self.config.data.raw))\n                # print(\"--------------------- cic-ton-iot-parquet -----------------------\")\n            elif dataset_name==\"cic_ids_2017_5_percent\":\n                df = pd.read_parquet(os.path.join('/kaggle/input/cic-ids-2017-5-percent', self.config.data.raw))\n                # print(\"--------------------- /kaggle/input/cic-ids-2017-5-percent -----------------------\")\n    \n            print(\"Clean data\")\n            df.replace([np.inf, -np.inf], np.nan, inplace=True)\n            df.dropna(inplace=True)\n            df.drop_duplicates(inplace=True)\n            \n            print(\"Reset index after cleaning\")\n            df = df.reset_index(drop=True)\n            \n            print(\"Identify non-numeric columns\")\n            self.non_numeric_cols = ['Label', 'Timestamp', 'Flow ID', 'Src IP', \n                                     'Src Port', 'Attack', 'Dst IP', 'Dst Port', 'Protocol']\n            self.non_numeric_cols = [col for col in self.non_numeric_cols if col in df.columns]\n    \n    \n            print(\"Encode labels\")\n            self.label_encoder = LabelEncoder()\n            df['Label_Num'] = self.label_encoder.fit_transform(df['Attack'])\n            self.classes = self.label_encoder.classes_\n            \n            # print(\"Initialize scaler\")\n            # self.scaler = StandardScaler()\n            # CHANGE 1.01\n            # print(\"Calculate Centralities start\")\n            print(\"--------------------- Centralities Calculations starting-----------------------\")\n            df = add_centralities(\n                df= df,\n                new_path=new_path,\n                graph_path=graph_path,\n                dataset=dataset,\n                cn_measures=cn_measures,\n                network_features=network_features\n            )\n        else:\n            # print(new_path,\" exist now!\")\n            print(\"--------------------- Centralities Extractions  -----------------------\")\n            df = pd.read_parquet(new_path)\n        # CHANGES TO REMOVE use only for test with 5_perecent \n        if dataset_name==\"cic_ids_2017_5_percent\":\n            multiplier = 5  # change as you wish\n    \n            target_substring = \"Sql Injection\"\n            target_rows = df[df['Attack'].str.contains(target_substring, na=False)]\n            \n            if target_rows.empty:\n                print(f\"Warning: no rows found matching '{target_substring}' in train set.\")\n            else:\n                duplicated_block = pd.concat([target_rows] * (multiplier - 1), ignore_index=True)\n                df = pd.concat([df, duplicated_block], ignore_index=True)\n                print(f\"Duplicated target class rows by factor {multiplier}. New count:\",\n                  df['Attack'].str.contains(target_substring, na=False).sum())\n            \n            \n            \n        print(\"\\n===== Class Distribution =====\")\n        counts = df[\"Attack\"].value_counts()\n        print(counts)\n        # print()\n        print(\"Initialize scaler\")\n        self.scaler = StandardScaler()\n        print(\"Train/Val/Test split\")\n        train_df, test_df = train_test_split(\n            df,\n            test_size=1 - self.config.training.train_size,\n            random_state=42,\n            stratify=df['Label_Num']\n        )\n        val_df, test_df = train_test_split(\n            test_df,\n            test_size=0.5,\n            random_state=42,\n            stratify=test_df['Label_Num']\n        )\n        # Reset indices after splitting\n        train_df = train_df.reset_index(drop=True)\n        val_df = val_df.reset_index(drop=True)\n        test_df = test_df.reset_index(drop=True)\n        \n        # Use `fit=True` only for training data\n        self.X_train, self.y_train = self._prepare_features(train_df, fit=True)\n        self.X_val, self.y_val = self._prepare_features(val_df, fit=False)\n        self.X_test, self.y_test = self._prepare_features(test_df, fit=False)\n        # print(\"Val:\\n\", val_df['Label_Num'].value_counts(normalize=True).sort_index())\n        # print(\"Test:\\n\", test_df['Label_Num'].value_counts(normalize=True).sort_index())\n\n    def _prepare_features(self, df, fit=False):\n        X = df.drop(['Label_Num'] + self.non_numeric_cols, axis=1)\n        y = df['Label_Num']\n        if fit:\n            X = self.scaler.fit_transform(X)\n        else:\n            X = self.scaler.transform(X)\n        # return self.create_sequences(X, y)\n        return X, y\n        \n    def create_sequences(self, X, y):\n        sequences = []\n        labels = []\n        y_values = y.values if hasattr(y, 'values') else y  # Convert to numpy array if pandas Series\n        for i in range(len(X) - self.sequence_length):\n            seq = X[i:i+self.sequence_length]\n            sequences.append(seq)\n            labels.append(y_values[i+self.sequence_length-1])\n        return np.array(sequences), np.array(labels)\n       \n    def setup(self, stage=None):\n        self.scaler = StandardScaler()\n        # self.scaler.fit(self.X_train.reshape(-1, self.X_train.shape[-1]))\n        \n        # self.X_train = self.scaler.transform(self.X_train.reshape(-1, self.X_train.shape[-1])).reshape(self.X_train.shape)\n        # self.X_val = self.scaler.transform(self.X_val.reshape(-1, self.X_val.shape[-1])).reshape(self.X_val.shape)\n        # self.X_test = self.scaler.transform(self.X_test.reshape(-1, self.X_test.shape[-1])).reshape(self.X_test.shape)\n        \n        # self.train_dataset = TensorDataset(torch.FloatTensor(self.X_train), torch.LongTensor(self.y_train))\n        # self.val_dataset = TensorDataset(torch.FloatTensor(self.X_val), torch.LongTensor(self.y_val))\n        # self.test_dataset = TensorDataset(torch.FloatTensor(self.X_test), torch.LongTensor(self.y_test))\n\n        self.train_dataset = TimeSeriesDataset(\n            data=torch.FloatTensor(self.X_train),\n            sequence_length=self.sequence_length,\n            target_idx=torch.LongTensor(self.y_train)\n        )\n        \n        self.val_dataset = TimeSeriesDataset(\n            data=torch.FloatTensor(self.X_val),\n            sequence_length=self.sequence_length,\n            target_idx=torch.LongTensor(self.y_val)\n        )\n        \n        self.test_dataset = TimeSeriesDataset(\n            data=torch.FloatTensor(self.X_test),\n            sequence_length=self.sequence_length,\n            target_idx=torch.LongTensor(self.y_test)\n        )\n            \n    def train_dataloader(self):\n        if self.oversample:\n            class_counts = np.bincount(self.y_train)\n            weights = 1. / class_counts[self.y_train]\n            sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n        else:\n            sampler = RandomSampler(self.train_dataset)\n            \n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            sampler=sampler,\n            num_workers=self.num_workers,\n            persistent_workers=True, \n            # persistent_workers=False, # better for tpu\n            pin_memory=True\n        )\n    # c=0\n    def val_dataloader(self):\n        # print(c)\n        # c+=1\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n            # pin_memory=False # better for tpu\n        )\n    \n    def test_dataloader(self):\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n            # pin_memory=False # TPU \n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T12:12:26.416738Z","iopub.execute_input":"2025-10-04T12:12:26.416900Z","iopub.status.idle":"2025-10-04T12:12:26.437622Z","shell.execute_reply.started":"2025-10-04T12:12:26.416885Z","shell.execute_reply":"2025-10-04T12:12:26.437129Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"warnings.filterwarnings('ignore')\n\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, data, sequence_length, target_idx=None):\n        \"\"\"\n        Dataset for time series data that creates sequences on-the-fly.\n \n        Args:\n            data: Input data tensor of shape (n_samples, n_features)\n            sequence_length: Length of sequences to create\n            target_idx: Optional tensor of target indices. If None, uses the last position in sequence\n        \"\"\"\n        self.data = data\n        self.sequence_length = sequence_length\n        self.target_idx = target_idx\n \n    def __len__(self):\n        return len(self.data)\n \n    def __getitem__(self, idx):\n        # Calculate start index for the sequence\n        start_idx = max(0, idx - self.sequence_length + 1)\n        \n        # Get the sequence\n        sequence = self.data[start_idx:idx + 1]\n        \n        # Ensure sequence is 2D [seq_len, features]\n        if sequence.dim() == 1:\n            sequence = sequence.unsqueeze(0)  # Add sequence length dimension if missing\n        \n        # Pad the beginning if needed\n        if len(sequence) < self.sequence_length:\n            padding = torch.zeros(self.sequence_length - len(sequence), sequence.shape[1])\n            sequence = torch.cat([padding, sequence], dim=0)\n        \n        # Get target\n        target = self.target_idx[idx] if self.target_idx is not None else -1\n        \n        return sequence, target\n\nclass AutoEncoderModel(pl.LightningModule):\n    def __init__(self, input_size, num_classes, config):\n        super().__init__()\n        self.save_hyperparameters({'config': config})\n        self.config = config\n        \n        self.encoder = nn.Sequential(\n            nn.Linear(input_size, 128),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n            nn.Linear(64, 32)\n        )\n\n        self.decoder = nn.Sequential(\n            nn.Linear(32, 64),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n            nn.Linear(64, 128),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n            nn.Linear(128, input_size)  # match input size for reconstruction\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Linear(32, 64),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n            nn.Linear(64, num_classes)\n        )\n\n        self.recon_loss = nn.MSELoss()\n        self.class_loss = nn.CrossEntropyLoss()\n\n        # Metrics\n        self.train_acc = torchmetrics.classification.Accuracy(task='multiclass', num_classes=num_classes)\n        self.val_acc = torchmetrics.classification.Accuracy(task='multiclass', num_classes=num_classes)\n        self.test_acc = torchmetrics.classification.Accuracy(task='multiclass', num_classes=num_classes)\n\n    def forward(self, x):\n        batch_size, seq_len, features = x.shape\n        x_flat = x.view(batch_size, -1)\n        z = self.encoder(x_flat)\n\n        x_hat = self.decoder(z).view(batch_size, seq_len, features)\n        logits = self.classifier(z)\n        return x_hat, logits\n    def on_train_epoch_end(self):\n        print(f\"✅ Finished Epoch {self.current_epoch+1}\")\n        \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        x_hat, logits = self(x)\n        loss_recon = self.recon_loss(x_hat, x)\n        loss_cls = self.class_loss(logits, y)\n        loss = loss_recon + loss_cls\n\n        preds = torch.argmax(logits, dim=1)\n        self.train_acc.update(preds, y)\n\n        self.log(\"train_loss_epoch\", loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"train_acc_epoch\", self.train_acc.compute() * 100, on_step=False, on_epoch=True, prog_bar=True)\n        # self.log(\"train_acc_epoch\", self.train_acc.compute()*100, on_step=False, on_epoch=True, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        x_hat, logits = self(x)\n        loss_recon = self.recon_loss(x_hat, x)\n        loss_cls = self.class_loss(logits, y)\n        loss = loss_recon + loss_cls\n\n        preds = torch.argmax(logits, dim=1)\n        self.val_acc.update(preds, y)\n\n        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n        # self.log(\"val_acc\", self.val_acc*100, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"val_acc\", self.val_acc.compute() * 100, on_step=False, on_epoch=True, prog_bar=True)\n        return loss\n\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        x_hat, logits = self(x)\n        loss_recon = self.recon_loss(x_hat, x)\n        # loss_cls = self.class_loss(logits, y)\n        loss = loss_recon \n        # loss = loss_recon + loss_cls\n\n        preds = torch.argmax(logits, dim=1)\n        self.test_acc.update(preds, y)\n\n        self.log(\"test_loss\", loss, on_step=False, on_epoch=True)\n        # print(\"loss: \",loss ,\".\")\n        # self.log(\"test_acc\", self.test_acc, on_step=False, on_epoch=True)\n        self.log(\"test_acc\", self.test_acc.compute() * 100, on_step=False, on_epoch=True, prog_bar=True)\n        return {\"loss\": loss}\n\n    def configure_optimizers(self):\n        return optim.AdamW(\n            self.parameters(),\n            lr=self.hparams.config.model.learning_rate,\n            weight_decay=self.hparams.config.model.weight_decay\n        )\n\ndef init_wandb():\n    user_secrets = UserSecretsClient()\n    wandb_api_key = user_secrets.get_secret(\"mohammad_wandb_secret\")\n    wandb.login(key=wandb_api_key)\n    \n    run = wandb.init(\n        project=config.wandb.project,\n        entity=config.wandb.entity,\n        tags=config.wandb.tags,\n        notes=config.wandb.notes,\n        config={\n            \"input_size\": None,\n            \"num_classes\": None,\n            \"sequence_length\": config.training.sequence_length,\n            # \"train_samples\": config.training.max_train_samples,\n            # \"val_samples\": config.training.max_val_samples,\n            # \"test_samples\": config.training.max_test_samples,\n            \"model_config\": dict(config.model),\n            \"training_config\": dict(config.training)\n        }\n    )\n    \n    wandb_logger = WandbLogger(\n        experiment=run,\n        log_model='all'\n    )\n    \n    return wandb_logger, run\n\ndef main():\n    # wandb_logger, run = init_wandb()\n    \n    data_module = NIDSDataModule(config)\n    data_module.prepare_data()\n    data_module.setup()\n    \n    sample_x, _ = next(iter(data_module.train_dataloader()))\n    input_size_per_timestep = sample_x.shape[2]  # Features per timestep\n    total_input_size = input_size_per_timestep * config.training.sequence_length\n    num_classes = len(data_module.classes)\n    \n    # run.config.update({\n    #     \"input_size_per_timestep\": input_size_per_timestep,\n    #     \"total_input_size\": total_input_size,\n    #     \"num_classes\": num_classes\n    # })\n    \n    model = AutoEncoderModel(total_input_size, num_classes, config)\n    \n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=config.training.early_stopping_patience,\n        mode='min'\n    )\n    \n    checkpoint_callback = ModelCheckpoint(\n        monitor='val_loss',\n        mode='min',\n        save_top_k=1,\n        dirpath='checkpoints',\n        filename='best_model'\n    )\n \n    trainer = pl.Trainer(\n        precision=16,\n        # logger=wandb_logger,\n        max_epochs=config.training.max_epochs,\n        callbacks=[early_stopping, checkpoint_callback],\n        deterministic=True,\n        gradient_clip_val=1.0,\n        enable_progress_bar=True,\n        log_every_n_steps=1000\n    )\n    \n    # Measure start time and memory\n    start_time = time.time()\n    process = psutil.Process()\n    print(\"------------- start training ---------------------\")\n    # Train model\n    trainer.fit(model, datamodule=data_module)\n    print(\"------------- end training ---------------------\")\n    print(\"------------- start testing ---------------------\")\n    # Test model\n    test_results = trainer.test(model, datamodule=data_module)\n    print(\"------------- end testing ---------------------\")\n    # Collect all predictions and targets for final evaluation\n    test_loader = data_module.test_dataloader()\n    all_preds = []\n    all_targets = []\n    all_probs = []  # <-- store probabilities here\n    criterion = torch.nn.CrossEntropyLoss()\n    test_loss_sum=0\n    total_samples=0\n    \n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            x, y = batch\n            _, logits = model(x)  # unpack tuple\n            preds = torch.argmax(logits, dim=1)\n            probs = torch.softmax(logits, dim=1).cpu().numpy()\n            \n            loss = criterion(logits, y)  # use logits for classification loss\n            test_loss_sum += loss.item() * x.size(0)\n            total_samples += x.size(0)\n            \n            all_probs.extend(probs)\n            all_preds.extend(preds.cpu().numpy())\n            all_targets.extend(y.cpu().numpy())  \n    # End time and memory\n    end_time = time.time()\n    time_consumption = end_time - start_time\n    memory_consumption_mb = process.memory_info().rss / (1024 * 1024)\n    class_names = data_module.classes.tolist()\n    # metrics calculation\n    print(\"------------- metric calculation ---------------------\")\n    test_acc = accuracy_score(all_targets, all_preds)\n    test_f1 = f1_score(all_targets, all_preds, average='weighted')\n    test_recall = recall_score(all_targets, all_preds, average='weighted')\n    test_precision = precision_score(all_targets, all_preds, average='weighted')\n    test_loss_OLD = test_results[0]['test_loss'] if 'test_loss' in test_results[0] else None\n    test_loss = test_loss_sum / total_samples\n    print(test_loss_OLD)\n    print(test_loss)\n\n    all_probs = np.array(all_probs)\n    \n    try:\n        if all_probs.shape[1] == 2:\n            # Binary classification → probability of positive class\n            auc_score = roc_auc_score(all_targets, all_probs[:, 1])\n        else:\n            # Multi-class classification\n            auc_score = roc_auc_score(all_targets, all_probs, multi_class='ovr')\n    except Exception as e:\n        print(f\"AUC computation failed: {e}\")\n        auc_score = None\n\n    \n    # False Positive / False Negative Rate\n    cm = confusion_matrix(all_targets, all_preds)\n    cr = classification_report(\n        all_targets, all_preds, digits=4, output_dict=True, zero_division=0)\n    report = classification_report(\n        all_targets, all_preds, digits=4, output_dict=False, zero_division=0)\n    # weighted_f1 = f1_score(all_targets, all_preds,\n    #                        average=\"weighted\") * 100\n\n    # results_fpr_fnr = calculate_fpr_fnr_with_global(cm)\n    # fpr = results_fpr_fnr[\"global\"][\"FPR\"]\n    # fnr = results_fpr_fnr[\"global\"][\"FNR\"]\n\n    # results = {\n    #     \"test_weighted_f1\": weighted_f1,\n    #     \"test_auc\": auc_score * 100 if auc_score is not None else None,\n    #     \"test_fpr\": fpr,\n    #     \"test_fnr\": fnr,\n    #     \"classification_report\": cr,\n    #     \"results_fpr_fnr\": results_fpr_fnr\n    # }\n    # os.makedirs(\"temp\", exist_ok=True)\n    # json_path = os.path.join(\"temp\", f\"LSTM_results.json\")\n    # with open(json_path, \"w\") as f:\n    #     json.dump(results, f, indent=4)\n        \n    FP = cm.sum(axis=0) - np.diag(cm)\n    FN = cm.sum(axis=1) - np.diag(cm)\n    TP = np.diag(cm)\n    TN = cm.sum() - (FP + FN + TP)\n    fpr = FP.sum() / (FP.sum() + TN.sum())\n    fnr = FN.sum() / (FN.sum() + TP.sum())\n    # --- Per-class FPR/FNR ---\n    per_class_fpr = np.where((FP + TN) > 0, FP / (FP + TN), 0.0)\n    per_class_fnr = np.where((FN + TP) > 0, FN / (FN + TP), 0.0)\n    \n    # Convert to dict with class names\n    fpr_dict = {class_names[i]: per_class_fpr[i] for i in range(len(class_names))}\n    fnr_dict = {class_names[i]: per_class_fnr[i] for i in range(len(class_names))}\n\n    metrics_dict = {\n        \"Test Accuracy\": test_acc,\n        \"F1 Score\": test_f1,\n        \"Test Loss\": test_loss,\n        \"Recall\": test_recall,\n        \"Precision\": test_precision,\n        \"AUC\": auc_score,\n        \"False Positive Rate\": fpr,\n        \"False Negative Rate\": fnr,\n        \"Time Consumption (s)\": time_consumption,\n        \"Memory Consumption (MB)\": memory_consumption_mb\n    }\n    # Print metrics\n    print(\"------------- final evaluation metric ---------------------\")\n    for k, v in metrics_dict.items():\n        print(f\"{k}: {v}\")\n\n    metrics_df = pd.DataFrame([metrics_dict])\n\n\n    # metrics_df.to_csv(\"/kaggle/working/metrics_results.csv\", index=False)\n\n    # Log final test metrics\n    # wandb.log({\n    #     'test_acc': test_acc,\n    #     'test_f1': test_f1,\n    #     'test_loss': test_results[0]['test_loss']\n    # })\n    \n    # Confusion matrix and classification report\n    # class_names = data_module.classes.tolist()\n    \n    # Confusion Matrix\n    # wandb.log({\n    #     \"confusion_matrix\": wandb.plot.confusion_matrix(\n    #         y_true=all_targets,\n    #         preds=all_preds,\n    #         class_names=class_names,\n    #         title=\"Confusion Matrix\"\n    #     )\n    # })\n    \n    # Classification Report\n    # Get class names from the data module's label encoder\n    # class_names = list(data_module.label_encoder.classes_)\n    \n    # Generate classification report with names instead of numbers\n    # class_names = list(data_module.label_encoder.classes_)\n    class_names = [str(c) for c in data_module.label_encoder.classes_]\n    print(\"--------------- class names --------------------\")\n    print(class_names)\n    # Generate classification report dict once\n    report = classification_report(\n        all_targets,\n        all_preds,\n        target_names=class_names,\n        output_dict=True\n    )\n    for i, cls in enumerate(class_names):\n        report[cls][\"FPR\"] = per_class_fpr[i]\n        report[cls][\"FNR\"] = per_class_fnr[i]\n\n    # Optionally, also store global ones\n    report[\"macro avg\"][\"FPR\"] = fpr\n    report[\"macro avg\"][\"FNR\"] = fnr\n    report[\"weighted avg\"][\"FPR\"] = np.average(per_class_fpr, weights=cm.sum(axis=1))\n    report[\"weighted avg\"][\"FNR\"] = np.average(per_class_fnr, weights=cm.sum(axis=1))\n        \n    print(\"Classification Report:\")\n    print(classification_report(all_targets, all_preds, target_names=class_names))\n    \n    print(\"Confusion Matrix:\")\n    print(confusion_matrix(all_targets, all_preds))\n    \n    \n    print(\"------------- classification report dict -------------\")\n    print(report)\n    final_report = pd.DataFrame(report).transpose()\n    metrics_df = pd.DataFrame([metrics_dict])\n    merged = pd.concat([final_report, metrics_df], axis=0)\n    \n    merged.to_csv(os.path.join(\"/kaggle/working/\",f\"AE_{dataset_name}_report.csv\"))\n    # Create a wandb Table for the classification report\n    # report_table = wandb.Table(columns=[\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"])\n    # for class_name in class_names:\n    #     report_table.add_data(\n    #         class_name,\n    #         report[class_name][\"precision\"],\n    #         report[class_name][\"recall\"],\n    #         report[class_name][\"f1-score\"],\n    #         report[class_name][\"support\"]\n    #     )\n    \n    # # Add weighted averages\n    # report_table.add_data(\n    #     \"Weighted Avg\",\n    #     report[\"weighted avg\"][\"precision\"],\n    #     report[\"weighted avg\"][\"recall\"],\n    #     report[\"weighted avg\"][\"f1-score\"],\n    #     report[\"weighted avg\"][\"support\"]\n    # )\n    # print(\"empty\")\n    # wandb.log({\"classification_report\": report_table})\n    \n    # Finish wandb run\n    # wandb.finish()\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T12:12:26.438225Z","iopub.execute_input":"2025-10-04T12:12:26.438462Z","iopub.status.idle":"2025-10-04T12:55:16.134863Z","shell.execute_reply.started":"2025-10-04T12:12:26.438444Z","shell.execute_reply":"2025-10-04T12:55:16.134136Z"}},"outputs":[{"name":"stdout","text":"--------------------- cic_ids_2017-parquet -----------------------\n--------------------- Centralities Calculations  -----------------------\nClean data\nReset index after cleaning\nIdentify non-numeric columns\nEncode labels\n--------------------- Centralities Calculations starting-----------------------\ncalculated betweenness\ncalculated degree\ncalculated closeness\ncalculated pagerank\ncalculated k_truss\n==>> features_dicts: ('betweenness', 19129)\n==>> features_dicts: ('degree', 19129)\n==>> features_dicts: ('pagerank', 19129)\n==>> features_dicts: ('closeness', 19129)\n==>> features_dicts: ('k_truss', 19129)\n--------------------------  writting the DataFrame to /kaggle/working/cic_ids_2017_with_centralities.parquet ----------------------\n--------------------------DataFrame written to /kaggle/working/cic_ids_2017_with_centralities.parquet --------------------------\n\n===== Class Distribution =====\nAttack\nBENIGN                        2271122\nDoS Hulk                       230123\nPortScan                       158804\nDDoS                           128025\nDoS GoldenEye                   10293\nFTP-Patator                      7935\nSSH-Patator                      5897\nDoS slowloris                    5796\nDoS Slowhttptest                 5499\nBot                              1956\nWeb Attack � Brute Force         1507\nWeb Attack � XSS                  652\nInfiltration                       36\nWeb Attack � Sql Injection         21\nHeartbleed                         11\nName: count, dtype: int64\nInitialize scaler\nTrain/Val/Test split\n------------- start training ---------------------\n--------------------- cic_ids_2017-parquet -----------------------\n--------------------- Centralities Extractions  -----------------------\n\n===== Class Distribution =====\nAttack\nBENIGN                        2271122\nDoS Hulk                       230123\nPortScan                       158804\nDDoS                           128025\nDoS GoldenEye                   10293\nFTP-Patator                      7935\nSSH-Patator                      5897\nDoS slowloris                    5796\nDoS Slowhttptest                 5499\nBot                              1956\nWeb Attack � Brute Force         1507\nWeb Attack � XSS                  652\nInfiltration                       36\nWeb Attack � Sql Injection         21\nHeartbleed                         11\nName: count, dtype: int64\nInitialize scaler\nTrain/Val/Test split\n","output_type":"stream"},{"name":"stderr","text":"2025-10-04 12:22:24.224765: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759580544.610163      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759580544.712259      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ccfe516de4547a0ac68178d450b81b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✅ Finished Epoch 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✅ Finished Epoch 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4d46bfeb0204c2bb8592a6e3ac819c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✅ Finished Epoch 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✅ Finished Epoch 5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✅ Finished Epoch 6\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✅ Finished Epoch 7\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✅ Finished Epoch 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✅ Finished Epoch 10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✅ Finished Epoch 11\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✅ Finished Epoch 12\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✅ Finished Epoch 13\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✅ Finished Epoch 14\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"✅ Finished Epoch 15\n------------- end training ---------------------\n------------- start testing ---------------------\n--------------------- cic_ids_2017-parquet -----------------------\n--------------------- Centralities Extractions  -----------------------\n\n===== Class Distribution =====\nAttack\nBENIGN                        2271122\nDoS Hulk                       230123\nPortScan                       158804\nDDoS                           128025\nDoS GoldenEye                   10293\nFTP-Patator                      7935\nSSH-Patator                      5897\nDoS slowloris                    5796\nDoS Slowhttptest                 5499\nBot                              1956\nWeb Attack � Brute Force         1507\nWeb Attack � XSS                  652\nInfiltration                       36\nWeb Attack � Sql Injection         21\nHeartbleed                         11\nName: count, dtype: int64\nInitialize scaler\nTrain/Val/Test split\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"992a9f32839546c7ad451a765bd7a245"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    96.45851135253906    \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.109741449356079    \u001b[0m\u001b[35m \u001b[0m│\n└───────────────────────────┴───────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     96.45851135253906     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.109741449356079     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"------------- end testing ---------------------\n------------- metric calculation ---------------------\n1.109741449356079\n0.10496269267248236\n------------- final evaluation metric ---------------------\nTest Accuracy: 0.9649583168298157\nF1 Score: 0.9804628929346761\nTest Loss: 0.10496269267248236\nRecall: 0.9649583168298157\nPrecision: 0.9973953180293609\nAUC: 0.9841009283586598\nFalse Positive Rate: 0.0025029773692988766\nFalse Negative Rate: 0.035041683170184273\nTime Consumption (s): 1982.3604185581207\nMemory Consumption (MB): 8796.66015625\n--------------- class names --------------------\n['BENIGN', 'Bot', 'DDoS', 'DoS GoldenEye', 'DoS Hulk', 'DoS Slowhttptest', 'DoS slowloris', 'FTP-Patator', 'Heartbleed', 'Infiltration', 'PortScan', 'SSH-Patator', 'Web Attack � Brute Force', 'Web Attack � Sql Injection', 'Web Attack � XSS']\nClassification Report:\n                            precision    recall  f1-score   support\n\n                    BENIGN       1.00      0.96      0.98    340668\n                       Bot       0.02      0.99      0.04       293\n                      DDoS       1.00      1.00      1.00     19204\n             DoS GoldenEye       0.97      1.00      0.98      1544\n                  DoS Hulk       1.00      1.00      1.00     34518\n          DoS Slowhttptest       0.96      0.99      0.97       825\n             DoS slowloris       0.86      0.99      0.92       870\n               FTP-Patator       0.92      0.99      0.95      1190\n                Heartbleed       0.17      1.00      0.29         2\n              Infiltration       0.01      0.50      0.01         6\n                  PortScan       1.00      1.00      1.00     23820\n               SSH-Patator       0.91      0.98      0.95       885\n  Web Attack � Brute Force       0.59      0.65      0.62       226\nWeb Attack � Sql Injection       0.25      0.33      0.29         3\n          Web Attack � XSS       0.10      0.20      0.13        98\n\n                  accuracy                           0.96    424152\n                 macro avg       0.65      0.84      0.67    424152\n              weighted avg       1.00      0.96      0.98    424152\n\nConfusion Matrix:\n[[326288  13433     13     12     67     19    117     67     10    471\n      28     44     12      0     87]\n [     2    291      0      0      0      0      0      0      0      0\n       0      0      0      0      0]\n [     3      0  19118      8     70      2      0      0      0      0\n       1      1      0      0      1]\n [     0      0      0   1537      1      3      1      0      0      0\n       0      0      1      1      0]\n [     7      0     21     24  34425      1      3      9      0      0\n       1      8      2      0     17]\n [     0      0      1      3      0    813      4      0      0      0\n       0      0      4      0      0]\n [     0      0      0      0      0      5    862      1      0      0\n       0      0      2      0      0]\n [     0      0      0      0      1      0      8   1179      0      0\n       0      1      1      0      0]\n [     0      0      0      0      0      0      0      0      2      0\n       0      0      0      0      0]\n [     2      1      0      0      0      0      0      0      0      3\n       0      0      0      0      0]\n [     0      0     17      0     10      0      8     29      0      0\n   23731      2      2      0     21]\n [     0      0      0      1      0      0      1      2      0      0\n       2    871      6      2      0]\n [     0      0      0      0      0      0      0      0      0      0\n       0     24    148      0     54]\n [     0      0      0      1      0      0      0      0      0      0\n       0      1      0      1      0]\n [     0      0      0      0      3      0      0      0      0      0\n       0      1     74      0     20]]\n------------- classification report dict -------------\n{'BENIGN': {'precision': 0.9999570949611096, 'recall': 0.9577888149165756, 'f1-score': 0.978418819437156, 'support': 340668, 'FPR': 0.0001676968041780461, 'FNR': 0.04221118508342433}, 'Bot': {'precision': 0.021202185792349726, 'recall': 0.9931740614334471, 'f1-score': 0.04151804822371237, 'support': 293, 'FPR': 0.031694502181149865, 'FNR': 0.006825938566552901}, 'DDoS': {'precision': 0.9972874282733437, 'recall': 0.9955217662986878, 'f1-score': 0.9964038150831291, 'support': 19204, 'FPR': 0.00012841154913717317, 'FNR': 0.004478233701312227}, 'DoS GoldenEye': {'precision': 0.9691046658259773, 'recall': 0.9954663212435233, 'f1-score': 0.9821086261980831, 'support': 1544, 'FPR': 0.00011594669291636693, 'FNR': 0.0045336787564766836}, 'DoS Hulk': {'precision': 0.9956040142291118, 'recall': 0.9973057535199027, 'f1-score': 0.9964541573196323, 'support': 34518, 'FPR': 0.0003901096926859566, 'FNR': 0.0026942464800973403}, 'DoS Slowhttptest': {'precision': 0.9644128113879004, 'recall': 0.9854545454545455, 'f1-score': 0.9748201438848921, 'support': 825, 'FPR': 7.08672019502654e-05, 'FNR': 0.014545454545454545}, 'DoS slowloris': {'precision': 0.8585657370517928, 'recall': 0.9908045977011494, 'f1-score': 0.919957310565635, 'support': 870, 'FPR': 0.0003354737503602799, 'FNR': 0.009195402298850575}, 'FTP-Patator': {'precision': 0.916083916083916, 'recall': 0.9907563025210084, 'f1-score': 0.9519580137262817, 'support': 1190, 'FPR': 0.0002553420874688506, 'FNR': 0.009243697478991597}, 'Heartbleed': {'precision': 0.16666666666666666, 'recall': 1.0, 'f1-score': 0.2857142857142857, 'support': 2, 'FPR': 2.3576564894494872e-05, 'FNR': 0.0}, 'Infiltration': {'precision': 0.006329113924050633, 'recall': 0.5, 'f1-score': 0.012499999999999999, 'support': 6, 'FPR': 0.001110466678926596, 'FNR': 0.5}, 'PortScan': {'precision': 0.9986533686824054, 'recall': 0.9962636439966415, 'f1-score': 0.9974570750057794, 'support': 23820, 'FPR': 7.993365506629497e-05, 'FNR': 0.0037363560033585224}, 'SSH-Patator': {'precision': 0.9139559286463799, 'recall': 0.984180790960452, 'f1-score': 0.9477693144722523, 'support': 885, 'FPR': 0.00019373114369889455, 'FNR': 0.015819209039548022}, 'Web Attack � Brute Force': {'precision': 0.5873015873015873, 'recall': 0.6548672566371682, 'f1-score': 0.6192468619246863, 'support': 226, 'FPR': 0.0002453258351693456, 'FNR': 0.34513274336283184}, 'Web Attack � Sql Injection': {'precision': 0.25, 'recall': 0.3333333333333333, 'f1-score': 0.28571428571428575, 'support': 3, 'FPR': 7.072986144020144e-06, 'FNR': 0.6666666666666666}, 'Web Attack � XSS': {'precision': 0.1, 'recall': 0.20408163265306123, 'f1-score': 0.13422818791946312, 'support': 98, 'FPR': 0.0004244742414881124, 'FNR': 0.7959183673469388}, 'accuracy': 0.9649583168298157, 'macro avg': {'precision': 0.6496749679217727, 'recall': 0.8385999213779664, 'f1-score': 0.6749512630126183, 'support': 424152, 'FPR': 0.0025029773692988766, 'FNR': 0.035041683170184273}, 'weighted avg': {'precision': 0.9973953180293609, 'recall': 0.9649583168298157, 'f1-score': 0.9804628929346761, 'support': 424152, 'FPR': 0.0002012478950502872, 'FNR': 0.03504168317018427}}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}